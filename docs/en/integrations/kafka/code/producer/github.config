# Required connection configs for Kafka producer, consumer, and admin
# bootstrap server - do not include the protocol e.g. pkc-2396y.us-east-1.aws.confluent.cloud:9092
bootstrap.servers=<boostrap host>:<port>
security.protocol=SASL_SSL
sasl.mechanisms=PLAIN
# username for kafka
sasl.username=<username>
# password for kafka
sasl.password=<password>
compression.type=lz4
batch.size=1000
request.timeout.ms=60000

# schema registry configuration
basic.auth.credentials.source=USER_INFO
# set registry url here
schema.registry.url=<registry url>
# set schema registry password here
schema.registry.basic.auth.user.info=<password>

# output topic and max docs to insert
output.topic=github
# -1 means all docs in the file. If this is greater than the number of entries in the file, the script will loop
# until the max_docs is achieved
output.num_messages=10000

# input file and schema
input.file=github_all_columns.ndjson
# auto generated schema if not specified - specify a filename if you want to use a specific schema
# input.schema=

# filter fields - the below removes the array fields (not supported by the jdbc sink for kafka connect) for the github dataset
input.filter_fields=requested_reviewers,assignees,labels