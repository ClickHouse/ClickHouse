---
title: コア設定
sidebar_label: コア設定
slug: /ja/operations/settings/settings
toc_max_heading_level: 2
---
<!-- Autogenerated -->
すべての設定は、テーブル [system.settings](/docs/ja/operations/system-tables/settings) で利用可能です。これらの設定は、[source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp) から自動生成されています。
## add_http_cors_header {#add_http_cors_header}

タイプ: Bool

デフォルト値: 0

HTTP CORS ヘッダーを追加します。

## additional_result_filter {#additional_result_filter}

タイプ: String

デフォルト値:

`SELECT` クエリの結果に適用される追加のフィルター式です。この設定は、いかなるサブクエリにも適用されません。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SElECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```

## additional_table_filters {#additional_table_filters}

タイプ: Map

デフォルト値: {}

指定されたテーブルから読み取った後に適用される追加のフィルター式です。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```

## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty}

タイプ: Bool

デフォルト値: 0

クエリ内のすべての集約関数を書き換える機能を有効または無効にします。また、[-OrNull](../../sql-reference/aggregate-functions/combinators.md/#agg-functions-combinator-ornull) サフィックスが追加されます。SQL 標準の互換性のために有効にします。分散クエリの一貫した結果を得るために、クエリのリライトを介して実装されています（[count_distinct_implementation](#count_distinct_implementation) 設定に類似）。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

次の集約関数を使用したクエリを考えてみます。
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

`aggregate_functions_null_for_empty = 0` の場合、次の結果が表示されます：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

`aggregate_functions_null_for_empty = 1` の場合、結果は次のようになります：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```

## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes}

タイプ: UInt64

デフォルト値: 50000000

プライマリーキーの順序で集約中に蓄積されるブロックの最大サイズ（バイト数）です。小さなブロックサイズは、集約の最終マージステージをより並列化することを可能にします。

## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads}

タイプ: UInt64

デフォルト値: 0

メモリ効率の良いモードで中間集約結果をマージするために使用するスレッド数です。大きいほど、メモリが消費されます。0は「max_threads」と同じ意味です。

## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently}

タイプ: Bool

デフォルト値: 0

パーティションキーがグループ化キーに適合する場合、別々のスレッドでパーティションの独立した集約を有効にします。パーティションの数がコアの数に近く、パーティションのサイズが大まかに同じである場合に有益です。

## allow_archive_path_syntax {#allow_archive_path_syntax}

タイプ: Bool

デフォルト値: 1

ファイル/S3 エンジン/テーブル関数は、アーカイブが正しい拡張子を持っている場合、'::' を '\\<archive\\> :: \\<file\\>' としてパスを解析します。

## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree}

タイプ: Bool

デフォルト値: 0

MergeTree テーブルから読み取るためにバックグラウンド I/O プールを使用します。この設定は、I/O 処理に制約のあるクエリの性能を向上させる可能性があります。

## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet}

タイプ: Bool

デフォルト値: 0

これが有効になっていると、ヘッジリクエストでは、最初のデータパケットを受信するまで新しい接続を開始できます。進行中でも、最初の進行が更新されていない場合 (ただし、`receive_data_timeout` タイムアウトが適用されている場合)、そうでなければ、進行が進んだ後はレプリカの変更を無効にします。

## allow_create_index_without_type {#allow_create_index_without_type}

タイプ: Bool

デフォルト値: 0

TYPE なしで CREATE INDEX クエリを許可します。クエリは無視されます。SQL 互換性テストのために行われています。

## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif}

タイプ: Bool

デフォルト値: 0

throwIf() 関数にカスタムエラーコードを有効にします。これが真の場合、投げられた例外が予期しないエラーコードを持つ可能性があります。

## allow_ddl {#allow_ddl}

タイプ: Bool

デフォルト値: 1

これが真に設定されている場合、ユーザーは DDL クエリを実行することが許可されます。

## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary}

タイプ: Bool

デフォルト値: 0

非推奨の Ordinary エンジンを使用してデータベースを作成することを許可します。

## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions}

タイプ: Bool

デフォルト値: 0

エラーが発生しやすいウィンドウ関数（neighbor, runningAccumulate, runningDifferenceStartingWithFirstValue, runningDifference）の使用を許可します。

## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions}

タイプ: Bool

デフォルト値: 0

`snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake`、および `dateTime64ToSnowflake` 関数は非推奨で、デフォルトで無効にされています。代わりに `snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID`、および `dateTime64ToSnowflakeID` 関数を使用してください。

非推奨の関数を再度有効にするには（例：移行期間中）、この設定を `true` に設定してください。

## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree}

タイプ: Bool

デフォルト値: 0

非推奨のエンジン定義構文を使用して *MergeTree テーブルを作成することを許可します。

## allow_distributed_ddl {#allow_distributed_ddl}

タイプ: Bool

デフォルト値: 1

これが真に設定されている場合、ユーザーは分散 DDL クエリを実行することが許可されます。

## allow_drop_detached {#allow_drop_detached}

タイプ: Bool

デフォルト値: 0

ALTER TABLE ... DROP DETACHED PART[ITION] ... クエリを許可します。

## allow_execute_multiif_columnar {#allow_execute_multiif_columnar}

タイプ: Bool

デフォルト値: 1

multiIf 関数をカラム形式で実行することを許可します。

## allow_experimental_analyzer {#allow_experimental_analyzer}

タイプ: Bool

デフォルト値: 1

新しいクエリアナライザーを許可します。

## allow_experimental_codecs {#allow_experimental_codecs}

タイプ: Bool

デフォルト値: 0

これが真に設定されている場合、エクスペリメンタルな圧縮コーデックを指定することを許可します（ただし、現時点ではそれは存在せず、このオプションは何も実行されません）。

## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql}

タイプ: Bool

デフォルト値: 0

Engine=MaterializedPostgreSQL(...) を使用してデータベースを作成することを許可します。

## allow_experimental_dynamic_type {#allow_experimental_dynamic_type}

タイプ: Bool

デフォルト値: 0

Dynamic データ型を許可します。

## allow_experimental_full_text_index {#allow_experimental_full_text_index}

タイプ: Bool

デフォルト値: 0

これが真に設定されている場合、エクスペリメンタルなフルテキストインデックスを使用することを許可します。

## allow_experimental_funnel_functions {#allow_experimental_funnel_functions}

タイプ: Bool

デフォルト値: 0

ファunnel 分析のためのエクスペリメンタルな関数を有効にします。

## allow_experimental_hash_functions {#allow_experimental_hash_functions}

タイプ: Bool

デフォルト値: 0

エクスペリメンタルなハッシュ関数を有効にします。

## allow_experimental_inverted_index {#allow_experimental_inverted_index}

タイプ: Bool

デフォルト値: 0

これが真に設定されている場合、エクスペリメンタルなインバーテッドインデックスを使用することを許可します。

## allow_experimental_join_condition {#allow_experimental_join_condition}

タイプ: Bool

デフォルト値: 0

左テーブルと右テーブルの両方のカラムを含む非等式条件での結合をサポートします。例： t1.y < t2.y。

## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting}

タイプ: Bool

デフォルト値: 0

これが真に設定されている場合、`join_to_sort_minimum_perkey_rows` と `join_to_sort_maximum_table_rows` の条件が満たされていると、右テーブルをキーで並べ替え、左または内部ハッシュ結合でパフォーマンスを向上させます。

## allow_experimental_json_type {#allow_experimental_json_type}

タイプ: Bool

デフォルト値: 0

JSON データ型を許可します。

## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper}

タイプ: Bool

デフォルト値: 0

ClickHouse Keeper に Kafka 関連のオフセットを保存するためのエクスペリメンタルな機能を許可します。有効にすると、ClickHouse Keeper パスとレプリカ名を Kafka テーブルエンジンに指定できます。この結果、通常の Kafka エンジンの代わりに、コミットされたオフセットを主に ClickHouse Keeper に保存する新しいタイプのストレージエンジンが使用されます。

## allow_experimental_live_view {#allow_experimental_live_view}

タイプ: Bool

デフォルト値: 0

非推奨の LIVE VIEW の作成を許可します。

可能な値:

- 0 — ライブビューでの操作が無効です。
- 1 — ライブビューでの操作が有効です。

## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table}

タイプ: Bool

デフォルト値: 0

MaterializedPostgreSQL テーブルエンジンを使用することを許可します。デフォルトでは無効です。この機能はエクスペリメンタルです。

## allow_experimental_nlp_functions {#allow_experimental_nlp_functions}

タイプ: Bool

デフォルト値: 0

自然言語処理のためのエクスペリメンタルな関数を有効にします。

## allow_experimental_object_type {#allow_experimental_object_type}

タイプ: Bool

デフォルト値: 0

Object および JSON データ型を許可します。

## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas}

タイプ: UInt64

デフォルト値: 0

SELECT クエリの実行のために、各シャードから `max_parallel_replicas` の数だけのレプリカを使用します。読み取りは、動的に並列化され、調整されます。0 - オフ、1 - 有効、失敗した場合は静かに無効化、2 - 有効、失敗した場合に例外をスローします。

## allow_experimental_query_deduplication {#allow_experimental_query_deduplication}

タイプ: Bool

デフォルト値: 0

部分 UUID に基づく SELECT クエリのエクスペリメンタルなデータ重複排除です。

## allow_experimental_shared_set_join {#allow_experimental_shared_set_join}

タイプ: Bool

デフォルト値: 1

ClickHouse Cloud のみ。ShareSet および SharedJoin の作成を許可します。

## allow_experimental_statistics {#allow_experimental_statistics}

タイプ: Bool

デフォルト値: 0

[統計](../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-creating-a-table)を持つカラムの定義および[統計を操作](../../engines/table-engines/mergetree-family/mergetree.md#column-statistics)を許可します。

## allow_experimental_time_series_table {#allow_experimental_time_series_table}

タイプ: Bool

デフォルト値: 0

[TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンを持つテーブルの作成を許可します。

可能な値:

- 0 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンは無効です。
- 1 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンは有効です。

## allow_experimental_variant_type {#allow_experimental_variant_type}

タイプ: Bool

デフォルト値: 0

エクスペリメンタルな [Variant](../../sql-reference/data-types/variant.md) の作成を許可します。

## allow_experimental_vector_similarity_index {#allow_experimental_vector_similarity_index}

タイプ: Bool

デフォルト値: 0

エクスペリメンタルなベクトル類似度インデックスを許可します。

## allow_experimental_window_view {#allow_experimental_window_view}

タイプ: Bool

デフォルト値: 0

WINDOW VIEW を有効にします。成熟していません。

## allow_get_client_http_header {#allow_get_client_http_header}

タイプ: Bool

デフォルト値: 0

現在の HTTP リクエストのヘッダーの値を取得するための `getClientHTTPHeader` 関数を使用することを許可します。セキュリティ上の理由からデフォルトでは有効になっていません。一部のヘッダー（`Cookie` など）には機密情報が含まれる可能性があります。この関数で取得できないヘッダーは、`X-ClickHouse-*` および `Authentication` ヘッダーです。

## allow_hyperscan {#allow_hyperscan}

タイプ: Bool

デフォルト値: 1

Hyperscan ライブラリを使用する関数を許可します。長いコンパイル時間や過剰なリソース使用を避けるために無効にします。

## allow_introspection_functions {#allow_introspection_functions}

タイプ: Bool

デフォルト値: 0

クエリプロファイリングのための [イントロスペクション関数](../../sql-reference/functions/introspection.md) を有効または無効にします。

可能な値:

- 1 — インストロスペクション関数有効。
- 0 — インストロスペクション関数無効。

**参照**

- [Sampling Query Profiler](../../operations/optimizing-performance/sampling-query-profiler.md)
- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select}

タイプ: Bool

デフォルト値: 1

存在しないテーブルやカラムを参照する SELECT クエリを使用して CREATE MATERIALIZED VIEW を許可します。それは依然として構文的に有効でなければなりません。リフレッシュ可能な MV には適用されません。SELECT クエリから MV スキーマを推測する必要がある場合（すなわち、CREATE にカラムリストがなく TO テーブルがない場合）には適用されません。MV のソーステーブルを作成するために使用できます。

## allow_named_collection_override_by_default {#allow_named_collection_override_by_default}

タイプ: Bool

デフォルト値: 1

デフォルトで名前付きコレクションのフィールドをオーバーライドすることを許可します。

## allow_non_metadata_alters {#allow_non_metadata_alters}

タイプ: Bool

デフォルト値: 1

テーブルメタデータだけでなく、ディスク上のデータにも影響を与える ALTER を実行することを許可します。

## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments}

タイプ: Bool

デフォルト値: 0

toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*() のような特定の時間関連関数で非定数のタイムゾーン引数を許可します。

## allow_nondeterministic_mutations {#allow_nondeterministic_mutations}

タイプ: Bool

デフォルト値: 0

ユーザーレベルの設定で、レプリケートされたテーブルが `dictGet` のような非決定的関数を使用するための変更を許可します。

例えば、Dictionaryがノード間で同期していない可能性があるため、Dictionaryから値を引っ張る変更がデフォルメでレプリケートされたテーブルでは許可されていません。この設定を有効にすると、この動作が許可されます。このデータがすべてのノード間で同期されていることをユーザーが確認する責任があります。

**例**

``` xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```

## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards}

タイプ: Bool

デフォルト値: 0

シャーディングキー内の非決定的（例えば、`rand` または `dictGet`、後者には更新に関していくつかの注意事項があります）機能を許可します。

可能な値:

- 0 — 不許可。
- 1 — 許可。

## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem}

タイプ: Bool

デフォルト値: 0

すべての部分がローカルファイルシステムにある場合、プリフェッチスレッドプールを優先します。

## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem}

タイプ: Bool

デフォルト値: 1

すべての部分がリモートファイルシステムにある場合、プリフェッチスレッドプールを優先します。

## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with}

タイプ: Bool

デフォルト値: 1

サブクエリに WITH 句が含まれている場合、プッシュ述語を許可します。

## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions}

タイプ: Bool

デフォルト値: 1

WHERE から PREWHERE に条件を移動する際、最適化フィルタリングのためにそれらを再順序にすることを許可します。

## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert}

タイプ: Bool

デフォルト値: 0

INSERT クエリ内の FORMAT の後に `SETTINGS` が許可されるかどうかを制御します。これは推奨されておらず、これにより `SETTINGS` の一部が値として解釈される可能性があります。

例：

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

しかし、次のクエリは `allow_settings_after_format_in_insert` のみで機能します：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能な値:

- 0 — 不許可。
- 1 — 許可。

:::note
この設定は、旧構文に依存するユースケースの場合にのみ、後方互換性のために使用します。
:::

## allow_simdjson {#allow_simdjson}

タイプ: Bool

デフォルト値: 1

AVX2 命令が利用可能な場合、'JSON*' 関数で simdjson ライブラリを使用することを許可します。無効にすると、rapidjson が使用されます。

## allow_statistics_optimize {#allow_statistics_optimize}

タイプ: Bool

デフォルト値: 0

クエリを最適化するために統計を使用することを許可します。

## allow_suspicious_codecs {#allow_suspicious_codecs}

タイプ: Bool

デフォルト値: 0

これが真に設定されている場合、意味のない圧縮コーデックを指定することを許可します。

## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types}

タイプ: Bool

デフォルト値: 0

CREATE TABLE ステートメントにおいて、n > 256 の FixedString(n) タイプのカラムを作成することを許可します。長さが 256 以上の FixedString は疑わしく、誤用を示す可能性が高いです。

## allow_suspicious_indices {#allow_suspicious_indices}

タイプ: Bool

デフォルト値: 0

同一の式を持つプライマリ/セカンダリインデックスおよびソートキーを拒否します。

## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types}

タイプ: Bool

デフォルト値: 0

8 バイト以下の固定サイズのデータ型と一緒に [LowCardinality](../../sql-reference/data-types/lowcardinality.md) を使用することを許可または制限します。数値データ型および `FixedString(8_bytes_or_less)`。

小さな固定値に対して `LowCardinality` の使用は通常非効率的です。なぜなら、ClickHouse は各行に対して数値インデックスを保存するからです。その結果：

- ディスクスペースの使用量が増加する可能性があります。
- RAM 消費が高くなる可能性があります。Dictionaryのサイズに依存します。
- 一部の関数は、追加のコーディング/エンコーディング操作のために遅くなることがあります。

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) エンジンテーブル内でのマージ時間は、上記のすべての理由により増加する可能性があります。

可能な値:

- 1 — `LowCardinality` の使用は制限されません。
- 0 — `LowCardinality` の使用は制限されます。

## allow_suspicious_primary_key {#allow_suspicious_primary_key}

タイプ: Bool

デフォルト値: 0

MergeTree の疑わしい `PRIMARY KEY`/`ORDER BY` を許可します（すなわち、SimpleAggregateFunction）。

## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions}

タイプ: Bool

デフォルト値: 0

テーブルのカラムに依存しない TTL 式を拒否します。これはほとんどの場合、ユーザーエラーを示しています。

## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by}

タイプ: Bool

デフォルト値: 0

GROUP BY キーで [Variant](../../sql-reference/data-types/variant.md) および [Dynamic](../../sql-reference/data-types/dynamic.md) タイプの使用を許可または制限します。

## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by}

タイプ: Bool

デフォルト値: 0

ORDER BY キーで [Variant](../../sql-reference/data-types/variant.md) および [Dynamic](../../sql-reference/data-types/dynamic.md) タイプの使用を許可または制限します。

## allow_suspicious_variant_types {#allow_suspicious_variant_types}

タイプ: Bool

デフォルト値: 0

CREATE TABLE ステートメントで、類似のバリアントタイプ（例えば、異なる数値または日付型）を持つバリアントタイプを指定することを許可します。この設定を有効にすると、類似のタイプを持つ値で作業する際に若干の曖昧性を引き起こす可能性があります。

## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper}

タイプ: Bool

デフォルト値: 0

条件なしで system.zookeeper テーブルからの無制限の読み取りを許可します。便利ですが、zookeeper に対して安全ではありません。

## alter_move_to_space_execute_async {#alter_move_to_space_execute_async}

タイプ: Bool

デフォルト値: 0

ALTER TABLE MOVE ... TO [DISK|VOLUME] を非同期に実行します。

## alter_partition_verbose_result {#alter_partition_verbose_result}

タイプ: Bool

デフォルト値: 0

パーティションおよび部品の操作が正常に適用された部品に関する情報の表示を有効または無効にします。これは [ATTACH PARTITION|PART](../../sql-reference/statements/alter/partition.md/#alter_attach-partition) および [FREEZE PARTITION](../../sql-reference/statements/alter/partition.md/#alter_freeze-partition) に適用されます。

可能な値：

- 0 — 詳細表示を無効にします。
- 1 — 詳細表示を有効にします。

**例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```

## alter_sync {#alter_sync}

タイプ: UInt64

デフォルト値: 1

[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または [TRUNCATE](../../sql-reference/statements/truncate.md) クエリによってレプリカでのアクションが実行されるのを待つ設定を行うことができます。

可能な値：

- 0 — 待機しない。
- 1 — 自分の実行を待つ。
- 2 — すべてを待つ。

クラウドデフォルト値: `0`。

:::note
`alter_sync` は `Replicated` テーブルにのみ適用され、`Replicated` でないテーブルの変更には無効です。
:::

## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves}

タイプ: Bool

デフォルト値: 1

テーブルのインデックスに空間充填曲線がある場合（例： `ORDER BY mortonEncode(x, y)` または `ORDER BY hilbertEncode(x, y)`）、およびクエリがその引数に条件を持つ場合（例： `x >= 10 AND x <= 20 AND y >= 20 AND y <= 30`）、インデックス分析のために空間充填曲線を使用します。

## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier}

タイプ: Bool

デフォルト値: 0

プロジェクションからの JOIN USING での識別子の解決を強制します（例えば、`SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` の場合、結合は `t1.a + 1 = t2.b` によって行われ、`t1.b = t2.b` ではありません）。

## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys}

タイプ: Bool

デフォルト値: 0

`ANY INNER|LEFT JOIN` 操作における従来の ClickHouse サーバの動作を有効にします。

:::note
この設定は、従来の `JOIN` の動作に依存するユースケースがある場合のみ使用してください。
:::

従来の動作が有効な場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` 操作の結果は等しくありません。なぜなら、ClickHouse は多対一の左から右にマッピングされたテーブルキーのロジックを使用するからです。
- `ANY INNER JOIN` 操作の結果は、`SEMI LEFT JOIN` 操作と同様に、左テーブルのすべての行を含みます。

従来の動作が無効な場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` 操作の結果は等しいことが保証されます。ClickHouse は `ANY RIGHT JOIN` 操作での一対多のキーのマッピングを提供するロジックを使用するためです。
- `ANY INNER JOIN` 操作の結果は、左および右の両方のテーブルからキーごとに一行を含みます。

可能な値：

- 0 — 従来の動作が無効。
- 1 — 従来の動作が有効。

参照:

- [JOIN の厳密さ](../../sql-reference/statements/select/join.md/#join-settings)

## apply_deleted_mask {#apply_deleted_mask}

タイプ: Bool

デフォルト値: 1

論理削除で削除された行をフィルタリングする機能を有効または無効にします。これが無効になっている場合、クエリはこれらの行を読み取ることができます。これはデバッグや「未削除」シナリオに便利です。

## apply_mutations_on_fly {#apply_mutations_on_fly}

タイプ: Bool

デフォルト値: 0

これが真の場合、データ部分にマテリアライズされていない変異（UPDATE および DELETE）が SELECT に適用されます。

## asterisk_include_alias_columns {#asterisk_include_alias_columns}

タイプ: Bool

デフォルト値: 0

ワイルドカードクエリ（`SELECT *`）のために [ALIAS](../../sql-reference/statements/create/table.md#alias) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効

## asterisk_include_materialized_columns {#asterisk_include_materialized_columns}

タイプ: Bool

デフォルト値: 0

ワイルドカードクエリ（`SELECT *`）のために [MATERIALIZED](../../sql-reference/statements/create/table.md#materialized) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効

## async_insert {#async_insert}

タイプ: Bool

デフォルト値: 0

これが真の場合、INSERT クエリからのデータはキューに保存され、その後バックグラウンドでテーブルにフラッシュされます。wait_for_async_insert が false の場合、INSERT クエリはほぼ瞬時に処理されます。そうでない場合、クライアントはデータがテーブルにフラッシュされるまで待機します。

## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate}

タイプ: Double

デフォルト値: 0.2

適応的な非同期挿入タイムアウトが減少する際の指数成長率。

## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate}

タイプ: Double

デフォルト値: 0.2

適応的な非同期挿入タイムアウトが増加する際の指数成長率。

## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms}

タイプ: ミリ秒

デフォルト値: 200

最初のデータが現れた後、クエリごとに収集されたデータをダンプするまで待機する最大時間。

## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms}

タイプ: ミリ秒

デフォルト値: 50

自動調整が async_insert_use_adaptive_busy_timeout を通じて有効になっている場合、最初のデータが現れてからクエリごとに収集されたデータをダンプするまでの最小時間。この値は、適応アルゴリズムの初期値としても機能します。

## async_insert_deduplicate {#async_insert_deduplicate}

タイプ: Bool

デフォルト値: 0

レプリケートされたテーブルでの非同期 INSERT クエリの場合、挿入ブロックの重複排除が行われるべきことを示します。

## async_insert_max_data_size {#async_insert_max_data_size}

タイプ: UInt64

デフォルト値: 10485760

挿入される前のクエリごとに収集された未解析データの最大サイズ（バイト数）。

## async_insert_max_query_number {#async_insert_max_query_number}

タイプ: UInt64

デフォルト値: 450

挿入される前の最大クエリ数。

## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms}

タイプ: ミリ秒

デフォルト値: 10

非同期挿入キューからデータをポーリングするためのタイムアウト。

## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout}

タイプ: Bool

デフォルト値: 1

これが真に設定されている場合、非同期挿入のために適応的ビジータイムアウトを使用します。

## async_query_sending_for_remote {#async_query_sending_for_remote}

タイプ: Bool

デフォルト値: 1

リモートクエリを実行する際の非同期接続作成およびクエリ送信を有効にします。

デフォルトで有効です。

## async_socket_for_remote {#async_socket_for_remote}

タイプ: Bool

デフォルト値: 1

リモートクエリを実行する際のソケットからの非同期読み取りを有効にします。

デフォルトで有効です。

## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload}

タイプ: Bool

デフォルト値: 1

複数のスレッドで Azure マルチパートアップロードを使用します。

## azure_check_objects_after_upload {#azure_check_objects_after_upload}

タイプ: Bool

デフォルト値: 0

アップロードが成功したことを確認するために、Azure Blob ストレージにアップロードされた各オブジェクトをチェックします。

## azure_create_new_file_on_insert {#azure_create_new_file_on_insert}

タイプ: Bool

デフォルト値: 0

Azure エンジンテーブルに各挿入時に新しいファイルを作成することを有効または無効にします。

## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist}

タイプ: Bool

デフォルト値: 0

特定のキーを読み取る際にファイルが存在しない場合、その不在を無視します。

可能な値:
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外をスローします。

## azure_list_object_keys_size {#azure_list_object_keys_size}

タイプ: UInt64

デフォルト値: 1000

ListObject リクエストによってバッチで返される可能性のあるファイルの最大数。

## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload}

タイプ: UInt64

デフォルト値: 50000

Azure のマルチパートアップロードでの最大ブロック数。

## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file}

タイプ: UInt64

デフォルト値: 20

マルチパートアップロードリクエストで同時に読み込まれる部品の最大数。0 は無制限を意味します。

## azure_max_single_part_copy_size {#azure_max_single_part_copy_size}

タイプ: UInt64

デフォルト値: 268435456

Azure Blob ストレージに単一のパートコピーを使用してコピーする際のオブジェクトの最大サイズ。

## azure_max_single_part_upload_size {#azure_max_single_part_upload_size}

タイプ: UInt64

デフォルト値: 104857600

以下は、単一パートアップロードを使用して Azure Blob ストレージにアップロードするオブジェクトの最大サイズに関する ClickHouse ドキュメントの翻訳です。

## azure_max_single_read_retries {#azure_max_single_read_retries}

タイプ: UInt64

デフォルト値: 4

単一の Azure Blob ストレージリード中の最大リトライ回数。

## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries}

タイプ: UInt64

デフォルト値: 4

Azure Blob ストレージへの書き込み中に予期しないエラーが発生した場合の最大リトライ回数。

## azure_max_upload_part_size {#azure_max_upload_part_size}

タイプ: UInt64

デフォルト値: 5368709120

マルチパートアップロード中に Azure Blob ストレージにアップロードするパートの最大サイズ。

## azure_min_upload_part_size {#azure_min_upload_part_size}

タイプ: UInt64

デフォルト値: 16777216

マルチパートアップロード中に Azure Blob ストレージにアップロードするパートの最小サイズ。

## azure_sdk_max_retries {#azure_sdk_max_retries}

タイプ: UInt64

デフォルト値: 10

Azure SDK の最大リトライ回数。

## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 10

Azure SDK におけるリトライ間の最小バックオフ時間（ミリ秒）。

## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 1000

Azure SDK におけるリトライ間の最大バックオフ時間（ミリ秒）。

## azure_skip_empty_files {#azure_skip_empty_files}

タイプ: Bool

デフォルト値: 0

S3 エンジンで空のファイルをスキップするかどうかを有効または無効にします。

可能な値:
- 0 — 空のファイルが要求されたフォーマットと互換性がない場合、`SELECT` は例外をスローします。
- 1 — 空のファイルに対して `SELECT` は空の結果を返します。

## azure_strict_upload_part_size {#azure_strict_upload_part_size}

タイプ: UInt64

デフォルト値: 0

Azure Blob ストレージにマルチパートアップロード中にアップロードするパートの正確なサイズ。

## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match}

タイプ: Bool

デフォルト値: 0

グロブ拡張ルールに基づいて一致するファイルがゼロの場合にエラーをスローします。

可能な値:
- 1 — `SELECT` は例外をスローします。
- 0 — `SELECT` は空の結果を返します。

## azure_truncate_on_insert {#azure_truncate_on_insert}

タイプ: Bool

デフォルト値: 0

Azure エンジンテーブルに挿入する前に切り捨てを有効または無効にします。

## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor}

タイプ: UInt64

デフォルト値: 2

Azure Blob ストレージに一度の書き込みからアップロードされた azure_multiply_parts_count_threshold パーツごとに azure_min_upload_part_size に掛ける係数。

## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold}

タイプ: UInt64

デフォルト値: 500

この数のパーツが Azure Blob ストレージにアップロードされるたびに、azure_min_upload_part_size は azure_upload_part_size_multiply_factor で掛け算されます。

## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi}

タイプ: UInt64

デフォルト値: 1000

バックアップまたはリストア中の [Zoo]Keeper へのマルチリクエストの最大バッチサイズ。

## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread}

タイプ: UInt64

デフォルト値: 10000

バックアップまたはリストア中の [Zoo]Keeper へのマルチリードリクエストの最大バッチサイズ。

## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds}

タイプ: UInt64

デフォルト値: 3600

`BACKUP ON CLUSTER` または `RESTORE ON CLUSTER` 操作中にホストがこの時間内に ZooKeeper 内の一時的な 'alive' ノードを再作成しない場合、全体のバックアップまたはリストアは失敗と見なされます。
この値は、ホストが障害後に ZooKeeper に再接続するのに十分な合理的な時間より大きい必要があります。
ゼロは無制限を意味します。

## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec}

タイプ: UInt64

デフォルト値: 180

イニシエーターが他のホストの 'error' ノードへの反応を待っている間の時間。

## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability}

タイプ: Float

デフォルト値: 0

バックアップまたはリストア中の keeper リクエストの障害挿入の近似確率。有効な値は [0.0f, 1.0f] の範囲です。

## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed}

タイプ: UInt64

デフォルト値: 0

0 - ランダムシード、そうでなければ設定値。

## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries}

タイプ: UInt64

デフォルト値: 1000

バックアップまたはリストア操作中の [Zoo]Keeper 操作の最大リトライ回数。
一時的な [Zoo]Keeper 障害のために操作全体が失敗しないように十分大きい必要があります。

## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error}

タイプ: UInt64

デフォルト値: 20

`BACKUP ON CLUSTER` または `RESTORE ON CLUSTER` 操作のエラーを処理する際の [Zoo]Keeper 操作の最大リトライ回数。

## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing}

タイプ: UInt64

デフォルト値: 20

`BACKUP ON CLUSTER` または `RESTORE ON CLUSTER` 操作の初期化中の [Zoo]Keeper 操作の最大リトライ回数。

## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

バックアップまたはリストア中の [Zoo]Keeper 操作の初期バックオフタイムアウト。

## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 5000

バックアップまたはリストア中の [Zoo]Keeper 操作の最大バックオフタイムアウト。

## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size}

タイプ: UInt64

デフォルト値: 1048576

バックアップ中の [Zoo]Keeper のノードのデータの最大サイズ。

## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts}

タイプ: UInt64

デフォルト値: 1000

Aws::Client::RetryStrategy の設定。Aws::Client は自動的にリトライを行い、0 はリトライを行わないことを意味します。これはバックアップ/リストアにのみ適用されます。

## cache_warmer_threads {#cache_warmer_threads}

タイプ: UInt64

デフォルト値: 4

ClickHouse Cloud のみで使用可能。キャッシュがフェッチによってポピュレートされるときに、新しいデータパーツをファイルキャッシュに推測的にダウンロードするためのバックグラウンドスレッドの数。ゼロは無効にします。

## calculate_text_stack_trace {#calculate_text_stack_trace}

タイプ: Bool

デフォルト値: 1

クエリ実行中の例外発生時にテキストスタックトレースを計算します。これはデフォルトです。シンボル解決を必要とし、大量の誤ったクエリを実行する場合、ファジングテストが遅くなる可能性があります。通常の場合、これは無効にしないでください。

## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}

タイプ: Bool

デフォルト値: 0

クライアントが応答を待たずに接続を閉じるときに HTTP 読取り専用クエリ（例: SELECT）をキャンセルします。

クラウドデフォルト値: `1`。

## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}

タイプ: Bool

デフォルト値: 0

IPv4 への CAST 演算子、IPv6 への CAST 演算子、toIPv4、toIPv6 関数は、変換エラー時に例外をスローする代わりにデフォルト値を返します。

## cast_keep_nullable {#cast_keep_nullable}

タイプ: Bool

デフォルト値: 0

[CAST](../../sql-reference/functions/type-conversion-functions.md/#castx-t) 操作における `Nullable` データ型の保持を有効または無効にします。

設定が有効になっている場合、`CAST` 関数の引数が `Nullable` であれば、結果も `Nullable` 型に変換されます。設定が無効の場合、結果は常に正確に宛先型になります。

可能な値:

- 0 — `CAST` 結果は指定された宛先型と正確に一致します。
- 1 — 引数の型が `Nullable` の場合、`CAST` 結果は `Nullable(DestinationDataType)` に変換されます。

**例**

以下のクエリは、宛先データ型を正確に返します。

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果:

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

以下のクエリは、宛先データ型に `Nullable` 修飾が適用されます。

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果:

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**参照**

- [CAST](../../sql-reference/functions/type-conversion-functions.md/#type_conversion_function-cast) 関数

## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference}

タイプ: Bool

デフォルト値: 0

文字列から動的への変換中に型の推論を使用します。

## check_query_single_value_result {#check_query_single_value_result}

タイプ: Bool

デフォルト値: 1

`MergeTree` ファミリーエンジンの [CHECK TABLE](../../sql-reference/statements/check-table.md/#checking-mergetree-tables) クエリ結果に対する詳細レベルを定義します。

可能な値:

- 0 — クエリはテーブルの各データパートのチェックステータスを表示します。
- 1 — クエリは一般的なテーブルチェックステータスを表示します。

## check_referential_table_dependencies {#check_referential_table_dependencies}

タイプ: Bool

デフォルト値: 0

DDL クエリ（例えば DROP TABLE や RENAME）が参照依存関係を壊さないことを確認します。

## check_table_dependencies {#check_table_dependencies}

タイプ: Bool

デフォルト値: 1

DDL クエリ（例えば DROP TABLE や RENAME）が依存関係を壊さないことを確認します。

## checksum_on_read {#checksum_on_read}

タイプ: Bool

デフォルト値: 1

読み取り時にチェックサムを検証します。これはデフォルトで有効になっており、本番環境では常に有効にしておくべきです。設定を無効にしても利点は期待できません。これは実験やベンチマークにのみ使用される可能性があります。この設定は MergeTree ファミリーのテーブルにのみ適用され、他のテーブルエンジンやネットワーク経由でデータを受信する場合は常にチェックサムが検証されます。

## cloud_mode {#cloud_mode}

タイプ: Bool

デフォルト値: 0

クラウドモード。

## cloud_mode_database_engine {#cloud_mode_database_engine}

タイプ: UInt64

デフォルト値: 1

クラウドで許可されるデータベースエンジン。1 - DDL を使用して Replicated データベースに書き換える、2 - DDL を使用して Shared データベースに書き換える。

## cloud_mode_engine {#cloud_mode_engine}

タイプ: UInt64

デフォルト値: 1

クラウドで許可されるエンジンファミリー。0 - すべてを許可、1 - DDL を使用して *ReplicatedMergeTree に書き換え、2 - DDL を使用して SharedMergeTree に書き換え。UInt64 により公開部分を最小化します。

## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}

タイプ: String

デフォルト値:

現在のサーバーが位置するシャードのクラスター。

## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}

タイプ: Bool

デフォルト値: 1

メモリ割り当てを最適化するために、ハッシュテーブルの統計を収集することを有効にします。

## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins}

タイプ: Bool

デフォルト値: 1

メモリ割り当てを最適化するために、ハッシュテーブルの統計を収集することを有効にします。

## compatibility {#compatibility}

タイプ: String

デフォルト値:

`compatibility` 設定は ClickHouse に前のバージョンのデフォルト設定を使用させます。前のバージョンは設定として提供されます。

設定がデフォルト値以外に設定されている場合、それらの設定は尊重されます（修正されていない設定のみが `compatibility` 設定の影響を受けます）。

この設定は ClickHouse バージョン番号を文字列として受け取ります。例えば、`22.3`、`22.8` 等。空の値はこの設定が無効であることを意味します。

デフォルトでは無効です。

:::note
ClickHouse Cloud では、互換性設定は ClickHouse Cloud サポートによって設定されなければなりません。設定を行うには [ケースを開いてください](https://clickhouse.cloud/support)。
:::

## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table}

タイプ: Bool

デフォルト値: 0

true の場合、カラム宣言の AUTO_INCREMENT キーワードを無視します。そうでなければエラーを返します。これは MySQL からの移行を簡素化します。

## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table}

タイプ: Bool

デフォルト値: 1

テーブル作成時の照合の互換性を無視します。

## compile_aggregate_expressions {#compile_aggregate_expressions}

タイプ: Bool

デフォルト値: 1

集計関数をネイティブコードに JIT コンパイルすることを有効または無効にします。この設定を有効にすると、パフォーマンスが向上する可能性があります。

可能な値:

- 0 — 集計は JIT コンパイルなしで行われます。
- 1 — 集計は JIT コンパイルを使用して行われます。

**参照**

- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)

## compile_expressions {#compile_expressions}

タイプ: Bool

デフォルト値: 0

いくつかのスカラー関数と演算子をネイティブコードにコンパイルします。LLVM コンパイラインフラストラクチャのバグにより、AArch64 マシンで nullptr 参照解除を引き起こし、その結果、サーバーがクラッシュすることが知られています。この設定は有効にしないでください。

## compile_sort_description {#compile_sort_description}

タイプ: Bool

デフォルト値: 1

ソート説明をネイティブコードにコンパイルします。

## connect_timeout {#connect_timeout}

タイプ: Seconds

デフォルト値: 10

レプリカがない場合の接続タイムアウト。

## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}

タイプ: Milliseconds

デフォルト値: 1000

分散テーブルエンジンにおけるリモートサーバー接続のタイムアウト（ミリ秒）。`shard` および `replica` セクションがクラスター定義に使用されている場合。
接続に失敗した場合、さまざまなレプリカへの複数の接続試行が行われます。

## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}

タイプ: Milliseconds

デフォルト値: 1000

最初の健全なレプリカを選択するための接続タイムアウト（セキュアな接続用）。

## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}

タイプ: Milliseconds

デフォルト値: 0

接続プールが満杯のときに接続を待つ時間（ミリ秒）。

可能な値:

- 正の整数。
- 0 — 無限タイムアウト。

## connections_with_failover_max_tries {#connections_with_failover_max_tries}

タイプ: UInt64

デフォルト値: 3

分散テーブルエンジンにおける各レプリカとの接続試行の最大数。

## convert_query_to_cnf {#convert_query_to_cnf}

タイプ: Bool

デフォルト値: 0

`true` に設定すると、`SELECT` クエリが結合標準形（CNF）に変換されます。CNF にクエリを書き換えることで実行が速くなるシナリオがあります（変更の説明についてはこの [Github issue](https://github.com/ClickHouse/ClickHouse/issues/11749) を参照してください）。

以下の `SELECT` クエリが変更されないことに注意してください（デフォルト動作）：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

結果は次のようになります：

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

`convert_query_to_cnf` を `true` に設定して、変更を確認しましょう：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

`WHERE` 句が CNF で書き換えられていることに気づきますが、結果セットは同じです。論理は変更されていません：

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能な値: true, false

## count_distinct_implementation {#count_distinct_implementation}

タイプ: String

デフォルト値: uniqExact

[CATEGORY(DISTINCT ...)](../../sql-reference/aggregate-functions/reference/count.md/#agg_function-count) 構文を実行するために使用する `uniq*` 関数の指定を行います。

可能な値:

- [uniq](../../sql-reference/aggregate-functions/reference/uniq.md/#agg_function-uniq)
- [uniqCombined](../../sql-reference/aggregate-functions/reference/uniqcombined.md/#agg_function-uniqcombined)
- [uniqCombined64](../../sql-reference/aggregate-functions/reference/uniqcombined64.md/#agg_function-uniqcombined64)
- [uniqHLL12](../../sql-reference/aggregate-functions/reference/uniqhll12.md/#agg_function-uniqhll12)
- [uniqExact](../../sql-reference/aggregate-functions/reference/uniqexact.md/#agg_function-uniqexact)

## count_distinct_optimization {#count_distinct_optimization}

タイプ: Bool

デフォルト値: 0

distinct のカウントを書き換えてグループ化のサブクエリとします。

## create_if_not_exists {#create_if_not_exists}

タイプ: Bool

デフォルト値: 0

`CREATE` 文でデフォルトで `IF NOT EXISTS` を有効にします。この設定または `IF NOT EXISTS` が指定され、提供された名前のテーブルがすでに存在する場合、例外はスローされません。

## create_index_ignore_unique {#create_index_ignore_unique}

タイプ: Bool

デフォルト値: 0

CREATE UNIQUE INDEX における UNIQUE キーワードを無視します。SQL 互換性のテスト用に作成されました。

## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability}

タイプ: Float

デフォルト値: 0

メタデータを ZooKeeper に作成した後にテーブルを作成する際の障害挿入の確率。

## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default}

タイプ: Bool

デフォルト値: 0

ORDER BY および PRIMARY KEY が指定されない場合に、*MergeTree テーブルを空の主キーで作成することを許可します。

## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress}

タイプ: UInt64

デフォルト値: 1073741824

CROSS JOIN で圧縮するためのブロックの最小サイズ。ゼロ値はこのしきい値を無効にします。このブロックは、行またはバイトのいずれかのしきい値に達したときに圧縮されます。

## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress}

タイプ: UInt64

デフォルト値: 10000000

CROSS JOIN で圧縮するブロックの最小行数。ゼロ値はこのしきい値を無効にします。このブロックは、行またはバイトのいずれかのしきい値に達したときに圧縮されます。

## data_type_default_nullable {#data_type_default_nullable}

タイプ: Bool

デフォルト値: 0

カラム定義に明示的な修飾子 [NULL または NOT NULL](../../sql-reference/statements/create/table.md/#null-modifiers) がないデータ型が [Nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable) であることを許可します。

可能な値:

- 1 — カラム定義のデータ型がデフォルトで `Nullable` に設定されます。
- 0 — カラム定義のデータ型がデフォルトで `Nullable` ではないように設定されます。

## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}

タイプ: Bool

デフォルト値: 0

すべての `DROP` および `DETACH` クエリに `SYNC` 修飾子を追加します。

可能な値:

- 0 — クエリは遅延して実行されます。
- 1 — クエリは遅延なしで実行されます。

## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid}

タイプ: UInt64

デフォルト値: 0

0 - 複製データベース内のテーブルに対して UUID を明示的に指定することを許可しません。1 - 許可します。2 - 許可しますが、指定された UUID を無視してランダムなものを生成します。

## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create}

タイプ: Bool

デフォルト値: 0

複製データベースエンジンでの長時間実行される DDL クエリ（CREATE AS SELECT および POPULATE）を許可します。これは DDL キューを長時間ブロックする可能性があります。

## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine}

タイプ: Bool

デフォルト値: 0

Replicated エンジンのデータベース内でのみ Replicated テーブルを作成することを許可します。

## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments}

タイプ: UInt64

デフォルト値: 0

0 - 複製データベース内の *MergeTree テーブルに対して ZooKeeper パスとレプリカ名を明示的に指定することを許可しません。1 - 許可します。2 - 許可しますが、指定されたパスを無視してデフォルトのものを使用します。3 - 許可し、警告をログに記録しません。

## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}

タイプ: Bool

デフォルト値: 0

データベースエンジンが Replicated の場合、DETACH TABLE を DETACH TABLE PERMANENTLY として実行します。

## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings}

タイプ: Bool

デフォルト値: 0

一部のクエリに対する同期待機を強制します（データベースの原子的な削除と切り離し、mutation_sync、alter_sync も参照）。これらの設定を有効にすることは推奨されません。

## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}

タイプ: UInt64

デフォルト値: 300

初期 DDL クエリが複製データベースにおいて前の DDL キューエントリを処理するのを待つ時間を秒単位で設定します。

可能な値:

- 正の整数。
- 0 — 無限。

## decimal_check_overflow {#decimal_check_overflow}

タイプ: Bool

デフォルト値: 1

小数の算術/比較操作のオーバーフローをチェックします。

## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}

タイプ: Bool

デフォルト値: 0

Replicated* テーブルからデータを受け取るマテリアライズドビューに対する重複排除チェックを有効または無効にします。

可能な値：

0 — 無効。
1 — 有効。

使用

デフォルトでは、マテリアライズドビューに対する重複排除は行われませんが、ソーステーブルで行われます。
ソーステーブルで重複排除のために挿入されたブロックがスキップされると、付随的なマテリアライズドビューへの挿入は行われません。この動作は、集約されたデータが異なる INSERT から得られている場合でもマテリアライズドビューに挿入できることを可能にします。
同時に、この挙動は `INSERT` の冪等性を「破ります」。主テーブルへの `INSERT` が成功し、マテリアライズドビューへの `INSERT` が失敗した場合（例: ClickHouse Keeper との通信に失敗）、クライアントはエラーを受け取り操作を再試行できます。しかし、最初の失敗により失われた行を挿入することは、ソーステーブルの重複排除によって棄却されます。設定 `deduplicate_blocks_in_dependent_materialized_views` は、この動作を変更することを可能にします。再試行時に、マテリアライズドビューは再挿入を受け取り、自身で重複排除チェックを行います。
ソーステーブルのチェック結果を無視し、最初の失敗により失った行を挿入します。

## default_materialized_view_sql_security {#default_materialized_view_sql_security}

タイプ: SQLSecurityType

デフォルト値: DEFINER

マテリアライズドビュー作成時に SQL SECURITY オプションのデフォルト値を設定することを許可します。 [SQL セキュリティの詳細](../../sql-reference/statements/create/view.md#sql_security)

デフォルト値は `DEFINER` です。

## default_max_bytes_in_join {#default_max_bytes_in_join}

タイプ: UInt64

デフォルト値: 1000000000

制限が必要な場合、右側のテーブルの最大サイズ。ただし max_bytes_in_join が設定されていない場合。

## default_normal_view_sql_security {#default_normal_view_sql_security}

タイプ: SQLSecurityType

デフォルト値: INVOKER

通常のビューを作成する際のデフォルト `SQL SECURITY` オプションを設定することを許可します。 [SQL セキュリティの詳細](../../sql-reference/statements/create/view.md#sql_security)。

デフォルト値は `INVOKER` です。

## default_table_engine {#default_table_engine}

タイプ: DefaultTableEngine

デフォルト値: MergeTree

`CREATE` 文で `ENGINE` が設定されていない場合のデフォルトテーブルエンジン。

可能な値:

- 有効なテーブルエンジン名を表す文字列。

クラウドのデフォルト値: `SharedMergeTree`。

**例**

クエリ：

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

結果：

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

この例では、エンジンを指定していない新しいテーブルは、`Log` テーブルエンジンを使用します：

クエリ：

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

結果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_temporary_table_engine {#default_temporary_table_engine}

タイプ: DefaultTableEngine

デフォルト値: Memory

一時テーブルのための [default_table_engine](#default_table_engine) と同じ。

この例では、エンジンを指定していない新しい一時テーブルは、`Log` テーブルエンジンを使用します：

クエリ：

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

結果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_view_definer {#default_view_definer}

タイプ: String

デフォルト値: CURRENT_USER

ビュー作成時にデフォルト `DEFINER` オプションを設定することを許可します。 [SQL セキュリティの詳細](../../sql-reference/statements/create/view.md#sql_security)

デフォルト値は `CURRENT_USER` です。

## describe_compact_output {#describe_compact_output}

タイプ: Bool

デフォルト値: 0

true の場合、DESCRIBE クエリの結果に列名と型のみを含めます。

## describe_extend_object_types {#describe_extend_object_types}

タイプ: Bool

デフォルト値: 0

DESCRIBE クエリでオブジェクト型の列の具体的な型を推測します。

## describe_include_subcolumns {#describe_include_subcolumns}

タイプ: Bool

デフォルト値: 0

[DESCRIBE](../../sql-reference/statements/describe-table.md) クエリのためにサブカラムの記述を有効にします。例えば、[Tuple](../../sql-reference/data-types/tuple.md) のメンバーや、[Map](../../sql-reference/data-types/map.md/#map-subcolumns)、[Nullable](../../sql-reference/data-types/nullable.md/#finding-null) または [Array](../../sql-reference/data-types/array.md/#array-size) データ型のサブカラム。

可能な値:

- 0 — サブカラムは `DESCRIBE` クエリに含まれません。
- 1 — サブカラムは `DESCRIBE` クエリに含まれます。

**例**

[DESCRIBE](../../sql-reference/statements/describe-table.md) ステートメントの例を参照。

## describe_include_virtual_columns {#describe_include_virtual_columns}

タイプ: Bool

デフォルト値: 0

true の場合、DESCRIBE クエリの結果にテーブルの仮想カラムが含まれます。

## dialect {#dialect}

タイプ: Dialect

デフォルト値: clickhouse

クエリを解析する際に使用されるダイアレクト。

## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type}

タイプ: Bool

デフォルト値: 0

Dictionaryのプライマリキータイプを検証します。デフォルトでは、シンプルなレイアウトの場合、id タイプは暗黙的に UInt64 に変換されます。

## distinct_overflow_mode {#distinct_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合の動作を指定します。

## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient}

タイプ: Bool

デフォルト値: 1

分散集計のメモリ節約モードが有効になっています。

## distributed_background_insert_batch {#distributed_background_insert_batch}

タイプ: Bool

デフォルト値: 0

挿入データのバッチ送信を有効または無効にします。

バッチ送信が有効な場合、[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンは、挿入データの複数のファイルを1つの操作で送信しようとします。バッチ送信はネットワークおよびサーバーリソースの効率を高め、クラスターのパフォーマンスを向上させます。

可能な値：

- 1 — 有効。
- 0 — 無効。
## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms}

タイプ: ミリ秒

デフォルト値: 30000

[分散](../../engines/table-engines/special/distributed.md)テーブルエンジンがデータを送信するための最大間隔です。これは、[distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms)設定で設定された間隔の指数的な成長を制限します。

可能な値:

- 正の整数のミリ秒。

## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms}

タイプ: ミリ秒

デフォルト値: 100

[分散](../../engines/table-engines/special/distributed.md)テーブルエンジンがデータを送信するための基本間隔です。エラーが発生した場合、実際の間隔は指数的に増加します。

可能な値:

- 正の整数のミリ秒。

## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure}

タイプ: ブール

デフォルト値: 0

失敗時にバッチを分割するかどうかを有効または無効にします。

特定のバッチをリモートシャードに送信する際に、`Memory limit exceeded`や類似のエラーによって失敗することがあります。この場合、リトライしても解決しない(このためテーブルの分散送信が停止します)が、そのバッチからのファイルを1つずつ送信してINSERTを成功させることができるかもしれません。

したがって、この設定を`1`にすると、そのようなバッチについてバッチ処理を無効にします（すなわち、失敗したバッチのために一時的に`distributed_background_insert_batch`を無効化します）。

可能な値:

- 1 — 有効。
- 0 — 無効。

:::note
この設定は、異常なサーバー（マシン）の停止と[分散](../../engines/table-engines/special/distributed.md)テーブルエンジンの`fsync_after_insert` / `fsync_directories`がない場合に発生する可能性のある壊れたバッチにも影響します。
:::

:::note
自動バッチ分割に依存しない方が良いでしょう。これはパフォーマンスに悪影響を及ぼす可能性があります。
:::

## distributed_background_insert_timeout {#distributed_background_insert_timeout}

タイプ: UInt64

デフォルト値: 0

分散へのINSERTクエリのタイムアウトです。この設定は、insert_distributed_syncが有効な場合にのみ使用されます。ゼロ値はタイムアウトなしを意味します。

## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool}

タイプ: ブール

デフォルト値: 0

ClickHouse Cloudのみ。分散キャッシュ接続プールをバイパスすることを許可します。

## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries}

タイプ: UInt64

デフォルト値: 100

ClickHouse Cloudのみ。分散キャッシュへの接続が失敗した場合の接続試行回数です。

## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window}

タイプ: UInt64

デフォルト値: 5

ClickHouse Cloudのみ。単一の分散キャッシュ読み取りリクエストにおけるDataPacketシーケンスのACK送信に対するウィンドウサイズです。

## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data}

タイプ: ブール

デフォルト値: 1

ClickHouse Cloudのみ。一部のデータが未読の場合、接続を破棄します。

## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az}

タイプ: ブール

デフォルト値: 1

ClickHouse Cloudのみ。system.distributed_cache_metricsやsystem.distributed_cache_eventsから現在のアベイラビリティゾーンのみからメトリクスを取得します。

## distributed_cache_log_mode {#distributed_cache_log_mode}

タイプ: DistributedCacheLogMode

デフォルト値: on_error

ClickHouse Cloudのみ。system.distributed_cache_logへの書き込みモードです。

## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets}

タイプ: UInt64

デフォルト値: 10

ClickHouse Cloudのみ。単一の分散キャッシュ読み取りリクエストにおける未確認の飛行中パケットの最大数です。

## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit}

タイプ: DistributedCachePoolBehaviourOnLimit

デフォルト値: allocate_bypassing_pool

ClickHouse Cloudのみ。プールの制限に達したときの分散キャッシュ接続の動作を特定します。

## distributed_cache_read_alignment {#distributed_cache_read_alignment}

タイプ: UInt64

デフォルト値: 0

ClickHouse Cloudのみ。テスト目的の設定であり、変更しないでください。

## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds}

タイプ: UInt64

デフォルト値: 60000

ClickHouse Cloudのみ。分散キャッシュからリクエストのデータを受信するための待機時間（ミリ秒単位）です。

## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds}

タイプ: UInt64

デフォルト値: 10000

ClickHouse Cloudのみ。分散キャッシュからの応答を受信するための待機時間（ミリ秒単位）です。

## distributed_cache_throw_on_error {#distributed_cache_throw_on_error}

タイプ: ブール

デフォルト値: 0

ClickHouse Cloudのみ。分散キャッシュとの通信中に発生した例外または分散キャッシュから受信した例外を再スローします。そうでない場合は、エラー時に分散キャッシュのスキップにフォールバックします。

## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds}

タイプ: UInt64

デフォルト値: 100

ClickHouse Cloudのみ。分散_cache_pool_behaviour_on_limitがwaitの場合に、接続プールから接続を受け取るための待機時間（ミリ秒単位）です。

## distributed_connections_pool_size {#distributed_connections_pool_size}

タイプ: UInt64

デフォルト値: 1024

分散されたすべてのクエリを単一のDistributedテーブルに対してリモートサーバーとの同時接続の最大数です。クラスタ内のサーバーの数以上の値を設定することをお勧めします。

## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version}

タイプ: UInt64

デフォルト値: 5

分散DDL (ON CLUSTER)クエリの互換性バージョンです。

## distributed_ddl_output_mode {#distributed_ddl_output_mode}

タイプ: DistributedDDLOutputMode

デフォルト値: throw

分散DDLクエリ結果のフォーマットを設定します。

可能な値:

- `throw` — クエリが終了したすべてのホストに対してクエリ実行ステータスを持つ結果セットを返します。クエリが一部のホストで失敗した場合、最初の例外を再スローします。すべてのホストでクエリがまだ終了していない場合、[distributed_ddl_task_timeout](#distributed_ddl_task_timeout)を超えた場合、`TIMEOUT_EXCEEDED`例外をスローします。
- `none` — throwに似ていますが、分散DDLクエリは結果セットを返しません。
- `null_status_on_timeout` — 一部の結果セットの行で実行ステータスとして`NULL`を返します。これは、クエリが対応するホストでまだ終了していない場合、`TIMEOUT_EXCEEDED`をスローするのではなく返します。
- `never_throw` — `TIMEOUT_EXCEEDED`をスローせず、一部のホストでクエリが失敗した場合も例外を再スローしません。
- `none_only_active` — `none`に似ていますが、`Replicated`データベースの非アクティブなレプリカを待ちません。注意: このモードでは、クエリが一部のレプリカで実行されないことを特定することはできず、バックグラウンドで実行されます。
- `null_status_on_timeout_only_active` — `null_status_on_timeout`に似ていますが、`Replicated`データベースの非アクティブなレプリカを待ちません。
- `throw_only_active` — `throw`に似ていますが、`Replicated`データベースの非アクティブなレプリカを待ちません。

クラウドのデフォルト値: `none`。

## distributed_ddl_task_timeout {#distributed_ddl_task_timeout}

タイプ: Int64

デフォルト値: 180

クラスタ内のすべてのホストからのDDLクエリ応答のタイムアウトを設定します。すべてのホストでDDLリクエストが実行されなかった場合、応答にはタイムアウトエラーが含まれ、リクエストは非同期モードで実行されます。負の値は無限を意味します。

可能な値:

- 正の整数。
- 0 — 非同期モード。
- 負の整数 — 無限タイムアウト。

## distributed_foreground_insert {#distributed_foreground_insert}

タイプ: ブール

デフォルト値: 0

[分散](../../engines/table-engines/special/distributed.md/#distributed)テーブルへの同期データ挿入を有効または無効にします。

デフォルトでは、`Distributed`テーブルにデータを挿入する際にClickHouseサーバーはバックグラウンドモードでクラスターノードにデータを送信します。`distributed_foreground_insert=1`の場合、データは同期処理され、すべてのデータがすべてのシャードに保存されるまで`INSERT`操作は成功しません（`internal_replication`がtrueの場合、各シャードに少なくとも1つのレプリカが必要です）。

可能な値:

- 0 — データはバックグラウンドモードで挿入されます。
- 1 — データは同期モードで挿入されます。

クラウドのデフォルト値: `1`。

**関連項目**

- [分散テーブルエンジン](../../engines/table-engines/special/distributed.md/#distributed)
- [分散テーブルの管理](../../sql-reference/statements/system.md/#query-language-system-distributed)

## distributed_group_by_no_merge {#distributed_group_by_no_merge}

タイプ: UInt64

デフォルト値: 0

分散クエリ処理のために異なるサーバーからの集計状態をマージしないようにします。異なるシャードに異なるキーがあることが確実な場合に使用できます。

可能な値:

- `0` — 無効（最終クエリ処理はイニシエータノードで行われます）。
- `1` - 異なるサーバーからの集計状態をマージせずに分散クエリ処理を行います（クエリはシャード上で完全に処理され、イニシエータはデータをリレーします）。これは、異なるシャードに異なるキーがあることが確実な場合に使用できます。
- `2` - `1`と同じですが、イニシエータの上で`ORDER BY`と`LIMIT`を適用します（クエリがリモートノードで完全に処理される場合には不可能です。例: `distributed_group_by_no_merge=1`）。

**例**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```

## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas}

タイプ: ブール

デフォルト値: 0

DistributedへのINSERTクエリ用に読み取り専用レプリカをスキップすることを有効または無効にします。

可能な値:

- 0 — 通常通りINSERTが行われ、読み取り専用レプリカに送信されると失敗します。
- 1 — イニシエータはデータをシャードに送信する前に、読み取り専用レプリカをスキップします。

## distributed_product_mode {#distributed_product_mode}

タイプ: DistributedProductMode

デフォルト値: deny

[分散サブクエリ](../../sql-reference/operators/in.md)の動作を変更します。

ClickHouseは、クエリが分散テーブルの非GLOBALサブクエリを含む場合にこの設定を適用します。

制限:

- INおよびJOINサブクエリにのみ適用されます。
- FROMセクションが複数のシャードを含む分散テーブルを使用している場合にのみ適用されます。
- サブクエリが複数のシャードを含む分散テーブルに関係する場合。
- テーブル値[remote](../../sql-reference/table-functions/remote.md)関数には使用されません。

可能な値:

- `deny` — デフォルト値。これらのタイプのサブクエリの使用を禁止します（「Double-distributed in/JOIN subqueries is denied」例外を返します）。
- `local` — サブクエリ内のデータベースとテーブルを宛先サーバー（シャード）のローカルテーブルに置き換えます（通常の`IN`/`JOIN`を置き換えます）。
- `global` — `IN`/`JOIN`クエリを`GLOBAL IN`/`GLOBAL JOIN`に置き換えます。
- `allow` — これらのタイプのサブクエリの使用を許可します。

## distributed_push_down_limit {#distributed_push_down_limit}

タイプ: UInt64

デフォルト値: 1

各シャードに別々に[LIMIT](#limit)を適用するかどうかを有効または無効にします。

これにより、次のことを回避できます：
- ネットワーク越しに追加の行を送信すること。
- イニシエータでリミットの後に行を処理すること。

21.9バージョンから、少なくとも1つの条件が満たされた場合にのみ`distributed_push_down_limit`がクエリ実行を変更します:
- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- クエリが`GROUP BY`/`DISTINCT`/`LIMIT BY`を持たず、ですが`ORDER BY`/`LIMIT`を持つ。
- クエリが`ORDER BY`/`LIMIT`を持つ`GROUP BY`/`DISTINCT`/`LIMIT BY`を持ち、且つ：
    - [optimize_skip_unused_shards](#optimize-skip-unused-shards)が有効です。
    - [optimize_distributed_group_by_sharding_key](#optimize-distributed-group-by-sharding-key)が有効です。

可能な値:

- 0 — 無効。
- 1 — 有効。

関連項目:

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize-skip-unused-shards)
- [optimize_distributed_group_by_sharding_key](#optimize-distributed-group-by-sharding-key)

## distributed_replica_error_cap {#distributed_replica_error_cap}

タイプ: UInt64

デフォルト値: 1000

- タイプ: 符号なし整数
- デフォルト値: 1000

各レプリカのエラー数がこの値に制限され、1つのレプリカがあまりにも多くのエラーを蓄積するのを防ぎます。

関連項目:

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン分散](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_error_half_life {#distributed_replica_error_half_life}

タイプ: 秒

デフォルト値: 60

- タイプ: 秒
- デフォルト値: 60秒

分散テーブルでのエラーがゼロになる速度を制御します。レプリカがしばらく利用できない場合、5つのエラーを蓄積し、distributed_replica_error_half_lifeが1秒に設定されている場合、最後のエラーの後3秒後にレプリカは正常と見なされます。

関連項目:

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン分散](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors}

タイプ: UInt64

デフォルト値: 0

- タイプ: 符号なし整数
- デフォルト値: 0

レプリカを選択する際に無視されるエラーの数（`load_balancing`アルゴリズムに従って）。

関連項目:

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン分散](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)

## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final}

タイプ: ブール

デフォルト値: 0

SELECT FINALで1つのパーティション内のみでパーツをマージします。

## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set}

タイプ: ブール

デフォルト値: 1

空セットに対して定数キーによる集計時、空の結果を返します。

## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set}

タイプ: ブール

デフォルト値: 0

空セットに対してキーなしに集計を行うと、空の結果を返します。

## enable_blob_storage_log {#enable_blob_storage_log}

タイプ: ブール

デフォルト値: 1

blobストレージ操作に関する情報をsystem.blob_storage_logテーブルに書き込みます。

## enable_early_constant_folding {#enable_early_constant_folding}

タイプ: ブール

デフォルト値: 1

関数やサブクエリの結果を分析し、定数が含まれる場合にクエリを書き換えることで、クエリ最適化を有効にします。

## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions}

タイプ: ブール

デフォルト値: 0

次の型の結果を返すことを有効または無効にします：
- `Date32`は、[toStartOfYear](../../sql-reference/functions/date-time-functions.md#tostartofyear)、[toStartOfISOYear](../../sql-reference/functions/date-time-functions.md#tostartofisoyear)、[toStartOfQuarter](../../sql-reference/functions/date-time-functions.md#tostartofquarter)、[toStartOfMonth](../../sql-reference/functions/date-time-functions.md#tostartofmonth)、[toLastDayOfMonth](../../sql-reference/functions/date-time-functions.md#tolastdayofmonth)、[toStartOfWeek](../../sql-reference/functions/date-time-functions.md#tostartofweek)、[toLastDayOfWeek](../../sql-reference/functions/date-time-functions.md#tolastdayofweek)、および[toMonday](../../sql-reference/functions/date-time-functions.md#tomonday)の関数に対して拡張された範囲（`Date`型と比較して）。
- `DateTime64`は、[toStartOfDay](../../sql-reference/functions/date-time-functions.md#tostartofday)、[toStartOfHour](../../sql-reference/functions/date-time-functions.md#tostartofhour)、[toStartOfMinute](../../sql-reference/functions/date-time-functions.md#tostartofminute)、[toStartOfFiveMinutes](../../sql-reference/functions/date-time-functions.md#tostartoffiveminutes)、[toStartOfTenMinutes](../../sql-reference/functions/date-time-functions.md#tostartoftenminutes)、[toStartOfFifteenMinutes](../../sql-reference/functions/date-time-functions.md#tostartoffifteenminutes)、および[timeSlot](../../sql-reference/functions/date-time-functions.md#timeslot)の関数に対して拡張された範囲（`DateTime`型と比較して）。

可能な値:

- 0 — 関数はすべての型の引数に対して`Date`または`DateTime`を返します。
- 1 — 関数は`Date32`または`DateTime64`引数に対して`Date32`または`DateTime64`を返し、そうでない場合は`Date`または`DateTime`を返します。

## enable_filesystem_cache {#enable_filesystem_cache}

タイプ: ブール

デフォルト値: 1

リモートファイルシステムのキャッシュを使用します。この設定は、ディスクのキャッシュのON/OFFを切り替えるものではありません（ディスク設定で行う必要があります）が、意図した場合に一部のクエリでキャッシュをバイパスできるようにします。

## enable_filesystem_cache_log {#enable_filesystem_cache_log}

タイプ: ブール

デフォルト値: 0

各クエリに対するファイルシステムキャッシュのログを記録することを許可します。

## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations}

タイプ: ブール

デフォルト値: 0

書き込み操作中にキャッシュに書き込む。実際に機能するためには、この設定もディスク設定に追加される必要があります。

## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log}

タイプ: ブール

デフォルト値: 0

クエリ中にsystem.filesystemのprefetch_logにログを記録します。テストまたはデバッグのみに使用することを推奨し、デフォルトでオンにしないことをお勧めします。

## enable_global_with_statement {#enable_global_with_statement}

タイプ: ブール

デフォルト値: 1

UNIONクエリおよびすべてのサブクエリにWITHステートメントを伝播させます。

## enable_http_compression {#enable_http_compression}

タイプ: ブール

デフォルト値: 0

HTTPリクエストに対する応答でのデータ圧縮を有効または無効にします。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

可能な値:

- 0 — 無効。
- 1 — 有効。

## enable_job_stack_trace {#enable_job_stack_trace}

タイプ: ブール

デフォルト値: 1

ジョブの作成者によるスタックトレースを出力し、ジョブが例外を引き起こした場合。

## enable_lightweight_delete {#enable_lightweight_delete}

タイプ: ブール

デフォルト値: 1

MergeTreeテーブルに対して論理削除を有効にします。

## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results}

タイプ: ブール

デフォルト値: 1

集計のためのメモリバウンドマージ戦略を有効にします。

## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps}

タイプ: ブール

デフォルト値: 1

WHEREからPREWHEREへの条件をより多く移動させ、複数の条件がANDで組み合わされている場合にディスクからの読み取りとフィルタリングを複数のステップで行います。

## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple}

タイプ: ブール

デフォルト値: 0

すべての名前がユニークで、引用なし識別子として扱える場合、function tuple()で名前付きタプルを生成します。

## enable_optimize_predicate_expression {#enable_optimize_predicate_expression}

タイプ: ブール

デフォルト値: 1

`SELECT`クエリでの述語プッシュダウンをオンにします。

述語プッシュダウンは、分散クエリでのネットワークトラフィックを大幅に削減する可能性があります。

可能な値:

- 0 — 無効。
- 1 — 有効。

使用法

次のクエリを考えてみてください：

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

`enable_optimize_predicate_expression = 1`の場合、これらのクエリの実行時間は等しくなります。ClickHouseはサブクエリを処理する際に`WHERE`を適用します。

`enable_optimize_predicate_expression = 0`の場合、2番目のクエリの実行時間ははるかに長くなります。なぜなら、サブクエリが終了した後で、`WHERE`句がすべてのデータに適用されるからです。

## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery}

タイプ: ブール

デフォルト値: 1

最終サブクエリに述語のプッシュを許可します。

## enable_order_by_all {#enable_order_by_all}

タイプ: ブール

デフォルト値: 1

`ORDER BY ALL`構文を使用したソートを有効または無効にします。詳細は[ORDER BY](../../sql-reference/statements/select/order-by.md)を参照してください。

可能な値:

- 0 — ORDER BY ALLを無効にします。
- 1 — ORDER BY ALLを有効にします。

**例**

クエリ:

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- ALLが曖昧であるというエラーが返されます。

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

結果:

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴─────┴───────┘
```

## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization}

タイプ: ブール

デフォルト値: 1

真の場合、データはテーブルから得られたシリアル化のヒントに従ってカスタムシリアル化（例：Sparse）を持つ列に直接解析されます。

## enable_positional_arguments {#enable_positional_arguments}

タイプ: ブール

デフォルト値: 1

[GROUP BY](../../sql-reference/statements/select/group-by.md)、[LIMIT BY](../../sql-reference/statements/select/limit-by.md)、[ORDER BY](../../sql-reference/statements/select/order-by.md)ステートメントに対する位置引数をサポートするかどうかを有効または無効にします。

可能な値:

- 0 — 位置引数はサポートされません。
- 1 — 位置引数がサポートされます: 列番号を列名の代わりに使用できます。

**例**

クエリ:

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

結果:

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```

## enable_reads_from_query_cache {#enable_reads_from_query_cache}

タイプ: ブール

デフォルト値: 1

オンの場合、`SELECT`クエリの結果は[クエリキャッシュ](../query-cache.md)から取得されます。

可能な値:

- 0 - 無効
- 1 - 有効

## enable_s3_requests_logging {#enable_s3_requests_logging}

タイプ: ブール

デフォルト値: 0

S3リクエストの非常に明示的なログ記録を有効にします。デバッグ専用で意味があります。

## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization}

タイプ: ブール

デフォルト値: 1

真に設定されている場合、大きなスカラ値のスカラーサブクエリの（非）シリアル化を防止し、同じサブクエリを複数回実行するのを回避できます。

## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations}

タイプ: ブール

デフォルト値: 1

INサブクエリのために構築された共有セットオブジェクトを異なるミューテーションのタスク間で共有できます。これにより、メモリ使用量とCPU消費が削減されます。

## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation}

タイプ: ブール

デフォルト値: 1

集計でソフトウェアプリフェッチを使用することを有効にします。

## enable_unaligned_array_join {#enable_unaligned_array_join}

タイプ: ブール

デフォルト値: 0

異なるサイズの複数の配列を使用したARRAY JOINを許可します。この設定が有効な場合、配列は最も長いものにサイズ変更されます。

## enable_url_encoding {#enable_url_encoding}

タイプ: ブール

デフォルト値: 1

[URL](../../engines/table-engines/special/url.md)エンジンのテーブルのURIのパスのデコード/エンコードを有効または無効にします。

デフォルトでオンです。

## enable_vertical_final {#enable_vertical_final}

タイプ: ブール

デフォルト値: 1

これを有効にすると、最終的に行をマージするのではなく、削除されたとして行をマークし、後でフィルタリングして重複行を削除します。

## enable_writes_to_query_cache {#enable_writes_to_query_cache}

タイプ: ブール

デフォルト値: 1

オンの場合、`SELECT`クエリの結果は[クエリキャッシュ](../query-cache.md)に格納されます。

可能な値:

- 0 - 無効
- 1 - 有効

## enable_zstd_qat_codec {#enable_zstd_qat_codec}

タイプ: ブール

デフォルト値: 0

オンの場合、ZSTD_QATコーデックを使用して列を圧縮できます。

## enforce_strict_identifier_format {#enforce_strict_identifier_format}

タイプ: ブール

デフォルト値: 0

有効な場合、英数字とアンダースコアを含む識別子のみを許可します。

## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files}

タイプ: ブール

デフォルト値: 0

ファイルエンジンのテーブルで、フォーマットがサフィックス（`JSON`、`ORC`、`Parquet`など）を持つ場合、各挿入時に新しいファイルを作成することを有効または無効にします。有効な場合、各挿入ごとに次のパターンに従った名前の新しいファイルが作成されます：

`data.Parquet` -> `data.1.Parquet` -> `data.2.Parquet` など。

可能な値:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリは新しいファイルを作成します。

## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists}

タイプ: ブール

デフォルト値: 0

ファイルのないファイルエンジンテーブルからデータを選択することを許可します。

可能な値:
- 0 — `SELECT`は例外をスローします。
- 1 — `SELECT`は空の結果を返します。

## engine_file_skip_empty_files {#engine_file_skip_empty_files}

タイプ: ブール

デフォルト値: 0

[File](../../engines/table-engines/special/file.md)エンジンのテーブルで空のファイルをスキップすることを有効または無効にします。

可能な値:
- 0 — 空のファイルが要求されたフォーマットに適合しない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルの場合、`SELECT`は空の結果を返します。

## engine_file_truncate_on_insert {#engine_file_truncate_on_insert}

タイプ: ブール

デフォルト値: 0

[File](../../engines/table-engines/special/file.md)エンジンのテーブルで、挿入時に切り捨てを有効または無効にします。

可能な値:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリは新しいデータでファイルの既存の内容を置き換えます。

## engine_url_skip_empty_files {#engine_url_skip_empty_files}

タイプ: ブール

デフォルト値: 0

[URL](../../engines/table-engines/special/url.md)エンジンのテーブルで空のファイルをスキップすることを有効または無効にします。

可能な値:
- 0 — 空のファイルが要求されたフォーマットに適合しない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルの場合、`SELECT`は空の結果を返します。

## except_default_mode {#except_default_mode}

タイプ: SetOperationMode

デフォルト値: ALL

EXCEPTクエリのデフォルトモードを設定します。可能な値: 空文字列、'ALL'、'DISTINCT'。空の場合、モードなしでクエリが例外をスローします。

## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec}

タイプ: UInt64

デフォルト値: 10

接続のタイムアウト（秒単位）。現在はMySQLのみにサポートされています。

## external_storage_max_read_bytes {#external_storage_max_read_bytes}

タイプ: UInt64

デフォルト値: 0

外部エンジンを使用したテーブルが履歴データをフラッシュする最大バイト数を制限します。

## external_storage_max_read_rows {#external_storage_max_read_rows}

タイプ: UInt64

デフォルト値: 0

外部エンジンを持つテーブルが履歴データをフラッシュする際の最大行数を制限します。これは現在、MySQLテーブルエンジン、データベースエンジン、Dictionary、およびMaterializedMySQLにのみサポートされています。0に等しい場合、この設定は無効になります。

## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec}

タイプ: UInt64

デフォルト値: 300

読み取り/書き込みのタイムアウト（秒単位）。現在はMySQLのみにサポートされています。

## external_table_functions_use_nulls {#external_table_functions_use_nulls}

タイプ: ブール

デフォルト値: 1

[mysql](../../sql-reference/table-functions/mysql.md)、[postgresql](../../sql-reference/table-functions/postgresql.md)、および[odbc](../../sql-reference/table-functions/odbc.md)テーブル関数がNullableカラムを使用する方法を定義します。

可能な値:

- 0 — テーブル関数は明示的にNullableカラムを使用します。
- 1 — テーブル関数は暗黙的にNullableカラムを使用します。

**使用法**

設定が`0`に設定されている場合、テーブル関数はNullableカラムを作成せず、NULLの代わりにデフォルト値を挿入します。これは配列内のNULL値にも適用されます。

## external_table_strict_query {#external_table_strict_query}

タイプ: ブール

デフォルト値: 0

真に設定されている場合、外部テーブルへのクエリのローカルフィルターへの変換が禁止されます。

## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row}

タイプ: UInt64

デフォルト値: 1000

`extractKeyValuePairs`関数で生成できるペアの最大数です。これは、過剰なメモリ消費を防ぐためのセーフガードとして使用されます。

## extremes {#extremes}

タイプ: ブール

デフォルト値: 0

クエリ結果の列の極端な値（最小値および最大値）をカウントするかどうか。0または1を受け入れます。デフォルトは0（無効）です。
極端な値に関する詳細は、「極端な値」セクションを参照してください。

## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries}

タイプ: ブール

デフォルト値: 1

更新されたデータが利用できない場合、古いレプリカに対してクエリを強制します。[レプリケーション](../../engines/table-engines/mergetree-family/replication.md)を参照してください。

ClickHouseは、テーブルの古いレプリカの中から最も関連性の高いものを選択します。

これは、レプリケーションされたテーブルを指す分散テーブルからの`SELECT`を実行する際に使用されます。

デフォルトでは1（有効）です。

## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch}

タイプ: ブール

デフォルト値: 1
Only in ClickHouse Cloud. ファイルシステムキャッシュにおけるスペース予約のためのキャッシュロック待機時間

## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage}

タイプ: Bool

デフォルト値: 1

Only in ClickHouse Cloud. ファイルシステムキャッシュにおけるスペース予約のためのキャッシュロック待機時間

## filesystem_cache_max_download_size {#filesystem_cache_max_download_size}

タイプ: UInt64

デフォルト値: 137438953472

単一のクエリでダウンロードできる最大リモートファイルシステムキャッシュサイズ

## filesystem_cache_name {#filesystem_cache_name}

タイプ: String

デフォルト値:

ステートレステーブルエンジンまたはデータレイクに使用するファイルシステムキャッシュ名

## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds}

タイプ: UInt64

デフォルト値: 1000

ファイルシステムキャッシュにおけるスペース予約のためのキャッシュロック待機時間

## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size}

タイプ: UInt64

デフォルト値: 20

読み取りバッファがキャッシュから要求できる単一バッチのファイルセグメントのサイズ制限。値が低すぎるとキャッシュへのリクエストが過剰になり、高すぎるとキャッシュのエビクションを遅くする可能性がある。

## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage}

タイプ: UInt64

デフォルト値: 1073741824

プリフェッチの最大メモリ使用量。

## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes}

タイプ: UInt64

デフォルト値: 0

バイト単位のプリフェッチステップ。ゼロは`auto`を意味し、最適なプリフェッチステップは自動的に推測されるが、100%最良とは限らない。実際の値は、`filesystem_prefetch_min_bytes_for_single_read_task`の設定によって異なる可能性がある。

## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks}

タイプ: UInt64

デフォルト値: 0

マーク単位のプリフェッチステップ。ゼロは`auto`を意味し、最適なプリフェッチステップは自動的に推測されるが、100%最良とは限らない。実際の値は、`filesystem_prefetch_min_bytes_for_single_read_task`の設定によって異なる可能性がある。

## filesystem_prefetches_limit {#filesystem_prefetches_limit}

タイプ: UInt64

デフォルト値: 200

最大プリフェッチ数。ゼロは無制限を意味する。プリフェッチ数を制限したい場合は、`filesystem_prefetches_max_memory_usage`の設定を使用することが推奨される。

## final {#final}

タイプ: Bool

デフォルト値: 0

クエリ内のすべてのテーブルに自動的に[FINAL](../../sql-reference/statements/select/from.md#final-modifier)修飾子を適用し、[FINAL](../../sql-reference/statements/select/from.md#final-modifier)が適用可能なテーブル、結合テーブルおよびサブクエリ内のテーブル、分散テーブルにも適用される。

可能な値:

- 0 - 無効
- 1 - 有効

例:

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```

## flatten_nested {#flatten_nested}

タイプ: Bool

デフォルト値: 1

[ネスト](../../sql-reference/data-types/nested-data-structures/index.md)カラムのデータ形式を設定します。

可能な値:

- 1 — ネストされたカラムは別々の配列にフラット化される。
- 0 — ネストされたカラムはタプルの単一配列のままとなる。

**使用法**

設定が`0`に設定されている場合、任意のネストレベルを使用することができます。

**例**

クエリ:

``` sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

結果:

``` text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

クエリ:

``` sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

結果:

``` text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

## force_aggregate_partitions_independently {#force_aggregate_partitions_independently}

タイプ: Bool

デフォルト値: 0

適用可能な場合に最適化の使用を強制しますが、ヒューリスティックの判断により使用されない場合があります。

## force_aggregation_in_order {#force_aggregation_in_order}

タイプ: Bool

デフォルト値: 0

この設定は、サーバー自身が分散クエリをサポートするために使用します。通常の操作を破るため、手動で変更しないでください。（分散集計中にリモートノードで順序での集計の使用を強制します）。

## force_data_skipping_indices {#force_data_skipping_indices}

タイプ: String

デフォルト値:

指定されたデータスキッピングインデックスが使用されない場合、クエリの実行を無効にします。

以下の例を考えてみてください。

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- クエリは CANNOT_PARSE_TEXT エラーを生成します。
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- クエリは INDEX_NOT_USED エラーを生成します。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok.
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok （完全なパーサの例）。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- d1_null_idx が使用されていないため、クエリは INDEX_NOT_USED エラーを生成します。
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok.
```

## force_grouping_standard_compatibility {#force_grouping_standard_compatibility}

タイプ: Bool

デフォルト値: 1

GROUPING 関数が引数が集約キーとして使用されていない場合に 1 を返すようにします。

## force_index_by_date {#force_index_by_date}

タイプ: Bool

デフォルト値: 0

インデックスが日付によって使用できない場合、クエリの実行を無効にします。

MergeTree ファミリーのテーブルで機能します。

`force_index_by_date=1`の場合、ClickHouse は、クエリにデータ範囲を制限するために使用できる日付キー条件があるかどうかを確認します。適切な条件がない場合、例外がスローされます。ただし、条件が読み取るデータ量を減少させるかどうかは確認しません。たとえば、条件 `Date != ' 2000-01-01 '` は、テーブル内のすべてのデータに一致する場合でも受け入れられます（つまり、クエリの実行にフルスキャンが必要です）。MergeTree テーブル内のデータ範囲の詳細については、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_optimize_projection {#force_optimize_projection}

タイプ: Bool

デフォルト値: 0

`SELECT` クエリ内で[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)の必須使用を有効または無効にします。プロジェクション最適化が有効な場合（[optimize_use_projections](#optimize_use_projections)設定を参照）。

可能な値:

- 0 — プロジェクション最適化は必須ではありません。
- 1 — プロジェクション最適化は必須です。

## force_optimize_projection_name {#force_optimize_projection_name}

タイプ: String

デフォルト値:

非空の文字列に設定されている場合、このプロジェクションがクエリ内で少なくとも一度使用されていることを確認します。

可能な値:

- 文字列: クエリ内で使用されるプロジェクションの名前

## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards}

タイプ: UInt64

デフォルト値: 0

[optimize_skip_unused_shards](#optimize-skip-unused-shards) が有効で、未使用のシャードをスキップできない場合にクエリの実行を有効または無効にします。スキップが不可能で、設定が有効な場合は、例外がスローされます。

可能な値:

- 0 — 無効。ClickHouse は例外をスローしません。
- 1 — 有効。テーブルにシャーディングキーがある場合のみクエリの実行が無効になります。
- 2 — 有効。テーブルにシャーディングキーが定義されているかどうかにかかわらず、クエリの実行が無効になります。

## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting}

タイプ: UInt64

デフォルト値: 0

分散クエリのネストレベルに応じて[`force_optimize_skip_unused_shards`](#force-optimize-skip-unused-shards)を制御します（別の`Distributed`テーブルが別の`Distributed`テーブルを参照している場合）。

可能な値:

- 0 - 無効。`force_optimize_skip_unused_shards`は常に機能します。
- 1 — 最初のレベルのみに対して`force_optimize_skip_unused_shards`を有効にします。
- 2 — 第二レベルまで`force_optimize_skip_unused_shards`を有効にします。

## force_primary_key {#force_primary_key}

タイプ: Bool

デフォルト値: 0

プライマリキーによるインデックス作成が不可能な場合、クエリの実行を無効にします。

MergeTree ファミリーのテーブルで機能します。

`force_primary_key=1`の場合、ClickHouse は、クエリにデータ範囲を制限するために使用できるプライマリキー条件があるかどうかを確認します。適切な条件がない場合は例外がスローされます。ただし、条件が読み取るデータ量を減少させるかどうかは確認しません。MergeTree テーブル内のデータ範囲に関する詳細については、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop}

タイプ: Bool

デフォルト値: 0

DROP クエリでデータを再帰的に削除します。'Directory not empty' エラーを回避しますが、デタッチされたデータを静かに削除する可能性があります。

## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero}

タイプ: Bool

デフォルト値: 0

関数 'formatDateTime()' のフォーマッタ '%f' は、フォーマットされた値に小数秒がない場合、六つのゼロの代わりに一つのゼロを印刷します。

## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros}

タイプ: Bool

デフォルト値: 0

関数 'formatDateTime()' のフォーマッタ '%c', '%l' および '%k' は、月と時間を先頭ゼロなしで印刷します。

## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name}

タイプ: Bool

デフォルト値: 1

関数 'formatDateTime()' および 'parseDateTime()' のフォーマッタ '%M' は、分の代わりに月名を印刷/解析します。

## fsync_metadata {#fsync_metadata}

タイプ: Bool

デフォルト値: 1

.sql ファイルを書き込む際に[fSync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html)を有効または無効にします。デフォルトでは有効です。

サーバーに数百万の小さなテーブルがあり、それらが絶えず作成されて削除される場合は、無効にする意味があります。

## function_implementation {#function_implementation}

タイプ: String

デフォルト値:

特定のターゲットまたはバリアントの関数実装を選択します（エクスペリメンタル）。空の場合はすべて有効になります。

## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex}

タイプ: Bool

デフォルト値: 0

json_value 関数に対して複雑なタイプ（構造体、配列、マップなど）の返却を許可するか制御します。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

可能な値:

- true — 許可。
- false — 不許可。

## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable}

タイプ: Bool

デフォルト値: 0

JSON_VALUE 関数に対して値が存在しない場合に`NULL`を返すことを許可するか制御します。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

可能な値:

- true — 許可。
- false — 不許可。

## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order}

タイプ: Bool

デフォルト値: 1

関数[locate](../../sql-reference/functions/string-search-functions.md#locate)の引数の順序を制御します。

可能な値:

- 0 — 関数 `locate` は引数 `(haystack, needle[, start_pos])`を受け入れます。
- 1 — 関数 `locate` は引数 `(needle, haystack, [, start_pos])`（MySQL互換の動作）を受け入れます。

## function_range_max_elements_in_block {#function_range_max_elements_in_block}

タイプ: UInt64

デフォルト値: 500000000

関数[range](../../sql-reference/functions/array-functions.md/#range)によって生成されるデータボリュームの安全閾値を設定します。データブロックごとに関数によって生成される値の最大数（ブロック内のすべての行の配列サイズの合計）を定義します。

可能な値:

- 正の整数。

**参照も参照**

- [max_block_size](#setting-max_block_size)
- [min_insert_block_size_rows](#min-insert-block-size-rows)

## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block}

タイプ: UInt64

デフォルト値: 3000000

関数 `sleep` が各ブロックごとに許可される最大マイクロ秒数です。ユーザーがそれより大きな値で呼び出した場合、例外がスローされます。これは安全閾値です。

## function_visible_width_behavior {#function_visible_width_behavior}

タイプ: UInt64

デフォルト値: 1

`visibleWidth` の動作のバージョン。0 - コードポイントの数だけをカウント; 1 - ゼロ幅および組み合わせ文字を正しくカウントし、全角文字を2つとしてカウントし、タブ幅を見積もり、削除文字をカウントします。

## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments}

タイプ: Bool

デフォルト値: 1

`geoDistance`、`greatCircleDistance`、`greatCircleAngle` 関数の全ての引数が Float64 の場合、Float64 を返し、内部計算で倍精度を使用します。以前の ClickHouse バージョンでは、これらの関数は常に Float32 を返しました。

## glob_expansion_max_elements {#glob_expansion_max_elements}

タイプ: UInt64

デフォルト値: 1000

許可されている最大アドレス数（外部ストレージ、テーブル関数など）。

## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets}

タイプ: UInt64

デフォルト値: 1

グレースハッシュ結合の初期バケット数

## grace_hash_join_max_buckets {#grace_hash_join_max_buckets}

タイプ: UInt64

デフォルト値: 1024

グレースハッシュ結合のバケット数の制限

## group_by_overflow_mode {#group_by_overflow_mode}

タイプ: OverflowModeGroupBy

デフォルト値: throw

制限が超過された場合に何をするか。

## group_by_two_level_threshold {#group_by_two_level_threshold}

タイプ: UInt64

デフォルト値: 100000

2階層の集計が始まるキーの数から。0 - 閾値が設定されていません。

## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes}

タイプ: UInt64

デフォルト値: 50000000

集計状態のサイズがバイト単位で、2階層の集計が使用されるようになる最小サイズ。0 - 閾値が設定されていません。いずれかの閾値がトリガーされたときに2階層の集計が使用されます。

## group_by_use_nulls {#group_by_use_nulls}

タイプ: Bool

デフォルト値: 0

[GROUP BY句](/docs/ja/sql-reference/statements/select/group-by.md)が集約キーのタイプを扱う方法を変更します。
`ROLLUP`、`CUBE`、または `GROUPING SETS` 指定子が使用される場合、いくつかの集約キーは特定の結果行を生成するために使用されない可能性があります。
これらのキーの列は、この設定に応じて、デフォルト値または `NULL` で相応の行に満たされます。

可能な値:

- 0 — 集約キータイプのデフォルト値が欠落値を生成するために使用されます。
- 1 — ClickHouse は SQL 標準で述べられている通りに `GROUP BY` を実行します。集約キーのタイプは[Nullable](/docs/ja/sql-reference/data-types/nullable.md/#data_type-nullable)に変換されます。該当する集約キーの列は、使用されなかった行の[NULL](/docs/ja/sql-reference/syntax.md)で満たされます。

参照も参照:

- [GROUP BY句](/docs/ja/sql-reference/statements/select/group-by.md)

## handshake_timeout_ms {#handshake_timeout_ms}

タイプ: ミリ秒

デフォルト値: 10000

ハンドシェイク中にレプリカから Hello パケットを受信するためのタイムアウト（ミリ秒）。

## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert}

タイプ: Bool

デフォルト値: 0

HDFS エンジンテーブルに対する各挿入時に新しいファイルを作成するかどうかを有効または無効にします。有効な場合、各挿入時に次のような名前で新しい HDFS ファイルが作成されます。

初期: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` など。

可能な値:
- 0 — `INSERT` クエリはファイルの最後に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist}

タイプ: Bool

デフォルト値: 0

特定のキーを読み取る際にファイルが存在しない場合にその不在を無視します。

可能な値:
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外をスローします。

## hdfs_replication {#hdfs_replication}

タイプ: UInt64

デフォルト値: 0

hdfs ファイルが作成される際に指定できる実際のレプリケーション数。

## hdfs_skip_empty_files {#hdfs_skip_empty_files}

タイプ: Bool

デフォルト値: 0

[HDFS](../../engines/table-engines/integrations/hdfs.md) エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

可能な値:
- 0 — 空のファイルがリクエストされたフォーマットと互換性がない場合、`SELECT` は例外をスローします。
- 1 — 空のファイルに対する`SELECT` は空の結果を返します。

## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match}

タイプ: Bool

デフォルト値: 0

グロブ展開規則に従って一致するゼロファイルがある場合にエラーをスローします。

可能な値:
- 1 — `SELECT` は例外をスローします。
- 0 — `SELECT` は空の結果を返します。

## hdfs_truncate_on_insert {#hdfs_truncate_on_insert}

タイプ: Bool

デフォルト値: 0

hdfs エンジンテーブルでの挿入前に切り詰めを有効または無効にします。無効の場合、HDFS にファイルが既に存在する場合に挿入を試みると例外がスローされます。

可能な値:
- 0 — `INSERT` クエリはファイルの最後に新しいデータを追加します。
- 1 — `INSERT` クエリは既存のファイルの内容を新しいデータで置き換えます。

## hedged_connection_timeout_ms {#hedged_connection_timeout_ms}

タイプ: ミリ秒

デフォルト値: 50

ヘッジ要求のためにレプリカとの接続を確立する際の接続タイムアウト。

## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search}

タイプ: UInt64

デフォルト値: 256

ベクトル類似性インデックスを探索する際の動的候補リストのサイズ、別名 'ef_search'。

## hsts_max_age {#hsts_max_age}

タイプ: UInt64

デフォルト値: 0

HSTS の有効期限。0 は HSTS を無効にします。

## http_connection_timeout {#http_connection_timeout}

タイプ: 秒

デフォルト値: 1

HTTP 接続タイムアウト（秒単位）。

可能な値:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

## http_headers_progress_interval_ms {#http_headers_progress_interval_ms}

タイプ: UInt64

デフォルト値: 100

指定された間隔ごとに、X-ClickHouse-Progress HTTP ヘッダーをそれ以上送信しない。

## http_make_head_request {#http_make_head_request}

タイプ: Bool

デフォルト値: 1

`http_make_head_request` 設定は、HTTP からデータを読み取る際にファイルのサイズなどの情報を取得するために `HEAD` リクエストを実行できるようにします。有効になっているため、`HEAD` リクエストをサポートしないサーバーではこの設定を無効にすることが望ましい場合があります。

## http_max_field_name_size {#http_max_field_name_size}

タイプ: UInt64

デフォルト値: 131072

HTTP ヘッダー内のフィールド名の最大長

## http_max_field_value_size {#http_max_field_value_size}

タイプ: UInt64

デフォルト値: 131072

HTTP ヘッダー内のフィールド値の最大長

## http_max_fields {#http_max_fields}

タイプ: UInt64

デフォルト値: 1000000

HTTP ヘッダー内のフィールドの最大数

## http_max_multipart_form_data_size {#http_max_multipart_form_data_size}

タイプ: UInt64

デフォルト値: 1073741824

multipart/form-data コンテンツのサイズの制限。この設定は URL パラメータから解析することはできず、ユーザープロファイルに設定する必要があります。コンテンツは、クエリの実行が開始される前にメモリ内で解析され、外部テーブルが作成されます。そして、これはその段階に影響を与える唯一の制限です（最大メモリ使用量と最大実行時間に関する制限は、HTTP フォームデータの読み取り中には影響がありません）。

## http_max_request_param_data_size {#http_max_request_param_data_size}

タイプ: UInt64

デフォルト値: 10485760

事前定義された HTTP リクエストでクエリパラメータとして使用されるリクエストデータのサイズの制限。

## http_max_tries {#http_max_tries}

タイプ: UInt64

デフォルト値: 10

HTTP 経由で読み込む最大試行回数。

## http_max_uri_size {#http_max_uri_size}

タイプ: UInt64

デフォルト値: 1048576

HTTP リクエストの最大 URI 長を設定します。

可能な値:

- 正の整数。

## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress}

タイプ: Bool

デフォルト値: 0

クライアントからの HTTP POST データを解凍する際のチェックサム検証を有効または無効にします。 ClickHouse 独自の圧縮フォーマットにのみ使用されます（`gzip` や `deflate` には使用されません）。

詳細については、[HTTP インターフェースの説明](../../interfaces/http.md)を読んでください。

可能な値:

- 0 — 無効。
- 1 — 有効。

## http_receive_timeout {#http_receive_timeout}

タイプ: 秒

デフォルト値: 30

HTTP 受信タイムアウト（秒単位）。

可能な値:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

## http_response_buffer_size {#http_response_buffer_size}

タイプ: UInt64

デフォルト値: 0

HTTP レスポンスをクライアントに送信する前にサーバーメモリ内でバッファリングするバイト数（http_wait_end_of_queryが有効な場合のディスクへのフラッシュも含む）。

## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

HTTP 経由で読み込む際のバックオフ初期最小マイクロ秒数。

## http_retry_max_backoff_ms {#http_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 10000

HTTP 経由で読み込む際のバックオフ最大マイクロ秒数。

## http_send_timeout {#http_send_timeout}

タイプ: 秒

デフォルト値: 30

HTTP 送信タイムアウト（秒単位）。

可能な値:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

:::note
これはデフォルトプロファイルにのみ適用されます。変更を有効にするには、サーバーの再起動が必要です。
:::

## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs}

タイプ: Bool

デフォルト値: 1

HTTP_NOT_FOUND エラーを伴うグローブ用の URL をスキップします。

## http_wait_end_of_query {#http_wait_end_of_query}

タイプ: Bool

デフォルト値: 0

サーバー側での HTTP レスポンスバッファリングを有効にします。

## http_write_exception_in_output_format {#http_write_exception_in_output_format}

タイプ: Bool

デフォルト値: 1

有効な出力を生成するために、出力形式に例外を書き込みます。JSON および XML 形式で動作します。

## http_zlib_compression_level {#http_zlib_compression_level}

タイプ: Int64

デフォルト値: 3

[enable_http_compression = 1](#enable_http_compression) の場合、HTTP リクエストへの応答でデータ圧縮のレベルを設定します。

可能な値: 1 から 9 の数値。

## idle_connection_timeout {#idle_connection_timeout}

タイプ: UInt64

デフォルト値: 3600

指定された秒数後にアイドル TCP 接続を閉じるタイムアウト。

可能な値:

- 正の整数（0 - すぐに閉じる、0秒後）。

## ignore_cold_parts_seconds {#ignore_cold_parts_seconds}

タイプ: Int64

デフォルト値: 0

Only in ClickHouse Cloud. プリウォームされるまで（see cache_populated_by_fetch）またはこの秒数古くなるまで、SELECT クエリから新しいデータパーツを除外します。Replicated-/SharedMergeTree のみ。

## ignore_data_skipping_indices {#ignore_data_skipping_indices}

タイプ: String

デフォルト値:

クエリで使用されている場合に指定されたスキッピングインデックスを無視します。

以下の例を考えてみてください。

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- クエリは CANNOT_PARSE_TEXT エラーを生成します。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- Ok.
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- Ok。

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx' ; -- クエリは INDEX_NOT_USED エラーを生成します。なぜなら xy_idx が明示的に無視されているからです。
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

無視せずにインデックスを使用していないクエリ:
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

`xy_idx` インデックスを無視:
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

MergeTree ファミリーのテーブルで機能します。

## ignore_drop_queries_probability {#ignore_drop_queries_probability}

タイプ: Float

デフォルト値: 0

有効な場合、サーバーは指定された確率ですべての DROP テーブルクエリを無視します（Memory および JOIN エンジンの場合、DROP を TRUNCATE に置き換えます）。テスト目的で使用されます。

## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table}

タイプ: Bool

デフォルト値: 0

ビューへのプッシュ中にターゲットテーブルが削除された状態のMVを無視します。

## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries}

タイプ: Bool

デフォルト値: 0

レプリケートアクセスエンティティ管理クエリのための ON CLUSTER 句を無視します。

## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされた名前付きコレクション管理クエリのための ON CLUSTER 句を無視します。

## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries}

タイプ: Bool

デフォルト値: 0

レプリケーションされた UDF 管理クエリのための ON CLUSTER 句を無視します。

## implicit_select {#implicit_select}

タイプ: Bool

デフォルト値: 0

先頭の SELECT キーワードなしに単純な SELECT クエリを書くことを許可し、それにより電卓スタイルの使用が簡単になります。例: `1 + 2` が有効なクエリになります。

`clickhouse-local` ではデフォルトで有効であり、明示的に無効にすることができます。

## implicit_transaction {#implicit_transaction}

タイプ: Bool

デフォルト値: 0

有効にされていて、すでにトランザクション内でない場合、クエリ全体をトランザクション内にラップします（開始 + コミットまたはロールバック）。

## input_format_parallel_parsing {#input_format_parallel_parsing}

タイプ: Bool

デフォルト値: 1

データフォーマットの順序を保持する並列解析を有効または無効にします。サポートされているのは [TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)、および [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 形式だけです。

可能な値:

- 1 — 有効。
- 0 — 無効。

## insert_allow_materialized_columns {#insert_allow_materialized_columns}

タイプ: Bool

デフォルト値: 0

設定が有効な場合、INSERT にマテリアライズドカラムを許可します。

## insert_deduplicate {#insert_deduplicate}

タイプ: Bool

デフォルト値: 1

`INSERT` のブロック重複（Replicated* テーブル用）を有効または無効にします。

可能な値:

- 0 — 無効。
- 1 — 有効。
デフォルトでは、`INSERT`ステートメントによってレプリケートされたテーブルに挿入されたブロックは重複排除されます（[データレプリケーション](../../engines/table-engines/mergetree-family/replication.md)を参照してください）。レプリケートされたテーブルでは、デフォルトで各パーティションの最近の100ブロックのみが重複排除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照してください）。レプリケートされていないテーブルについては、[non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)を参照してください。

## insert_deduplication_token {#insert_deduplication_token}

タイプ: 文字列

デフォルト値:

この設定により、ユーザーはMergeTree/ReplicatedMergeTreeにおける独自の重複排除セマンティクスを提供できます。たとえば、各INSERTステートメントで設定のユニークな値を提供することにより、ユーザーは同じデータが重複排除されるのを回避できます。

可能な値:

- 任意の文字列

`insert_deduplication_token`は空でない場合にのみ重複排除に使用されます。

レプリケートされたテーブルでは、デフォルトで各パーティションの最近の挿入のうち100のみが重複排除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照してください）。レプリケートされていないテーブルについては、[non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)を参照してください。

:::note
`insert_deduplication_token`はパーティションレベルで機能します（`insert_deduplication`チェックサムと同様）。複数のパーティションは同じ`insert_deduplication_token`を持つことができます。
:::

例:

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- 次の挿入は、insert_deduplication_tokenが異なるため、重複排除されません
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- 次の挿入は、insert_deduplication_tokenが以前のものの1つと同じであるため、重複排除されます
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```

## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability}

タイプ: Float

デフォルト値: 0

挿入中のkeeperリクエストの失敗確率の概算。有效な値は[0.0f, 1.0f]の範囲です。

## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed}

タイプ: UInt64

デフォルト値: 0

0 - ランダムシード、それ以外は設定値。

## insert_keeper_max_retries {#insert_keeper_max_retries}

タイプ: UInt64

デフォルト値: 20

この設定は、レプリケートされたMergeTreeへの挿入中にClickHouse Keeper（またはZooKeeper）リクエストの最大再試行回数を設定します。ネットワークエラー、Keeperセッションタイムアウト、またはリクエストタイムアウトによって失敗したKeeperリクエストのみが再試行の対象とされます。

可能な値:

- 正の整数。
- 0 — 再試行は無効

クラウドのデフォルト値: `20`。

Keeperリクエストの再試行は、あるタイムアウトの後に行われます。タイムアウトは次の設定によって制御されます: `insert_keeper_retry_initial_backoff_ms`, `insert_keeper_retry_max_backoff_ms`。最初の再試行は`insert_keeper_retry_initial_backoff_ms`のタイムアウト後に行われます。その後のタイムアウトは次のように計算されます:
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

例えば、`insert_keeper_retry_initial_backoff_ms=100`、`insert_keeper_retry_max_backoff_ms=10000`、および`insert_keeper_max_retries=8`の場合、タイムアウトは`100, 200, 400, 800, 1600, 3200, 6400, 10000`になります。

障害耐性の他に、再試行はより良いユーザーエクスペリエンスを提供する目的もあります - 例えば、Keeperが再起動中（アップグレードによる）にINSERT実行中にエラーを返さずに済むようにします。

## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

INSERTクエリ実行中に失敗したKeeperリクエストを再試行するための初期タイムアウト（ミリ秒）。

可能な値:

- 正の整数。
- 0 — タイムアウトなし。

## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 10000

INSERTクエリ実行中に失敗したKeeperリクエストを再試行するための最大タイムアウト（ミリ秒）。

可能な値:

- 正の整数。
- 0 — 最大タイムアウトは無制限。

## insert_null_as_default {#insert_null_as_default}

タイプ: Bool

デフォルト値: 1

[NULL](../../sql-reference/syntax.md/#null-literal)の代わりに、[デフォルト値](../../sql-reference/statements/create/table.md/#create-default-values)を挿入するかどうかを有効または無効にします。NULLでないデータ型のカラムに対しては、カラムタイプがNULLでない場合、この設定が無効のときに`NULL`を挿入すると例外が発生します。カラムタイプがNULL可能な場合、`NULL`値はこの設定に関係なくそのまま挿入されます。

この設定は[INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select)クエリに適用されます。`SELECT`サブクエリは`UNION ALL`句で連結可能であることに注意してください。

可能な値:

- 0 — NULLをNULLでないカラムに挿入すると例外が発生します。
- 1 — NULLの代わりにデフォルトカラム値が挿入されます。

## insert_quorum {#insert_quorum}

タイプ: UInt64Auto

デフォルト値: 0

:::note
この設定はSharedMergeTreeには適用されません。詳しくは[SharedMergeTreeの整合性](/docs/ja/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

クォーラム書き込みを有効にします。

- `insert_quorum < 2`の場合、クォーラム書き込みは無効です。
- `insert_quorum >= 2`の場合、クォーラム書き込みは有効です。
- `insert_quorum = 'auto'`の場合、過半数の数（`number_of_replicas / 2 + 1`）をクォーラム数として使用します。

クォーラム書き込み

`INSERT`は、ClickHouseが`insert_quorum_timeout`の間に`insert_quorum`のレプリカにデータを書き込むことに成功した場合にのみ成功します。何らかの理由で成功した書き込みのレプリカ数が`insert_quorum`に達しない場合、書き込みは失敗と見なされ、ClickHouseはすでに書き込まれたレプリカから挿入されたブロックを削除します。

`insert_quorum_parallel`が無効の場合、クォーラム内のすべてのレプリカは一貫性があります。つまり、すべての以前の`INSERT`クエリのデータを含んでいます（`INSERT`シーケンスは線形化されます）。`insert_quorum`および`insert_quorum_parallel`を使用して書き込まれたデータを読み取る際には、[select_sequential_consistency](#select_sequential_consistency)を使用して`SELECT`クエリの順次整合性を有効にすることができます。

ClickHouseは次の場合に例外を生成します:

- クエリの時点での利用可能なレプリカの数が`insert_quorum`未満の場合。
- `insert_quorum_parallel`が無効であり、前のブロックが`insert_quorum`のレプリカにまだ挿入されていないときにデータを書き込もうとした場合。この状況は、ユーザーが以前の`insert_quorum`の`INSERT`クエリが完了する前に同じテーブルに別の`INSERT`クエリを実行しようとした場合に発生することがあります。

その他の情報は以下を参照してください：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_parallel {#insert_quorum_parallel}

タイプ: Bool

デフォルト値: 1

:::note
この設定はSharedMergeTreeには適用されません。詳しくは[SharedMergeTreeの整合性](/docs/ja/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

クォーラム`INSERT`クエリに対して並列性を有効または無効にします。これが有効な場合、前のクエリがまだ終了していない間に追加の`INSERT`クエリを送信できます。無効にした場合、同じテーブルに対する追加の書き込みは拒否されます。

可能な値:

- 0 — 無効。
- 1 — 有効。

その他の情報は以下を参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_timeout {#insert_quorum_timeout}

タイプ: ミリ秒

デフォルト値: 600000

クォーラムへの書き込みタイムアウト（ミリ秒）。タイムアウトが経過し、まだ書き込みが行われていない場合、ClickHouseは例外を生成し、クライアントは同じブロックを同じまたは他のレプリカに書き込むためにクエリを再試行する必要があります。

その他の情報は以下を参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_shard_id {#insert_shard_id}

タイプ: UInt64

デフォルト値: 0

`0`でない場合、データが同期的に挿入される[分散テーブル](../../engines/table-engines/special/distributed.md/#distributed)のシャードを指定します。

`insert_shard_id`の値が無効な場合、サーバーは例外を投げます。

`requested_cluster`のシャード数を取得するには、サーバーの設定を確認するか、次のクエリを使用できます:

``` sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能な値:

- 0 — 無効。
- 対応する[分散テーブル](../../engines/table-engines/special/distributed.md/#distributed)の`1`から`shards_num`の任意の数。

**例**

クエリ:

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

結果:

``` text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```

## interactive_delay {#interactive_delay}

タイプ: UInt64

デフォルト値: 100000

リクエストの実行がキャンセルされたかどうかをチェックし、進捗を送信するためのマイクロ秒単位の間隔。

## intersect_default_mode {#intersect_default_mode}

タイプ: SetOperationMode

デフォルト値: ALL

INTERSECTクエリのデフォルトモードを設定します。可能な値: 空の文字列、'ALL'、'DISTINCT'。空の場合、モードなしでのクエリは例外をスローします。

## join_algorithm {#join_algorithm}

タイプ: JoinAlgorithm

デフォルト値: default

使用される[JOIN](../../sql-reference/statements/select/join.md)アルゴリズムを指定します。

複数のアルゴリズムを指定でき、適切なものが特定のクエリの種類/厳密性およびテーブルエンジンに基づいて選択されます。

可能な値:

- default

 これは、可能であれば`hash`または`direct`の同等です（`direct,hash`と同じ）。

- grace_hash

  [Grace hash join](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join)が使用されます。Grace hashは、メモリの使用を制限しながらパフォーマンスの良い複雑な結合を提供するアルゴリズムオプションを提供します。

  grace joinの最初のフェーズでは、右側のテーブルを読み取り、それをキー列のハッシュ値に基づいてNバケットに分割します（最初はNは`grace_hash_join_initial_buckets`です）。各バケットが独立して処理できることを保証する方法でこれが行われます。最初のバケットの行はメモリ内ハッシュテーブルに追加され、他の行はディスクに保存されます。ハッシュテーブルがメモリ制限を超えた場合（例: [`max_bytes_in_join`](/docs/ja/operations/settings/query-complexity.md/#max_bytes_in_join)で設定されたもの）、バケットの数が増加し、各行の割り当てバケットも増えます。現在のバケットに属さない行はフラッシュされ、再割り当てされます。

  `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`をサポートします。

- hash

  [Hash join algorithm](https://en.wikipedia.org/wiki/Hash_join)が使用されます。すべての種類および厳密性の組み合わせと、`JOIN ON`セクションで`OR`で結合された複数の結合キーをサポートする最も一般的な実装です。

- parallel_hash

  `hash`の変種で、データをバケットに分割し、プロセスを加速するために同時に複数のハッシュテーブルを構築します。

  `hash`アルゴリズムを使用する際、右側の`JOIN`の部分がRAMにアップロードされます。

- partial_merge

  [sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join)の変種で、右側のテーブルのみが完全にソートされています。

  `RIGHT JOIN`および`FULL JOIN`は、厳密さのすべてが`ALL`である場合にのみサポートされます（`SEMI`、`ANTI`、`ANY`、および`ASOF`はサポートされていません）。

  `partial_merge`アルゴリズムを使用する際、ClickHouseはデータをソートし、ディスクにダンプします。ClickHouseの`partial_merge`アルゴリズムは、古典的な実現とは少し異なります。最初に、ClickHouseは結合キーでブロックごとに右のテーブルをソートし、ソートされたブロックに対して最小-最大インデックスを作成します。次に、左側のテーブルの部分を`join key`でソートし、それを右のテーブルに対して結合します。最小-最大インデックスも必要ない右テーブルのブロックをスキップするために使用されます。

- direct

  このアルゴリズムは、右のテーブルがキー-バリューリクエストをサポートするストレージのために適用できます。

  `direct`アルゴリズムは、左のテーブルの行をキーとして使用して右のテーブルでルックアップを行います。これは、[Dictionary](../../engines/table-engines/special/dictionary.md/#dictionary)や[EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)などの特別なストレージによってのみサポートされており、`LEFT`及び`INNER`JOINだけがサポートされています。

- auto

  `auto`に設定されている場合、最初に`hash`結合が試みられ、メモリ制限が違反された場合は、動的に他のアルゴリズムに切り替えられます。

- full_sorting_merge

  [Sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join)を完全にソートされた結合テーブルを結合する前に使用します。

- prefer_partial_merge

  ClickHouseは可能であれば常に`partial_merge`結合を使用しようとし、そうでなければ`hash`を使用します。*非推奨*、`partial_merge,hash`と同じです。

## join_any_take_last_row {#join_any_take_last_row}

タイプ: Bool

デフォルト値: 0

`ANY`の厳密さを使用した結合操作の動作を変更します。

:::note
この設定は、[Join](../../engines/table-engines/special/join.md)エンジンのテーブルのみの`JOIN`操作に適用されます。
:::

可能な値:

- 0 — 右のテーブルに一致する行が複数ある場合、最初に見つかった行のみが結合されます。
- 1 — 右のテーブルに一致する行が複数ある場合、最後に見つかった行のみが結合されます。

その他の情報は以下を参照してください：

- [JOIN句](../../sql-reference/statements/select/join.md/#select-join)
- [Joinテーブルエンジン](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)

## join_default_strictness {#join_default_strictness}

タイプ: JoinStrictness

デフォルト値: ALL

[JOIN句](../../sql-reference/statements/select/join.md/#select-join)のデフォルトの厳密さを設定します。

可能な値:

- `ALL` — 右のテーブルに一致する行が複数ある場合、ClickHouseは一致する行から[デカルト積](https://en.wikipedia.org/wiki/Cartesian_product)を生成します。これが標準SQLからの通常の`JOIN`の動作です。
- `ANY` — 右のテーブルに一致する行が複数ある場合、最初に見つかった行のみが結合されます。右のテーブルに一致する行が1つしかない場合、`ANY`と`ALL`の結果は同じです。
- `ASOF` — 不確実な一致を持つシーケンスの結合に使用されます。
- 空の文字列 — クエリに`ALL`または`ANY`が指定されていない場合、ClickHouseは例外をスローします。

## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge}

タイプ: UInt64

デフォルト値: 64

ディスク上で実行されるMergeJoin操作の並列ソートのために許可されるファイルの数を制限します。

設定値が大きいほど、使用されるRAMが多く、ディスクI/Oが少なくなります。

可能な値:

- 2から始まる正の整数。

## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold}

タイプ: UInt64

デフォルト値: 5

ハッシュ結合において、行リストで出力するかどうかを決定するための右のテーブルのキーごとの平均行数の下限。

## join_overflow_mode {#join_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合に何をするかを指定します。

## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows}

タイプ: UInt64

デフォルト値: 10000

左または内部結合において、右のテーブルをキーによって再整理するかどうかを決定するための右のテーブルの最大行数。

## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows}

タイプ: UInt64

デフォルト値: 40

左または内部結合において、右のテーブルをキーによって再整理するかどうかを決定するための右のテーブルのキーごとの平均行数の下限。この設定は、スパーステーブルキーに対して最適化が適用されないようにします。

## join_use_nulls {#join_use_nulls}

タイプ: Bool

デフォルト値: 0

[JOIN](../../sql-reference/statements/select/join.md)の動作タイプを設定します。テーブルをマージする際に空のセルが出現することがあります。ClickHouseは、この設定に基づいてそれらを異なる方法で埋めます。

可能な値:

- 0 — 空のセルは対応するフィールドタイプのデフォルト値で埋まります。
- 1 — `JOIN`は標準SQLと同様の動作をします。対応するフィールドの型は[Nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)に変換され、空のセルは[NULL](../../sql-reference/syntax.md)で埋まります。

## joined_subquery_requires_alias {#joined_subquery_requires_alias}

タイプ: Bool

デフォルト値: 1

正しい名前の資格のために、結合サブクエリとテーブル関数に別名を付けることを強制します。

## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit}

タイプ: Bool

デフォルト値: 0

利用可能なCPUコアの数に依存するkafka_num_consumersの制限を無効にします。

## kafka_max_wait_ms {#kafka_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 5000

再試行前に[Kafka](../../engines/table-engines/integrations/kafka.md/#kafka)からメッセージを読み取るための待機時間（ミリ秒）。

可能な値:

- 正の整数。
- 0 — 無限のタイムアウト。

その他の情報は以下を参照してください：

- [Apache Kafka](https://kafka.apache.org/)

## keeper_map_strict_mode {#keeper_map_strict_mode}

タイプ: Bool

デフォルト値: 0

KeeperMapに対する操作中に追加のチェックを強制します。例えば、既存のキーに対する挿入に対して例外をスローします。

## keeper_max_retries {#keeper_max_retries}

タイプ: UInt64

デフォルト値: 10

一般的なkeeper操作の最大再試行回数。

## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

一般的なkeeper操作のための初期のバックオフタイムアウト。

## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 5000

一般的なkeeper操作のための最大バックオフタイムアウト。

## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal}

タイプ: Bool

デフォルト値: 0

大きなタプルリテラルの要素のすべての名前をハッシュの代わりにそのカラム名でリストします。この設定は互換性の理由のみで存在します。これを'true'に設定することは、21.7よりも低いバージョンからより高いバージョンへのクラスターのローリングアップデートを行う際に意味があります。

## lightweight_deletes_sync {#lightweight_deletes_sync}

タイプ: UInt64

デフォルト値: 2

これは[`mutations_sync`](#mutations_sync)と同じですが、論理削除の実行のみを制御します。

可能な値:

- 0 - 削除は非同期に実行されます。
- 1 - クエリは現在のサーバーで論理削除が完了するのを待機します。
- 2 - クエリはすべてのレプリカ（存在する場合）で論理削除が完了するのを待機します。

**その他の情報**

- [ALTERクエリの同期性](../../sql-reference/statements/alter/index.md#synchronicity-of-alter-queries)
- [ミューテーション](../../sql-reference/statements/alter/index.md#mutations)

## limit {#limit}

タイプ: UInt64

デフォルト値: 0

クエリ結果から取得する最大行数を設定します。これは、[LIMIT](../../sql-reference/statements/select/limit.md/#limit-clause)句で設定された値を調整し、クエリに指定された制限がこの設定で設定された制限を超えることがないようにします。

可能な値:

- 0 — 行数に制限はありません。
- 正の整数。

## live_view_heartbeat_interval {#live_view_heartbeat_interval}

タイプ: 秒

デフォルト値: 15

ライブクエリが生きていることを示すハートビート間隔（秒）。

## load_balancing {#load_balancing}

タイプ: LoadBalancing

デフォルト値: random

分散クエリ処理に使用されるレプリカ選択のアルゴリズムを指定します。

ClickHouseは、次のようなレプリカの選択アルゴリズムをサポートしています:

- [Random](#load_balancing-random)（デフォルト）
- [Nearest hostname](#load_balancing-nearest_hostname)
- [Hostname levenshtein distance](#load_balancing-hostname_levenshtein_distance)
- [In order](#load_balancing-in_order)
- [First or random](#load_balancing-first_or_random)
- [Round robin](#load_balancing-round_robin)

その他の情報は以下を参照してください：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

### Random (デフォルト) {#load_balancing-random}

``` sql
load_balancing = random
```

各レプリカのエラーの数がカウントされます。クエリは、最も少ないエラーを持つレプリカに送信され、同じエラー数のレプリカが複数ある場合は、そのうちのどれかに送信されます。
欠点: サーバーの近接性は考慮されず、レプリカが異なるデータを持っている場合、異なるデータが返される可能性があります。

### Nearest Hostname {#load_balancing-nearest_hostname}

``` sql
load_balancing = nearest_hostname
```

各レプリカのエラーの数がカウントされます。5分ごとに、エラーの数は2で統合的に割り算されます。これにより、最近の時間に基づいてエラーの数が指数的にスムージングされて計算されます。エラー数が最も少ないレプリカが1つある場合（すなわち、他のレプリカでは最近エラーが発生している）、クエリはそのレプリカに送信されます。同じ最小エラー数のレプリカが複数ある場合は、設定ファイルのサーバーのホスト名に最も類似したホスト名のレプリカにクエリが送信されます（同じ位置に異なる文字数による）。

たとえば、example01-01-1とexample01-01-2は1箇所で異なり、example01-01-1とexample01-02-2は2箇所で異なります。
この方法は単純に見えるかもしれませんが、ネットワークトポロジーに関する外部データを必要とせず、IPアドレスを比較することも難しいIPv6アドレスに対しても対処します。

したがって、等価なレプリカが存在する場合、名前によって最も近いものが優先されます。
さらに、同じサーバーにクエリを送信する場合、障害がない場合、分散クエリも同じサーバーに送信されると考えられます。そのため、レプリカに異なるデータが格納されていても、クエリはほとんど同じ結果を返します。

### Hostname levenshtein distance {#load_balancing-hostname_levenshtein_distance}

``` sql
load_balancing = hostname_levenshtein_distance
```

`nearest_hostname`と同様ですが、ホスト名を[levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)の方式で比較します。例えば:

``` text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```

### In Order {#load_balancing-in_order}

``` sql
load_balancing = in_order
```

同じエラー数のレプリカには、設定ファイルに記載された順序のままアクセスされます。
この方法は、どのレプリカが優先されるかが明確にわかっているときに適切です。

### First or Random {#load_balancing-first_or_random}

``` sql
load_balancing = first_or_random
```

このアルゴリズムは、設定された最初のレプリカを選択します。最初のレプリカが利用できない場合はランダムなレプリカを選択します。これは、クロスレプリケーショントポロジーのセットアップで効果的ですが、他の構成では役に立ちません。

`first_or_random`アルゴリズムは、`in_order`アルゴリズムの問題を解決します。`in_order`の場合、あるレプリカがダウンすると次のレプリカに二重負荷がかかり、残りのレプリカは通常のトラフィックの量を処理します。`first_or_random`アルゴリズムを使用する場合、まだ利用可能なレプリカの間で負荷が均等に分散されます。

`first_or_random`アルゴリズムを使用して、最初のレプリカを明示的に定義することもできます。この設定を使用すると、レプリカ間でクエリのワークロードを再バランスするより多くの制御が可能になります。

### Round Robin {#load_balancing-round_robin}

``` sql
load_balancing = round_robin
```

このアルゴリズムは、同じエラー数のレプリカに対してラウンドロビンポリシーを使用します（`round_robin`ポリシーのクエリのみがカウントされます）。

## load_balancing_first_offset {#load_balancing_first_offset}

タイプ: UInt64

デフォルト値: 0

FIRST_OR_RANDOMロードバランシング戦略が使用されているときに、優先的にクエリを送信するレプリカを指定します。

## load_marks_asynchronously {#load_marks_asynchronously}

タイプ: Bool

デフォルト値: 0

MergeTreeマークを非同期にロードします。

## local_filesystem_read_method {#local_filesystem_read_method}

タイプ: 文字列

デフォルト値: pread_threadpool

ローカルファイルシステムからデータを読み取るためのメソッド。選択肢: read, pread, mmap, io_uring, pread_threadpool。'io_uring'メソッドはエクスペリメンタルで、Log、TinyLog、StripeLog、File、Set、および同時読み書きがある場合は他のテーブルでは機能しません。

## local_filesystem_read_prefetch {#local_filesystem_read_prefetch}

タイプ: Bool

デフォルト値: 0

ローカルファイルシステムからデータを読み取る際にプリフェッチを使用するかどうか。

## lock_acquire_timeout {#lock_acquire_timeout}

タイプ: 秒

デフォルト値: 120

ロックリクエストが失敗するまでに待機する秒数を定義します。

ロックタイムアウトは、テーブルでの読み取り/書き込み操作を実行中にデッドロックから保護するために使用されます。タイムアウトが経過するとロックリクエストが失敗し、ClickHouseサーバーは「ロック試行がタイムアウトしました！デッドロックの回避が可能です。クライアントは再試行する必要があります。」という例外を投げます。エラーコードは`DEADLOCK_AVOIDED`です。

可能な値:

- 正の整数（秒単位）。
- 0 — ロックタイムアウトなし。

## log_comment {#log_comment}

タイプ: 文字列

デフォルト値:

[system.query_log](../system-tables/query_log.md)テーブルの`log_comment`フィールドの値と、サーバーログのコメントテキストを指定します。

これにより、サーバーログの可読性を向上させることができます。さらに、[clickhouse-test](../../development/tests.md)を実行した後に、テストに関連するクエリを`system.query_log`から選択するのにも役立ちます。

可能な値:

- [max_query_size](#max_query_size)を超えない任意の文字列。max_query_sizeを超えると、サーバーは例外をスローします。

**例**

クエリ:

``` sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

結果:

``` text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```

## log_formatted_queries {#log_formatted_queries}

タイプ: Bool

デフォルト値: 0

[system.query_log](../../operations/system-tables/query_log.md)システムテーブルにフォーマットされたクエリをログすることを許可します（[system.query_log](../../operations/system-tables/query_log.md)の`formatted_query`列にデータを埋め込みます）。

可能な値:

- 0 — フォーマットされたクエリはシステムテーブルにログされません。
- 1 — フォーマットされたクエリはシステムテーブルにログされます。

## log_processors_profiles {#log_processors_profiles}

タイプ: Bool

デフォルト値: 1

実行中にプロセッサが費やした時間を`system.processors_profile_log`テーブルに書き込みます。

その他の情報は以下を参照してください：

- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md#explain-pipeline)

## log_profile_events {#log_profile_events}

タイプ: Bool

デフォルト値: 1

クエリパフォーマンス統計を`query_log`、`query_thread_log`、および`query_views_log`にログします。

## log_queries {#log_queries}

タイプ: Bool

デフォルト値: 1

クエリログを設定します。

この設定でClickHouseに送信されたクエリは、[query_log](../../operations/server-configuration-parameters/settings.md/#query-log)サーバー設定パラメータのルールに従ってログされます。

例:

``` text
log_queries=1
```

## log_queries_cut_to_length {#log_queries_cut_to_length}

タイプ: UInt64

デフォルト値: 100000

クエリの長さが指定された閾値（バイト単位）を超える場合、クエリはクエリログに書き込む際にカットされます。また、通常のテキストログに印刷されるクエリの長さも制限されます。

## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms}

タイプ: ミリ秒

デフォルト値: 0

無効にした場合（0以外）、この設定の値よりも早く実行されたクエリはログに記録されません（これは[MySQLのスロークエリログ](https://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html)の`long_query_time`のように考えられます）。基本的に、次のテーブルには表示されません。

- `system.query_log`
- `system.query_thread_log`

次のタイプのクエリのみがログに記載されます。

- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- タイプ: ミリ秒
- デフォルト値: 0（すべてのクエリ）

## log_queries_min_type {#log_queries_min_type}

タイプ: LogQueriesType

デフォルト値: QUERY_START

`query_log`に記録する最小タイプです。

可能な値:
- `QUERY_START`（`=1`）
- `QUERY_FINISH`（`=2`）
- `EXCEPTION_BEFORE_START`（`=3`）
- `EXCEPTION_WHILE_PROCESSING`（`=4`）

これを使用して、`query_log`にどのエンティティをログに記録するかを制限できます。たとえば、エラーのみに関心がある場合は、`EXCEPTION_WHILE_PROCESSING`を使用できます:

``` text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```

## log_queries_probability {#log_queries_probability}

タイプ: Float

デフォルト値: 1

ユーザーが[query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md)、および[query_views_log](../../operations/system-tables/query_views_log.md)システムテーブルに、指定された確率でランダムに選択されたクエリのサンプルのみをログに記録できるようにします。これにより、1秒あたり大量のクエリが発生する際の負荷を削減できます。

可能な値:

- 0 — クエリはシステムテーブルにログされません。
- 正の浮動小数点数で範囲は [0..1]。例えば、設定値が `0.5` の場合、クエリの約半分がシステムテーブルに記録されます。
- 1 — すべてのクエリがシステムテーブルに記録されます。

## log_query_settings {#log_query_settings}

タイプ: Bool

デフォルト値: 1

クエリログと OpenTelemetry span ログにクエリ設定を記録します。

## log_query_threads {#log_query_threads}

タイプ: Bool

デフォルト値: 0

クエリスレッドのロギングを設定します。

クエリスレッドは [system.query_thread_log](../../operations/system-tables/query_thread_log.md) テーブルにログとして記録されます。この設定は、[log_queries](#log-queries) が true の場合のみ効果があります。この設定で ClickHouse によって実行されるクエリのスレッドは、[query_thread_log](../../operations/server-configuration-parameters/settings.md/#query_thread_log) サーバー設定パラメーターのルールに従って記録されます。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

``` text
log_query_threads=1
```

## log_query_views {#log_query_views}

タイプ: Bool

デフォルト値: 1

クエリビューのロギングを設定します。

この設定が有効な状態で ClickHouse によって実行されるクエリに関連するビュー（物化ビューまたはライブビュー）があれば、それらは [query_views_log](../../operations/server-configuration-parameters/settings.md/#query_views_log) サーバー設定パラメーターに記録されます。

例:

``` text
log_query_views=1
```

## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format}

タイプ: Bool

デフォルト値: 1

[LowCardinality](../../sql-reference/data-types/lowcardinality.md) データ型を [Native](../../interfaces/formats.md/#native) 形式で使用することを許可または制限します。

`LowCardinality` の使用が制限されている場合、ClickHouse サーバーは `SELECT` クエリのために `LowCardinality` カラムを通常のカラムに変換し、`INSERT` クエリのために通常のカラムを `LowCardinality` カラムに変換します。

この設定は、`LowCardinality` データ型をサポートしないサードパーティクライアント向けに主に必要です。

可能な値:

- 1 — `LowCardinality` の使用は制限されません。
- 0 — `LowCardinality` の使用は制限されます。

## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size}

タイプ: UInt64

デフォルト値: 8192

[LowCardinality](../../sql-reference/data-types/lowcardinality.md) データ型のためにストレージファイルシステムに書き込むことができる共有グローバルDictionaryの最大サイズ（行数）を設定します。この設定は、無制限のDictionary成長によるRAMの問題を防ぎます。最大Dictionaryサイズの制限によりエンコードできないすべてのデータは、ClickHouse が通常の方法で書き込みます。

可能な値:

- 任意の正の整数。

## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part}

タイプ: Bool

デフォルト値: 0

データパートのために単一Dictionaryの使用をオンまたはオフにします。

デフォルトでは、ClickHouse サーバーはDictionaryのサイズを監視し、Dictionaryがオーバーフローした場合に次のDictionaryの書き込みを開始します。複数のDictionaryの作成を禁止するには `low_cardinality_use_single_dictionary_for_part = 1` を設定します。

可能な値:

- 1 — データパートのための複数のDictionaryの作成が禁止されます。
- 0 — データパートのための複数のDictionaryの作成は禁止されません。

## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert}

タイプ: Bool

デフォルト値: 1

true の場合、挿入時にスキップインデックスが計算されます。そうでない場合、スキップインデックスはマージ時にのみ計算されます。

## materialize_statistics_on_insert {#materialize_statistics_on_insert}

タイプ: Bool

デフォルト値: 1

true の場合、挿入時に統計が計算されます。そうでない場合、統計はマージ時にのみ計算されます。

## materialize_ttl_after_modify {#materialize_ttl_after_modify}

タイプ: Bool

デフォルト値: 1

ALTER MODIFY TTL クエリの後に古いデータに TTL を適用します。

## materialized_views_ignore_errors {#materialized_views_ignore_errors}

タイプ: Bool

デフォルト値: 0

MATERIALIZED VIEW のエラーを無視し、MV に関わらず元のブロックをテーブルに配信することを許可します。

## max_analyze_depth {#max_analyze_depth}

タイプ: UInt64

デフォルト値: 5000

インタープリタによって実行される最大分析数。

## max_ast_depth {#max_ast_depth}

タイプ: UInt64

デフォルト値: 1000

クエリ構文木の最大深さ。解析後にチェックされます。

## max_ast_elements {#max_ast_elements}

タイプ: UInt64

デフォルト値: 50000

構文木の最大ノード数。解析後にチェックされます。

## max_backup_bandwidth {#max_backup_bandwidth}

タイプ: UInt64

デフォルト値: 0

サーバー上の特定のバックアップ用の最大読み取り速度（バイト毎秒）。ゼロは無制限を意味します。

## max_block_size {#max_block_size}

タイプ: UInt64

デフォルト値: 65409

ClickHouse では、データは列の部分のセットであるブロックによって処理されます。単一ブロックの内部処理サイクルは効率的ですが、各ブロックを処理する際には目に見えるコストが発生します。

`max_block_size` 設定は、テーブルからデータをロードする際に単一ブロックに含める推奨最大行数を示します。 `max_block_size` サイズのブロックは常にテーブルからロードされるわけではありません。もし ClickHouse が必要なデータが少ないと判断した場合は、小さいブロックが処理されます。

ブロックサイズが小さすぎると、各ブロックを処理する際に目に見えるコストが発生するので注意が必要です。大きすぎると、最初のブロックの処理後に LIMIT 句を持つクエリが迅速に実行されない可能性があります。`max_block_size` を設定する際の目標は、多くのカラムを複数のスレッドで取り出す際にメモリを使いすぎないようにしつつ、少なくともある程度のキャッシュの局所性を保つことです。

## max_bytes_before_external_group_by {#max_bytes_before_external_group_by}

タイプ: UInt64

デフォルト値: 0

GROUP BY 操作中のメモリ使用量がこのバイト数の閾値を超えた場合は、'external aggregation' モードを有効にします（データをディスクにスピルします）。推奨値はシステムメモリの半分です。

## max_bytes_before_external_sort {#max_bytes_before_external_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY 操作中のメモリ使用量がこのバイト数の閾値を超えた場合は、'external sorting' モードを有効にします（データをディスクにスピルします）。推奨値はシステムメモリの半分です。

## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort}

タイプ: UInt64

デフォルト値: 1000000000

ORDER BY で LIMIT がある場合、メモリ使用量が指定された閾値を超えた場合、最終的なマージに向けてブロックを追加的にマージする手順を実行し、上位 LIMIT 行のみを保持します。

## max_bytes_in_distinct {#max_bytes_in_distinct}

タイプ: UInt64

デフォルト値: 0

DISTINCT の実行中の状態の最大合計サイズ（圧縮されていないバイト単位）をメモリ内で管理します。

## max_bytes_in_join {#max_bytes_in_join}

タイプ: UInt64

デフォルト値: 0

JOIN のためのハッシュテーブルの最大サイズ（メモリ内のバイト数）。

## max_bytes_in_set {#max_bytes_in_set}

タイプ: UInt64

デフォルト値: 0

IN セクションの実行結果の最大サイズ（メモリ内のバイト数）です。

## max_bytes_to_read {#max_bytes_to_read}

タイプ: UInt64

デフォルト値: 0

最も「深い」ソースからの読み取りバイト数の制限（解凍後）。つまり、最も深いサブクエリのみ。リモートサーバーから読み取るときは、この制限はリモートサーバーでのみ確認されます。

## max_bytes_to_read_leaf {#max_bytes_to_read_leaf}

タイプ: UInt64

デフォルト値: 0

分散クエリの葉ノードでの読み取りバイト数の制限。この制限はローカルリードのみに適用され、ルートノードでの最終的なマージステージを除外します。この設定は、prefer_localhost_replica=1 の場合に不安定です。

## max_bytes_to_sort {#max_bytes_to_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY 操作のために処理しなければならないバイト数が指定された量を超えた場合、その動作は 'sort_overflow_mode' によって決まります。デフォルトでは例外がスローされます。

## max_bytes_to_transfer {#max_bytes_to_transfer}

タイプ: UInt64

デフォルト値: 0

GLOBAL IN/JOIN セクションが実行されるときに、転送される外部テーブルの最大サイズ（圧縮されていないバイト単位）。

## max_columns_to_read {#max_columns_to_read}

タイプ: UInt64

デフォルト値: 0

クエリが指定された数のカラムを読み取る必要がある場合、例外がスローされます。ゼロの値は無制限を意味します。この設定は、あまりにも複雑なクエリを防ぐために便利です。

## max_compress_block_size {#max_compress_block_size}

タイプ: UInt64

デフォルト値: 1048576

テーブルに書き込むために圧縮する前の未圧縮データのブロックの最大サイズ。デフォルトは1,048,576（1 MiB）。一般に小さいブロックサイズを指定すると圧縮率がわずかに低下し、キャッシュの局所性により圧縮および解凍速度がわずかに向上し、メモリ消費が減少します。

:::note
これは専門的な設定であり、ClickHouse を始めたばかりの場合は変更しないでください。
:::

圧縮用のブロック（バイトのチャンクからなるメモリの断片）とクエリ処理用のブロック（テーブルからの行のセット）を混同しないでください。

## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users}

タイプ: UInt64

デフォルト値: 0

この設定の値が同時に処理されているクエリの現在の数以下の場合、例外をスローします。

例: `max_concurrent_queries_for_all_users` を99に設定し、データベース管理者は自身が100に設定することで、サーバーがオーバーロードされている場合でも調査のためのクエリを実行できます。

1つのクエリまたはユーザーの設定を変更しても、他のクエリには影響しません。

可能な値:

- 正の整数。
- 0 — 制限なし。

**例**

``` xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**関連情報**

- [max_concurrent_queries](/docs/ja/operations/server-configuration-parameters/settings.md/#max_concurrent_queries)

## max_concurrent_queries_for_user {#max_concurrent_queries_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーごとに同時に処理されるクエリの最大数。

可能な値:

- 正の整数。
- 0 — 制限なし。

**例**

``` xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```

## max_distributed_connections {#max_distributed_connections}

タイプ: UInt64

デフォルト値: 1024

単一の分散テーブルに対して単一のクエリのためのリモートサーバーとの同時接続の最大数。クラスター内のサーバーの数以上の値を設定することを推奨します。

次のパラメータは、分散テーブルを作成する際とサーバーを起動する際のみ使用されるため、実行時に変更する理由はありません。

## max_distributed_depth {#max_distributed_depth}

タイプ: UInt64

デフォルト値: 5

[Distributed](../../engines/table-engines/special/distributed.md) テーブルの再帰クエリの最大深さを制限します。

値が超えた場合、サーバーは例外をスローします。

可能な値:

- 正の整数。
- 0 — 深さ制限なし。

## max_download_buffer_size {#max_download_buffer_size}

タイプ: UInt64

デフォルト値: 10485760

各スレッドのための並行ダウンロード用バッファの最大サイズ（例: URL エンジン用）。

## max_download_threads {#max_download_threads}

タイプ: MaxThreads

デフォルト値: 4

データをダウンロードするための最大スレッド数（例: URL エンジン用）。

## max_estimated_execution_time {#max_estimated_execution_time}

タイプ: Seconds

デフォルト値: 0

クエリの推定最大実行時間（秒単位）。

## max_execution_speed {#max_execution_speed}

タイプ: UInt64

デフォルト値: 0

1秒あたりの実行行数の最大数。

## max_execution_speed_bytes {#max_execution_speed_bytes}

タイプ: UInt64

デフォルト値: 0

1秒あたりの実行バイト数の最大数。

## max_execution_time {#max_execution_time}

タイプ: Seconds

デフォルト値: 0

クエリの実行時間が指定された秒数を超えた場合、その動作は 'timeout_overflow_mode' によって決まります。デフォルトでは例外がスローされます。このタイムアウトはチェックされ、クエリはデータ処理の指定された場所でのみ停止できます。現在、集計状態のマージやクエリ分析中に停止することはできず、実際の実行時間はこの設定の値を超える可能性があります。

## max_execution_time_leaf {#max_execution_time_leaf}

タイプ: Seconds

デフォルト値: 0

max_execution_time に類似の意味を持ちますが、分散クエリの葉ノードでのみ適用されます。タイムアウトは 'timeout_overflow_mode_leaf' によって決定され、デフォルトでは例外がスローされます。

## max_expanded_ast_elements {#max_expanded_ast_elements}

タイプ: UInt64

デフォルト値: 500000

エイリアスとアスタリスクを拡張した後のクエリ構文木の最大サイズ（ノード数）。

## max_fetch_partition_retries_count {#max_fetch_partition_retries_count}

タイプ: UInt64

デフォルト値: 5

別のホストからパーティションを取得する際のリトライ回数。

## max_final_threads {#max_final_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

[FINAL](../../sql-reference/statements/select/from.md#select-from-final) 修飾子を持つ `SELECT` クエリのデータ読み取りフェーズに対する最大並列スレッド数を設定します。

可能な値:

- 正の整数。
- 0 または 1 — 無効。 `SELECT` クエリは単一スレッドで実行されます。

## max_http_get_redirects {#max_http_get_redirects}

タイプ: UInt64

デフォルト値: 0

許可される最大数のHTTP GETリダイレクトホップ。悪意のあるサーバーがリクエストを予期しないサービスにリダイレクトするのを防ぐために、追加のセキュリティ対策が講じられています。\n\nこれは、外部サーバーが他のアドレスにリダイレクトする場合ですが、そのアドレスが企業のインフラに内部的に見える場合に該当します。内部サーバーにHTTPリクエストを送信すると、認証をバイパスして内部ネットワークから内部APIを要求したり、Redis や Memcached などの他のサービスをクエリしたりすることができます。内部のインフラストラクチャ（ローカルホストで動作しているものを含む）を持っていない場合や、サーバーを信頼する場合は、リダイレクトを許可するのは安全です。ただし、URLがHTTPを使用している場合は、リモートサーバーだけでなく、ISPや中間のすべてのネットワークも信頼しなければなりません。

## max_hyperscan_regexp_length {#max_hyperscan_regexp_length}

タイプ: UInt64

デフォルト値: 0

[hyperscan マルチマッチ関数](../../sql-reference/functions/string-search-functions.md/#multimatchanyhaystack-pattern1-pattern2-patternn)での各正規表現の最大長を定義します。

可能な値:

- 正の整数。
- 0 — 長さに制限なし。

**例**

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

結果:

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

結果:

```text
Exception: Regexp length too large.
```

**関連情報**

- [max_hyperscan_regexp_total_length](#max-hyperscan-regexp-total-length)

## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length}

タイプ: UInt64

デフォルト値: 0

[hyperscan マルチマッチ関数](../../sql-reference/functions/string-search-functions.md/#multimatchanyhaystack-pattern1-pattern2-patternn)内のすべての正規表現の合計最大長を設定します。

可能な値:

- 正の整数。
- 0 — 長さに制限なし。

**例**

クエリ:

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果:

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果:

```text
Exception: Total regexp lengths too large.
```

**関連情報**

- [max_hyperscan_regexp_length](#max-hyperscan-regexp-length)

## max_insert_block_size {#max_insert_block_size}

タイプ: UInt64

デフォルト値: 1048449

テーブルに挿入するために形成されるブロックのサイズ（行数）です。この設定は、サーバーがブロックを形成する場合にのみ適用されます。例えば、HTTP インターフェース経由のINSERTの場合、サーバーはデータ形式を解析し、指定されたサイズのブロックを形成します。しかし、clickhouse-clientを使用する場合、クライアントはデータを独自に解析し、サーバーでの `max_insert_block_size` 設定は挿入されるブロックのサイズに影響を与えません。この設定は、INSERT SELECT を使用する場合には目的がありません。SELECT の後に形成される同じブロックを使用してデータが挿入されるためです。

デフォルト値は `max_block_size` よりわずかに大きくなっています。これは、特定のテーブルエンジン（`*MergeTree`）がディスクに挿入された各ブロックのデータパートを形成するため、かなり大きなエンティティになるためです。同様に、`*MergeTree` テーブルは挿入中にデータをソートし、十分に大きなブロックサイズによってメモリ内でより多くのデータをソートできるようにします。

## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write}

タイプ: UInt64

デフォルト値: 0

最終パートフラッシュを遅らせるための最大ストリーム（カラム）の数。デフォルトは自動（基盤となるストレージが並列書き込みをサポートしている場合は1000、それ以外は無効）。

## max_insert_threads {#max_insert_threads}

タイプ: UInt64

デフォルト値: 0

`INSERT SELECT` クエリを実行するための最大スレッド数。

可能な値:

- 0（または 1） — `INSERT SELECT` は並列実行されません。
- 正の整数。1より大きい。

クラウドデフォルト値: サービスのサイズに応じて2〜4。

並列 `INSERT SELECT` は、`SELECT` 部分が並列で実行されている場合にのみ効果があります。詳細については [max_threads](#max_threads) 設定を参照してください。より高い値は、より多くのメモリ使用量につながります。

## max_joined_block_size_rows {#max_joined_block_size_rows}

タイプ: UInt64

デフォルト値: 65409

JOIN 結果の最大ブロックサイズ（結合アルゴリズムがサポートしている場合）。0は無制限を意味します。

## max_limit_for_ann_queries {#max_limit_for_ann_queries}

タイプ: UInt64

デフォルト値: 1000000

この設定を超える LIMIT を持つ SELECT クエリは、ベクトル類似インデックスを使用できません。ベクトル類似インデックスでのメモリオーバーフローを防ぐのに役立ちます。

## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh}

タイプ: UInt64

デフォルト値: 64

マージ可能なブロックがドロップされ、クエリが再実行される前の最大挿入ブロック数を制限します。

## max_local_read_bandwidth {#max_local_read_bandwidth}

タイプ: UInt64

デフォルト値: 0

ローカルリードの最大速度（バイト毎秒）。

## max_local_write_bandwidth {#max_local_write_bandwidth}

タイプ: UInt64

デフォルト値: 0

ローカル書き込みの最大速度（バイト毎秒）。

## max_memory_usage {#max_memory_usage}

タイプ: UInt64

デフォルト値: 0

単一クエリの処理に対する最大メモリ使用量。ゼロは無制限を意味します。

## max_memory_usage_for_user {#max_memory_usage_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーによるすべての同時実行クエリの処理に対する最大メモリ使用量。ゼロは無制限を意味します。

## max_network_bandwidth {#max_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

ネットワーク越しのデータ交換の速度を制限します（バイト毎秒）。この設定は、すべてのクエリに適用されます。

可能な値:

- 正の整数。
- 0 — 帯域幅制御が無効です。

## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users}

タイプ: UInt64

デフォルト値: 0

ネットワーク越しのデータ交換の速度を制限します（バイト毎秒）。この設定は、サーバー上のすべての同時実行クエリに適用されます。

可能な値:

- 正の整数。
- 0 — データ速度制御が無効です。

## max_network_bandwidth_for_user {#max_network_bandwidth_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーによるネットワーク越しのデータ交換の速度を制限します（バイト毎秒）。この設定は、単一のユーザーが実行するすべての同時実行クエリに適用されます。

可能な値:

- 正の整数。
- 0 — データ速度制御が無効です。

## max_network_bytes {#max_network_bytes}

タイプ: UInt64

デフォルト値: 0

クエリを実行する際に、ネットワーク越しに受信または送信されるデータ量（バイト単位）を制限します。この設定は、各個別のクエリに適用されます。

可能な値:

- 正の整数。
- 0 — データ量制御が無効です。

## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation}

タイプ: UInt64

デフォルト値: 128

最適化を適用するためのテーブル内の最大パーティション数。

## max_parallel_replicas {#max_parallel_replicas}

タイプ: NonZeroUInt64

デフォルト値: 1

クエリを実行する際の各シャードに対する最大のレプリカ数。

可能な値:

- 正の整数。

**追加情報**

このオプションは使用される設定に応じて異なる結果を生じます。

:::note
この設定は、結合またはサブクエリが関与している場合や、すべてのテーブルが特定の要件を満たしていない場合に不正確な結果を生じます。詳細については、[Distributed Subqueries and max_parallel_replicas](../../sql-reference/operators/in.md/#max_parallel_replica-subqueries) を参照してください。
:::

### `SAMPLE` キーを使用した並列処理

クエリは、複数のサーバーで同時に実行される場合、より速く処理されることがあります。しかし、次のような場合にはクエリのパフォーマンスが低下する可能性があります。

- サンプリングキーの位置がパーティショニングキー内にあり、効率的な範囲スキャンができない。
- テーブルにサンプリングキーを追加することで、他のカラムによるフィルタリングの効率が低下する。
- サンプリングキーが計算コストの高い式である。
- クラスタのレイテンシ分布にロングテールがあるため、サーバーを増やすと全体のレイテンシが増加する。

### [parallel_replicas_custom_key](#parallel_replicas_custom_key) を使用した並列処理

この設定は、任意のレプリケートテーブルに便利です。

## max_parser_backtracks {#max_parser_backtracks}

タイプ: UInt64

デフォルト値: 1000000

再帰下降解析プロセス中に異なる代替を試みる最大バックトラック数。

## max_parser_depth {#max_parser_depth}

タイプ: UInt64

デフォルト値: 1000

再帰下降パーサーでの再帰の最大深さを制限します。スタックサイズを制御できます。

可能な値:

- 正の整数。
- 0 — 再帰の深さに制限なし。

## max_parsing_threads {#max_parsing_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

並行解析をサポートする入力形式のデータを解析するための最大スレッド数。デフォルトでは自動的に決定されます。

## max_partition_size_to_drop {#max_partition_size_to_drop}

タイプ: UInt64

デフォルト値: 50000000000

クエリ時のパーティション削除に対する制限。値が0の場合、制限なしでパーティションを削除できます。

クラウドデフォルト値: 1 TB。

:::note
このクエリ設定は、そのサーバー設定の同等のものを上書きします。詳細については、[max_partition_size_to_drop](/docs/ja/operations/server-configuration-parameters/settings.md/#max-partition-size-to-drop) を参照してください。
:::

## max_partitions_per_insert_block {#max_partitions_per_insert_block}

タイプ: UInt64

デフォルト値: 100

単一の INSERT されたブロック内の最大パーティション数を制限します。ゼロは無制限を意味します。ブロックにパーティションが多すぎる場合は例外をスローします。この設定は安全閾値です。多くのパーティションを使用することは一般的な誤解です。

## max_partitions_to_read {#max_partitions_to_read}

タイプ: Int64

デフォルト値: -1

1つのクエリでアクセスできる最大パーティション数を制限します。 <= 0 は無制限を意味します。

## max_parts_to_move {#max_parts_to_move}

タイプ: UInt64

デフォルト値: 1000

1つのクエリで移動できるパーツの数を制限します。ゼロは無制限を意味します。

## max_query_size {#max_query_size}

タイプ: UInt64

デフォルト値: 262144

SQL パーサーによって解析されるクエリ文字列の最大バイト数。
INSERT クエリの VALUES 句内のデータは、別のストリームパーサー（O(1) RAM を消費）によって処理され、この制限には影響しません。

:::note
`max_query_size` は SQL クエリ内（例えば、`SELECT now() SETTINGS max_query_size=10000`）では設定できません。ClickHouse はクエリを解析するためにバッファを割り当てる必要がありますが、このバッファサイズは実行前に設定する必要がある `max_query_size` 設定によって決まります。
:::

## max_read_buffer_size {#max_read_buffer_size}

タイプ: UInt64

デフォルト値: 1048576

ファイルシステムから読み取るためのバッファの最大サイズ。

## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs}

タイプ: UInt64

デフォルト値: 131072

ローカルファイルシステムから読み取るためのバッファの最大サイズ。 0 に設定した場合、max_read_buffer_size が使用されます。

## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs}

タイプ: UInt64

デフォルト値: 0

リモートファイルシステムから読み取るためのバッファの最大サイズ。 0 に設定した場合、max_read_buffer_size が使用されます。

## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth}

タイプ: UInt64

デフォルト値: 1000

再帰 CTE 評価深度の最大限度。

## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

読み取りのためのネットワーク越しのデータ交換の最大速度（バイト毎秒）。

## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

書き込みのためのネットワーク越しのデータ交換の最大速度（バイト毎秒）。

## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries}

タイプ: UInt64

デフォルト値: 300

分散クエリに対する遅延レプリカを無効にします。詳細は [Replication](../../engines/table-engines/mergetree-family/replication.md) を参照してください。

秒単位で時間を設定します。レプリカの遅延が設定値以上に大きい場合は、このレプリカは使用されません。

可能な値:

- 正の整数。
- 0 — レプリカの遅延はチェックされません。

非ゼロの遅延を持つレプリカを使用しないようにするには、このパラメータを1に設定します。

レプリケートされたテーブルを指す分散テーブルから `SELECT` を実行する際に使用されます。

## max_result_bytes {#max_result_bytes}

タイプ: UInt64

デフォルト値: 0

結果のサイズ（バイト）が制限されます（未圧縮）。閾値に達すると、データのブロックを処理した後にクエリは停止しますが、結果の最後のブロックは切り捨てられません。したがって、結果のサイズは閾値を超える可能性があります。注意事項: 結果のメモリ内サイズがこの閾値にカウントされます。結果のサイズが小さくても、LowCardinality カラムのDictionaryや、AggregateFunction カラムのアリーナのように、大きなデータ構造をメモリ内で参照している場合、この閾値を超える可能性があります。この設定は非常に低レベルであり、注意して使用する必要があります。

## max_result_rows {#max_result_rows}

タイプ: UInt64

デフォルト値: 0

結果サイズに関する制限（行）。閾値に達すると、データのブロックを処理した後にクエリは停止しますが、結果の最後のブロックは切り捨てられません。したがって、結果のサイズは閾値を超える可能性があります。

## max_rows_in_distinct {#max_rows_in_distinct}

タイプ: UInt64

デフォルト値: 0

DISTINCT の実行中の最大要素数。

## max_rows_in_join {#max_rows_in_join}

タイプ: UInt64

デフォルト値: 0

JOIN のためのハッシュテーブルの最大サイズ（行数）。

## max_rows_in_set {#max_rows_in_set}

タイプ: UInt64

デフォルト値: 0

IN セクションの実行結果の最大サイズ（要素数）。

## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join}

タイプ: UInt64

デフォルト値: 0

結合テーブルをお互いの行セットでフィルタリングするための最大セットサイズ。

可能な値:

- 0 — 無効。
- 任意の正の整数。

## max_rows_to_group_by {#max_rows_to_group_by}

タイプ: UInt64

デフォルト値: 0

GROUP BY中に指定された行数（ユニークなGROUP BYキー）が生成される場合、その動作は 'group_by_overflow_mode' によって決まります。デフォルトでは例外をスローされますが、大まかなGROUP BYモードに切り替えることもできます。

## max_rows_to_read {#max_rows_to_read}

タイプ: UInt64

デフォルト値: 0

最も「深い」ソースから読み取る行数の制限。つまり、最も深いサブクエリでのみ。リモートサーバーから読み取るときは、この制限はリモートサーバーでのみ確認されます。

## max_rows_to_read_leaf {#max_rows_to_read_leaf}

タイプ: UInt64

デフォルト値: 0

分散クエリの葉ノードでの読み取り行数の制限。この制限はローカルリードのみに適用され、ルートノードでの最終的なマージステージを除外します。この設定は、prefer_localhost_replica=1 の場合に不安定です。

## max_rows_to_sort {#max_rows_to_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY 操作のために処理しなければならないレコード数が指定された量を超えた場合、その動作は 'sort_overflow_mode' によって決まります。デフォルトでは例外がスローされます。

## max_rows_to_transfer {#max_rows_to_transfer}

タイプ: UInt64

デフォルト値: 0

GLOBAL IN/JOIN セクションが実行される際に伝達される外部テーブルの最大サイズ（行数）。

## max_sessions_for_user {#max_sessions_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーのための同時セッションの最大数。

## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation}

タイプ: UInt64

デフォルト値: 100000000

集約前に全ハッシュテーブルに合計で事前に確保を許可する要素数。

## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins}

タイプ: UInt64

デフォルト値: 100000000

結合前に全ハッシュテーブルに合計で事前に確保を許可する要素数。

## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading}

タイプ: UInt64

デフォルト値: 0

ゼロでない場合、MergeTree テーブルのための読み取りストリームの数を制限します。
## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables}

タイプ: Float

デフォルト値: 5

マージテーブルから読み込む際にストリームを追加します。ストリームは、マージテーブルが使用するテーブルに分散されます。これにより、スレッド間での作業の均等な分配が可能になり、特にマージされたテーブルのサイズが異なる場合に役立ちます。

## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio}

タイプ: Float

デフォルト値: 1

スレッドの数以上のソースを使用できるようにし、作業をスレッド間で均等に分配します。将来的には、ソースの数とスレッドの数を等しくし、それぞれのソースが自ら利用可能な作業を動的に選択できるようになると想定されています。

## max_subquery_depth {#max_subquery_depth}

タイプ: UInt64

デフォルト値: 100

クエリに指定された数以上のネストされたサブクエリが含まれている場合、例外を投げます。これにより、クラスターのユーザーがクエリで苦しむのを防ぐサニティチェックを提供します。

## max_table_size_to_drop {#max_table_size_to_drop}

タイプ: UInt64

デフォルト値: 50000000000

クエリ実行時にテーブルを削除する際の制限。値0は、制限なしで全てのテーブルを削除することを意味します。

クラウドのデフォルト値: 1 TB。

:::note
このクエリ設定は、サーバー設定の同等のものを上書きします。詳細は[こちら](docs/ja/operations/server-configuration-parameters/settings.md/#max-table-size-to-drop)を参照してください。
:::

## max_temporary_columns {#max_temporary_columns}

タイプ: UInt64

デフォルト値: 0

クエリが中間計算の結果としてメモリ内に指定された数以上の一時カラムを生成した場合、例外が投げられます。ゼロの値は無制限を意味します。この設定は、あまりにも複雑なクエリを防ぐのに役立ちます。

## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query}

タイプ: UInt64

デフォルト値: 0

すべての同時実行クエリに対して、ディスク上の一時ファイルによって消費されるデータの最大量（バイト単位）。ゼロは無制限を意味します。

## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user}

タイプ: UInt64

デフォルト値: 0

すべての同時実行ユーザークエリに対して、ディスク上の一時ファイルによって消費されるデータの最大量（バイト単位）。ゼロは無制限を意味します。

## max_temporary_non_const_columns {#max_temporary_non_const_columns}

タイプ: UInt64

デフォルト値: 0

`max_temporary_columns`設定に類似していますが、非定数カラムのみに適用されます。定数カラムはコストが低いため、より多くの定数カラムを許可するのが合理的です。

## max_threads {#max_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

クエリ処理スレッドの最大数。リモートサーバーからデータを取得するためのスレッドは除外されます（`max_distributed_connections`パラメータを参照）。

このパラメータは、クエリ処理パイプラインの同じ段階を並行して実行するスレッドに適用されます。例えば、テーブルから読み込む際、関数を用いた式の評価、WHEREでのフィルタリング、およびGROUP BYのための事前集計を最低でも`max_threads`の数のスレッドを使用して並行して実行可能であれば、`max_threads`が使用されます。

LIMITによって早く完了するクエリの場合、より少ない`max_threads`を設定できます。例えば、必要な数のエントリが各ブロックに存在し、max_threads = 8であれば、8つのブロックが取得されますが、実際には1つを読むだけで済みます。

`max_threads`の値が小さいほど、消費されるメモリは少なくなります。

## max_threads_for_indexes {#max_threads_for_indexes}

タイプ: UInt64

デフォルト値: 0

インデックスを処理するためのスレッドの最大数。

## max_untracked_memory {#max_untracked_memory}

タイプ: UInt64

デフォルト値: 4194304

小さなメモリの割り当てと解放はスレッドローカル変数にグループ化され、指定された値よりも大きくなるまでは追跡またはプロファイリングされません。値が'memory_profiler_step'を超える場合、実質的には'memory_profiler_step'に制限されます。

## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator}

タイプ: UInt64

デフォルト値: 1073741824

これは、グローバルレベルでハードリミットに達したときのソフトメモリリミットを表します。この値は、クエリのオーバーコミット比を計算するために使用されます。ゼロはクエリをスキップすることを意味します。
[メモリオーバーコミット](memory-overcommit.md)についての詳細をお読みください。

## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user}

タイプ: UInt64

デフォルト値: 1073741824

これは、ユーザーレベルでハードリミットに達したときのソフトメモリリミットを表します。この値は、クエリのオーバーコミット比を計算するために使用されます。ゼロはクエリをスキップすることを意味します。
[メモリオーバーコミット](memory-overcommit.md)についての詳細をお読みください。

## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size}

タイプ: UInt64

デフォルト値: 0

指定された値以下のサイズのランダムな割り当てを`memory_profiler_sample_probability`の確率で収集します。0は無効を意味します。これが期待通りに動作するために、`max_untracked_memory`を0に設定することをお勧めします。

## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size}

タイプ: UInt64

デフォルト値: 0

指定された値以上のサイズのランダムな割り当てを`memory_profiler_sample_probability`の確率で収集します。0は無効を意味します。これが期待通りに動作するために、`max_untracked_memory`を0に設定することをお勧めします。

## memory_profiler_sample_probability {#memory_profiler_sample_probability}

タイプ: Float

デフォルト値: 0

ランダムな割り当てと解放を収集し、'MemorySample'トレースタイプでsystem.trace_logに書き込みます。この確率は、サイズに関係なくすべての割り当て/解放に対して適用されます（`memory_profiler_sample_min_allocation_size`と`memory_profiler_sample_max_allocation_size`で変更可能です）。追跡されていないメモリの量が'max_untracked_memory'を超えたときのみサンプリングは行われます。追加の詳細なサンプリングのために`max_untracked_memory`を0に設定することをお勧めします。

## memory_profiler_step {#memory_profiler_step}

タイプ: UInt64

デフォルト値: 4194304

メモリプロファイラのステップを設定します。クエリのメモリ使用量が次のステップのバイト数よりも大きくなると、メモリプロファイラは割り当てスタックトレースを収集し、それを[trace_log](../../operations/system-tables/trace_log.md#system_tables-trace_log)に書き込みます。

可能な値:

- 正の整数（バイト単位）。

- メモリプロファイラをオフにするには0を設定します。

## memory_tracker_fault_probability {#memory_tracker_fault_probability}

タイプ: Float

デフォルト値: 0

`exception safety`のテスト - 指定された確率でメモリを割り当てるたびに例外を投げます。

## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds}

タイプ: UInt64

デフォルト値: 5000000

ユーザーレベルでのメモリオーバーコミットの場合、スレッドがメモリの解放を待つ最大時間。
タイムアウトに達し、メモリが解放されない場合、例外が投げられます。
[メモリオーバーコミット](memory-overcommit.md)についての詳細をお読みください。

## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity}

タイプ: UInt64

デフォルト値: 8

データを検索する際、ClickHouseはインデックスファイルのデータマークを確認します。必要なキーが特定の範囲に存在する場合、ClickHouseはこの範囲を`merge_tree_coarse_index_granularity`のサブ範囲に分割し、再帰的にそこに必要なキーを探索します。

可能な値:

- 任意の正の偶数整数。

## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read}

タイプ: UInt64

デフォルト値: 16

ClickHouse Cloud専用。MergeTreeテーブルのコンパクト部のストライプ内のグラニュール数で、マルチバッファリーダーを使用して並行読み込みとプリフェッチをサポートします。リモートfsから読み込む際にマルチバッファリーダーを使用すると、読み取り要求の数が増加します。

## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns}

タイプ: Bool

デフォルト値: 1

読み取りタスクのサイズを決定するためにのみ、前に指定されたカラムのサイズを使用するかどうか。

## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache}

タイプ: UInt64

デフォルト値: 2013265920

ClickHouseが1つのクエリで`merge_tree_max_bytes_to_use_cache`バイトを超えて読み込む必要がある場合、未圧縮ブロックのキャッシュは使用されません。

未圧縮ブロックのキャッシュは、クエリのために抽出されたデータを格納します。ClickHouseは、このキャッシュを使用して繰り返される小さなクエリへの応答を迅速化します。この設定は、大量のデータを読み込むクエリによってキャッシュが破壊されるのを防ぎます。[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)サーバー設定は、未圧縮ブロックのキャッシュのサイズを定義します。

可能な値:

- 任意の正の整数。

## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache}

タイプ: UInt64

デフォルト値: 1048576

ClickHouseが1つのクエリで`merge_tree_max_rows_to_use_cache`行を超えて読み込む必要がある場合、未圧縮ブロックのキャッシュは使用されません。

未圧縮ブロックのキャッシュは、クエリのために抽出されたデータを格納します。ClickHouseは、このキャッシュを使用して繰り返される小さなクエリへの応答を迅速化します。この設定は、大量のデータを読み込むクエリによってキャッシュが破壊されるのを防ぎます。[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)サーバー設定は、未圧縮ブロックのキャッシュのサイズを定義します。

可能な値:

- 任意の正の整数。

## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read}

タイプ: UInt64

デフォルト値: 251658240

MergeTreeエンジンテーブルの1ファイルから読み取るバイト数が`merge_tree_min_bytes_for_concurrent_read`を超える場合、ClickHouseはこのファイルから複数のスレッドでの並行読み取りを試みます。

可能な値:

- 正の整数。

## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem}

タイプ: UInt64

デフォルト値: 0

リモートファイルシステムからの読み取り時に、MergeTreeエンジンが読み取りを並列化できるようにする前に、1ファイルから読み取るバイト数の最小値です。この設定の使用は推奨されません。

可能な値:

- 正の整数。

## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek}

タイプ: UInt64

デフォルト値: 0

1ファイル内の2つのデータブロックの間の距離が`merge_tree_min_bytes_for_seek`バイト未満である場合、ClickHouseは両方のブロックを含むファイルの範囲を sequentiallyに読み込み、余分なシークを回避します。

可能な値:

- 任意の正の整数。

## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading}

タイプ: UInt64

デフォルト値: 2097152

タスクごとに読み取るバイト数の最小値。

## merge_tree_min_read_task_size {#merge_tree_min_read_task_size}

タイプ: UInt64

デフォルト値: 8

タスクサイズの絶対下限（グラニュール数が少なく、利用可能なスレッド数が多くても小さなタスクを割り当てません）。

## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read}

タイプ: UInt64

デフォルト値: 163840

1ファイルから読み取る行数が`merge_tree_min_rows_for_concurrent_read`を超える場合、ClickHouseはこのファイルから複数のスレッドでの並行読み取りを試みます。

可能な値:

- 正の整数。

## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem}

タイプ: UInt64

デフォルト値: 0

リモートファイルシステムからの読み取り時に、MergeTreeエンジンが読み取りを並列化できるようにする前に、1ファイルから読み取る行数の最小値です。この設定の使用は推奨されません。

可能な値:

- 正の整数。

## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek}

タイプ: UInt64

デフォルト値: 0

1ファイル内の2つのデータブロックの間の距離が`merge_tree_min_rows_for_seek`行未満である場合、ClickHouseはファイルをシークせず、データを逐次読み込みます。

可能な値:

- 任意の正の整数。

## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability}

タイプ: Float

デフォルト値: 0

`PartsSplitter`のテスト - MergeTreeからデータを読み取る際に、指定された確率で範囲を交差するものと交差しないものに分割します。

## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading}

タイプ: Bool

デフォルト値: 1

リモートテーブルから読み取る際に、固定サイズのタスクを使用するかどうか。

## metrics_perf_events_enabled {#metrics_perf_events_enabled}

タイプ: Bool

デフォルト値: 0

有効にすると、クエリの実行中に一部のパフォーマンスイベントが測定されます。

## metrics_perf_events_list {#metrics_perf_events_list}

タイプ: String

デフォルト値:

クエリの実行中に測定されるパフォーマンスメトリクスをコンマで区切ったリスト。空はすべてのイベントを意味します。利用可能なイベントについては、ソースのPerfEventInfoを参照してください。

## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io}

タイプ: UInt64

デフォルト値: 0

ストレージディスクに対してダイレクトI/Oアクセスを使用するために必要な最小データボリューム。

ClickHouseは、テーブルからデータを読み込む際にこの設定を使用します。読み取るデータの総ストレージボリュームが`min_bytes_to_use_direct_io`バイトを超える場合、ClickHouseは`O_DIRECT`オプションを使ってストレージディスクからデータを読み取ります。

可能な値:

- 0 — ダイレクトI/Oは無効。
- 正の整数。

## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io}

タイプ: UInt64

デフォルト値: 0

これはエクスペリメンタルな設定です。カーネルからユーザースペースにデータをコピーしないで大きなファイルを読み取るための最小メモリ量を設定します。推奨されるしきい値は約64 MBです。なぜなら、[mmap/munmap](https://en.wikipedia.org/wiki/Mmap)は遅いからです。これは大きなファイルのみに意味があり、データがページキャッシュに存在する場合にのみ役立ちます。

可能な値:

- 正の整数。
- 0 — 大きなファイルはカーネルからユーザースペースにデータをコピーするのみで読み込まれます。

## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing}

タイプ: UInt64

デフォルト値: 10485760

- タイプ: 正の整数
- デフォルト値: 1 MiB

各スレッドが並行して解析する最小チャンクサイズ（バイト単位）。

## min_compress_block_size {#min_compress_block_size}

タイプ: UInt64

デフォルト値: 65536

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブル用。クエリ処理時のレイテンシを減少させるために、次のマークを書き込む際、そのサイズが`min_compress_block_size`以上であればブロックが圧縮されます。デフォルトは65,536です。

未圧縮データの実際のサイズが`max_compress_block_size`未満であれば、ブロックのサイズはこの値以上、かつ1マーク分のデータ量以上でなければなりません。

例を見てみましょう。`index_granularity`がテーブル作成時に8192に設定されていたとします。

UInt32型のカラム（値あたり4バイト）を書き込む場合。8192行を書き込むと、合計32 KBのデータになります。`min_compress_block_size`が65,536であるため、2マークごとに圧縮ブロックが形成されます。

URL型のカラム（平均サイズ60バイト）の場合も見てみましょう。8192行を書き込むと、平均で500 KB弱のデータになります。これは65,536を超えているため、各マークごとに圧縮ブロックが形成されます。この場合、ディスクから単一のマークの範囲を読み込むときに余分なデータは解凍されません。

:::note
これは専門家向けの設定であり、ClickHouseを使用し始めたばかりの人が変更するべきではありません。
:::

## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression}

タイプ: UInt64

デフォルト値: 3

JITコンパイルを開始するための同一の集約式の最小数。この設定は、[compile_aggregate_expressions](#compile_aggregate_expressions)が有効になっている場合にのみ機能します。

可能な値:

- 正の整数。
- 0 — 同一の集約式は常にJITコンパイルされます。

## min_count_to_compile_expression {#min_count_to_compile_expression}

タイプ: UInt64

デフォルト値: 3

コンパイルが開始される前に実行される同じ式の最小カウント。

## min_count_to_compile_sort_description {#min_count_to_compile_sort_description}

タイプ: UInt64

デフォルト値: 3

JITコンパイルされる前の同一のソート説明の数。

## min_execution_speed {#min_execution_speed}

タイプ: UInt64

デフォルト値: 0

1秒あたりの実行行数の最小値。

## min_execution_speed_bytes {#min_execution_speed_bytes}

タイプ: UInt64

デフォルト値: 0

1秒あたりの実行バイト数の最小値。

## min_external_table_block_size_bytes {#min_external_table_block_size_bytes}

タイプ: UInt64

デフォルト値: 268402944

外部テーブルに渡されるブロックを指定されたバイトサイズに圧縮します。ブロックが十分に大きくない場合。

## min_external_table_block_size_rows {#min_external_table_block_size_rows}

タイプ: UInt64

デフォルト値: 1048449

外部テーブルに渡されたブロックを指定された行数に圧縮します。ブロックが十分に大きくない場合。

## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert}

タイプ: UInt64

デフォルト値: 0

挿入を行うために必要な最小空きディスク容量 (バイト単位)。

## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert}

タイプ: Float

デフォルト値: 0

挿入を行うために必要な最小空きディスクスペースの比率。

## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data}

タイプ: UInt64

デフォルト値: 0

外部ソートや集約で使用される一時データの書き込み中に保持する必要がある最小ディスクスペース。

## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization}

タイプ: Float

デフォルト値: 0.5

合計キー最適化を保持するために使用されるキャッシュの最小ヒット率。

## min_insert_block_size_bytes {#min_insert_block_size_bytes}

タイプ: UInt64

デフォルト値: 268402944

`INSERT`クエリによってテーブルに挿入できるブロック内の最小バイト数を設定します。小さいサイズのブロックは大きなブロックに圧縮されます。

可能な値:

- 正の整数。
- 0 — 圧縮無効。

## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views}

タイプ: UInt64

デフォルト値: 0

`INSERT`クエリによってテーブルに挿入できるブロック内の最小バイト数を設定します。小さいサイズのブロックは大きなブロックに圧縮されます。この設定は、[マテリアライズドビュー](../../sql-reference/statements/create/view.md)に挿入されるブロックのみに適用されます。この設定を調整することで、マテリアライズドビューにプッシュする際のブロック圧縮を制御し、過剰なメモリ使用を回避します。

可能な値:

- 任意の正の整数。
- 0 — 圧縮無効。

**詳細情報**

- [min_insert_block_size_bytes](#min-insert-block-size-bytes)

## min_insert_block_size_rows {#min_insert_block_size_rows}

タイプ: UInt64

デフォルト値: 1048449

`INSERT`クエリによってテーブルに挿入できるブロック内の最小行数を設定します。小さいサイズのブロックは大きなブロックに圧縮されます。

可能な値:

- 正の整数。
- 0 — 圧縮無効。

## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views}

タイプ: UInt64

デフォルト値: 0

`INSERT`クエリによってテーブルに挿入できるブロック内の最小行数を設定します。小さいサイズのブロックは大きなブロックに圧縮されます。この設定は、[マテリアライズドビュー](../../sql-reference/statements/create/view.md)に挿入されるブロックのみに適用されます。この設定を調整することで、マテリアライズドビューにプッシュする際のブロック圧縮を制御し、過剰なメモリ使用を回避します。

可能な値:

- 任意の正の整数。
- 0 — 圧縮無効。

**詳細情報**

- [min_insert_block_size_rows](#min-insert-block-size-rows)

## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query}

タイプ: Bool

デフォルト値: 1

有効にすると、MongoDBクエリを構築できない場合、MongoDBテーブルはエラーを返します。そうでない場合、ClickHouseはテーブル全体を読み込み、ローカルで処理します。このオプションは、レガシー実装や`allow_experimental_analyzer=0`のときには適用されません。

## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere}

タイプ: Bool

デフォルト値: 1

WHEREからPREWHEREにすべての適用可能な条件を移動します。

## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere}

タイプ: Bool

デフォルト値: 1

主キー列を含むPREWHERE条件をANDチェーンの最後に移動します。これらの条件は主キー分析中に考慮される可能性が高く、PREWHEREフィルタリングには多く寄与しないと考えられます。

## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names}

タイプ: Bool

デフォルト値: 0

複数の結合のリライト時にトップレベルの式リストにエイリアスを追加しない。

## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator}

タイプ: Bool

デフォルト値: 0

この値がtrueの場合、定数非決定性関数（例えば、関数`now()`）はイニシエーター上で実行され、`UPDATE`および`DELETE`クエリのリテラルに置き換えられます。これは、定数非決定性関数を含む変異を実行中にデータをレプリカ間で同期させるのに役立ちます。デフォルト値: `false`。

## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator}

タイプ: Bool

デフォルト値: 0

この値がtrueの場合、スカラサブクエリはイニシエーター上で実行され、`UPDATE`および`DELETE`クエリのリテラルに置き換えられます。デフォルト値: `false`。

## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace}

タイプ: UInt64

デフォルト値: 16384

`UPDATE`および`DELETE`クエリで置換されるシリアライズされたリテラルの最大サイズ（バイト単位）。上記の2つの設定のうち少なくとも1つが有効な場合にのみ有効です。デフォルト値: 16384（16 KiB）。

## mutations_sync {#mutations_sync}

タイプ: UInt64

デフォルト値: 0

`ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN`クエリ（[変異](../../sql-reference/statements/alter/index.md#mutations)）を同期的に実行することを許可します。

可能な値:

- 0 - 変異は非同期で実行されます。
- 1 - クエリは現在のサーバー上で全ての変異が完了するのを待ちます。
- 2 - クエリは全てのレプリカ（存在する場合）で全ての変異が完了するのを待ちます。

## mysql_datatypes_support_level {#mysql_datatypes_support_level}

タイプ: MySQLDataTypesSupport

デフォルト値:

MySQLデータ型がそれに対応するClickHouseデータ型に変換される方法を定義します。`decimal`、`datetime64`、`date2Date32`または`date2String`のいずれかの組み合わせにコンマで区切られたリスト。
- `decimal`: 精度が許す限り、`NUMERIC`および`DECIMAL`型を`Decimal`に変換します。
- `datetime64`: 精度が`0`でない場合、`DATETIME`および`TIMESTAMP`型を`DateTime64`に変換します。
- `date2Date32`: `DATE`を`Date32`に変換します（`Date`の代わりに）。これは`date2String`より優先されます。
- `date2String`: `DATE`を`String`に変換します（`Date`の代わりに）。`datetime64`によって上書きされます。

## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns}

タイプ: Bool

デフォルト値: 1

有効にすると、[FixedString](../../sql-reference/data-types/fixedstring.md)のClickHouseデータ型は、[SHOW COLUMNS](../../sql-reference/statements/show.md#show_columns)で`TEXT`として表示されます。

これは、MySQLワイヤプロトコルを介して接続されている場合にのみ有効です。

- 0 - `BLOB`を使用。
- 1 - `TEXT`を使用。

## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns}

タイプ: Bool

デフォルト値: 1

有効にすると、[String](../../sql-reference/data-types/string.md)のClickHouseデータ型は、[SHOW COLUMNS](../../sql-reference/statements/show.md#show_columns)で`TEXT`として表示されます。

これは、MySQLワイヤプロトコルを介して接続されている場合にのみ有効です。

- 0 - `BLOB`を使用。
- 1 - `TEXT`を使用。

## mysql_max_rows_to_insert {#mysql_max_rows_to_insert}

タイプ: UInt64

デフォルト値: 65536

MySQLストレージエンジンのMySQLバッチ挿入における最大行数。

## network_compression_method {#network_compression_method}

タイプ: String

デフォルト値: LZ4

サーバー間およびサーバーと[clickhouse-client](../../interfaces/cli.md)間の通信に使用されるデータ圧縮方式を設定します。

可能な値:

- `LZ4` — LZ4圧縮方式を設定。
- `ZSTD` — ZSTD圧縮方式を設定。

**詳細情報**

- [network_zstd_compression_level](#network_zstd_compression_level)

## network_zstd_compression_level {#network_zstd_compression_level}

タイプ: Int64

デフォルト値: 1

ZSTD圧縮のレベルを調整します。[network_compression_method](#network_compression_method)が`ZSTD`に設定されているときのみ使用されます。

可能な値:

- 1から15までの正の整数。

## normalize_function_names {#normalize_function_names}

タイプ: Bool

デフォルト値: 1

関数名をその正規名に正規化します。

## number_of_mutations_to_delay {#number_of_mutations_to_delay}

タイプ: UInt64

デフォルト値: 0

変異したテーブルに未処理の変異が少なくともその数ある場合、テーブルの変異を人工的に遅らせる。0 - 無効。

## number_of_mutations_to_throw {#number_of_mutations_to_throw}

タイプ: UInt64

デフォルト値: 0

変異したテーブルに未処理の変異が少なくともその数ある場合、 'Too many mutations ...' 例外を投げる。0 - 無効。

## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size}

タイプ: UInt64

デフォルト値: 16

ODBCブリッジ内の各接続設定文字列の接続プールサイズ。

## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling}

タイプ: Bool

デフォルト値: 1

ODBCブリッジで接続プールを使用します。falseに設定すると、毎回新しい接続が作成されます。

## offset {#offset}

タイプ: UInt64

デフォルト値: 0

クエリから返される行の前にスキップする行数を設定します。[OFFSET](../../sql-reference/statements/select/offset.md/#offset-fetch)句によって設定されたオフセットを調整し、これら2つの値を合計します。

可能な値:

- 0 — 行はスキップされません。
- 正の整数。

**例**

入力テーブル:

``` sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

クエリ:

``` sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```
結果:

``` text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```

## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability}

タイプ: Float

デフォルト値: 0

ClickHouseが実行されたクエリのトレースを開始できる確率を設定します（親[トレースコンテキスト](https://www.w3.org/TR/trace-context/)が提供されていない場合）。

可能な値:

- 0 — 実行されたすべてのクエリのトレースが無効になります（親トレースコンテキストが提供されていない場合）。
- 0..1の範囲内の正の浮動小数点数。例えば、設定値が`0.5`の場合、ClickHouseは平均して半分のクエリでトレースを開始することができます。
- 1 — 実行されたすべてのクエリのトレースが有効になります。

## opentelemetry_trace_processors {#opentelemetry_trace_processors}

タイプ: Bool

デフォルト値: 0

プロセッサのためにOpenTelemetryスパンを収集します。

## optimize_aggregation_in_order {#optimize_aggregation_in_order}

タイプ: Bool

デフォルト値: 0

MergeTreeテーブルのデータを対応する順序で集約するために[SELECT](../../sql-reference/statements/select/index.md)クエリの`GROUP BY`最適化を有効にします。

可能な値:

- 0 — `GROUP BY`最適化は無効。
- 1 — `GROUP BY`最適化は有効。

**詳細情報**

- [GROUP BY最適化](../../sql-reference/statements/select/group-by.md/#aggregation-in-order)

## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys}

タイプ: Bool

デフォルト値: 1

SELECTセクションでGROUP BYキーの最小/最大/any/anyLastアグリゲーターを排除します。

## optimize_append_index {#optimize_append_index}

タイプ: Bool

デフォルト値: 0

インデックス条件を追加するために[制約](../../sql-reference/statements/create/table.md#constraints)を使用します。デフォルトは`false`です。

可能な値:

- true, false

## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions}

タイプ: Bool

デフォルト値: 1

集約関数の外で算術演算を移動します。

## optimize_count_from_files {#optimize_count_from_files}

タイプ: Bool

デフォルト値: 1

異なる入力形式のファイルからの行数のカウントの最適化を有効または無効にします。これはテーブル関数/エンジン`file`/`s3`/`url`/`hdfs`/`azureBlobStorage`に適用されます。

可能な値:

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_distinct_in_order {#optimize_distinct_in_order}

タイプ: Bool

デフォルト値: 1

DISTINCTの最適化を有効にし、DISTINCTの一部のカラムがソートのプレフィックスを形成する場合に適用されます。例えば、マージツリーまたはORDER BYステートメントのソートキーのプレフィックス。

## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key}

タイプ: Bool

デフォルト値: 1

コストのかかるイニシエーターサーバーでの集約を避けることにより、`GROUP BY sharding_key`クエリを最適化します。これにより、イニシエーターサーバーでのメモリ使用量が削減されます。
以下のタイプのクエリがサポートされています（そのすべての組み合わせも含まれます）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

次の種類のクエリはサポートされていません（いくつかは後でサポートされる可能性があります）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

可能な値：

- 0 — 無効。
- 1 — 有効。

参照：

- [distributed_group_by_no_merge](#distributed-group-by-no-merge)
- [distributed_push_down_limit](#distributed-push-down-limit)
- [optimize_skip_unused_shards](#optimize-skip-unused-shards)

:::note
現在のところ、`optimize_skip_unused_shards`が必要です（これには、データが分散テーブル経由で挿入され、そのためデータがsharding_keyに従って分散されている場合にのみ正しく動作するという理由があります）。
:::

## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns}

タイプ: Bool

デフォルト値: 1

サブカラムの読み込みに変換することによって最適化を有効または無効にします。これにより、読み込むデータの量が減ります。

これらの関数は変換可能です：

- [length](../../sql-reference/functions/array-functions.md/#array_functions-length)を[サイズ0](../../sql-reference/data-types/array.md/#array-size)サブカラムを読み込むために使用。
- [empty](../../sql-reference/functions/array-functions.md/#function-empty)を[サイズ0](../../sql-reference/data-types/array.md/#array-size)サブカラムを読み込むために使用。
- [notEmpty](../../sql-reference/functions/array-functions.md/#function-notempty)を[サイズ0](../../sql-reference/data-types/array.md/#array-size)サブカラムを読み込むために使用。
- [isNull](../../sql-reference/operators/index.md#operator-is-null)を[nullable](../../sql-reference/data-types/nullable.md/#finding-null)サブカラムを読み込むために使用。
- [isNotNull](../../sql-reference/operators/index.md#is-not-null)を[nullable](../../sql-reference/data-types/nullable.md/#finding-null)サブカラムを読み込むために使用。
- [count](../../sql-reference/aggregate-functions/reference/count.md)を[nullable](../../sql-reference/data-types/nullable.md/#finding-null)サブカラムを読み込むために使用。
- [mapKeys](../../sql-reference/functions/tuple-map-functions.md/#mapkeys)を[keys](../../sql-reference/data-types/map.md/#map-subcolumns)サブカラムを読み込むために使用。
- [mapValues](../../sql-reference/functions/tuple-map-functions.md/#mapvalues)を[values](../../sql-reference/data-types/map.md/#map-subcolumns)サブカラムを読み込むために使用。

可能な値：

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_group_by_constant_keys {#optimize_group_by_constant_keys}

タイプ: Bool

デフォルト値: 1

すべてのキーが定数である場合のGROUP BYを最適化します。

## optimize_group_by_function_keys {#optimize_group_by_function_keys}

タイプ: Bool

デフォルト値: 1

GROUP BYセクションの他のキーの関数を排除します。

## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif}

タイプ: Bool

デフォルト値: 0

if(cond1, then1, if(cond2, ...))のチェーンをmultiIfに置き換えます。現在、数値型には有益ではありません。

## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum}

タイプ: Bool

デフォルト値: 0

IfおよびTransformの文字列型引数をenumに置き換えます。デフォルトでは無効です。これは、分散クエリに矛盾した変更をもたらし、それが失敗につながる可能性があるためです。

## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by}

タイプ: Bool

デフォルト値: 1

GROUP BYセクションで引数によって射影関数を置き換えます。

## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq}

タイプ: Bool

デフォルト値: 1

uniq*()関数内の単一引数の射影関数を削除します。

## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length}

タイプ: UInt64

デフォルト値: 3

最適化のための式`expr = x1 OR ... expr = xN`の最小長さ。

## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length}

タイプ: UInt64

デフォルト値: 3

最適化のための式`expr <> x1 AND ... expr <> xN`の最小長さ。

## optimize_move_to_prewhere {#optimize_move_to_prewhere}

タイプ: Bool

デフォルト値: 1

[SELECT](../../sql-reference/statements/select/index.md)クエリでの自動[PREFIX](../../sql-reference/statements/select/prewhere.md)最適化を有効または無効にします。

これは[*MergeTree](../../engines/table-engines/mergetree-family/index.md)テーブルにのみ適用されます。

可能な値：

- 0 — 自動`PREWHERE`最適化無効。
- 1 — 自動`PREWHERE`最適化有効。

## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final}

タイプ: Bool

デフォルト値: 0

[FINAL](../../sql-reference/statements/select/from.md#select-from-final)修飾子を持つ[SELECT](../../sql-reference/statements/select/index.md)クエリにおける自動[PREFIX](../../sql-reference/statements/select/prewhere.md)最適化を有効または無効にします。

これは[*MergeTree](../../engines/table-engines/mergetree-family/index.md)テーブルにのみ適用されます。

可能な値：

- 0 — `FINAL`修飾子のある`SELECT`クエリでの自動`PREWHERE`最適化無効。
- 1 — `FINAL`修飾子のある`SELECT`クエリでの自動`PREWHERE`最適化有効。

**参照**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere)設定

## optimize_multiif_to_if {#optimize_multiif_to_if}

タイプ: Bool

デフォルト値: 1

`multiIf`の条件を1つに置き換えます。

## optimize_normalize_count_variants {#optimize_normalize_count_variants}

タイプ: Bool

デフォルト値: 1

semantically equal count()として集計関数を再記述します。

## optimize_on_insert {#optimize_on_insert}

タイプ: Bool

デフォルト値: 1

挿入の前にデータ変換を有効または無効にします。これは、テーブルエンジンに従って（マージがこのブロックで実行されたかのように）行われます。

可能な値：

- 0 — 無効。
- 1 — 有効。

**例**

有効と無効の違い：

クエリ：

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

結果：

``` text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

この設定は[Materialized view](../../sql-reference/statements/create/view.md/#materialized)の動作に影響を与えることに注意してください。

## optimize_or_like_chain {#optimize_or_like_chain}

タイプ: Bool

デフォルト値: 0

複数のOR LIKEをmultiMatchAnyに最適化します。この最適化はデフォルトでは無効にすべきです。なぜなら、場合によってはインデックス分析を無効にするためです。

## optimize_read_in_order {#optimize_read_in_order}

タイプ: Bool

デフォルト値: 1

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブルからデータを読み取るための[ORDER BY](../../sql-reference/statements/select/order-by.md/#optimize_read_in_order)最適化を有効または無効にします。

可能な値：

- 0 — `ORDER BY`最適化無効。
- 1 — `ORDER BY`最適化有効。

**参照**

- [ORDER BY句](../../sql-reference/statements/select/order-by.md/#optimize_read_in_order)

## optimize_read_in_window_order {#optimize_read_in_window_order}

タイプ: Bool

デフォルト値: 1

MergeTreeテーブルでのウィンドウ句におけるORDER BY最適化を有効にします。

## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by}

タイプ: Bool

デフォルト値: 1

ORDER BY内の引数がORDER BYに含まれている場合、ORDER BYから関数を削除します。

## optimize_respect_aliases {#optimize_respect_aliases}

タイプ: Bool

デフォルト値: 1

この設定がtrueに設定されている場合、WHERE/GROUP BY/ORDER BY内のエイリアスを尊重し、パーティションプルーニング/セカンダリインデックス/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_countで助けになります。

## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if}

タイプ: Bool

デフォルト値: 1

論理的に等しい場合、if式を引数とする集計関数を再記述します。たとえば、`avg(if(cond, col, null))`は`avgOrNullIf(cond, col)`に再記述できます。性能を改善する可能性があります。

:::note
これは分析器（`enable_analyzer = 1`）でのみサポートされます。
:::

## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has}

タイプ: Bool

デフォルト値: 0

論理的に等しい場合、arrayExists()関数をhas()に再記述します。たとえば、arrayExists(x -> x = 1, arr)はhas(arr, 1)に再記述できます。

## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if}

タイプ: Bool

デフォルト値: 1

論理的に等しい場合、sumIf()およびsum(if())関数をcountIf()関数に再記述します。

## optimize_skip_merged_partitions {#optimize_skip_merged_partitions}

タイプ: Bool

デフォルト値: 0

一つのパーティションにアクティブなパーツが1つだけあり、期限切れのTTLがない場合に、[OPTIMIZE TABLE ... FINAL](../../sql-reference/statements/optimize.md)クエリのための最適化を有効または無効にします。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

デフォルトでは、`OPTIMIZE TABLE ... FINAL`クエリは、単一の部分があっても書き換えられます。

可能な値：

- 1 - 最適化有効。
- 0 - 最適化無効。

## optimize_skip_unused_shards {#optimize_skip_unused_shards}

タイプ: Bool

デフォルト値: 0

`WHERE/PREWHERE`の条件にsharding keyが含まれている[SELECT](../../sql-reference/statements/select/index.md)クエリに対して未使用のシャードのスキップを有効または無効にします（データがsharding keyによって分散されていると仮定）。さもなければ、クエリは不正確な結果を返します。

可能な値：

- 0 — 無効。
- 1 — 有効。

## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit}

タイプ: UInt64

デフォルト値: 1000

sharding key値の制限で、制限に達することによって`optimize_skip_unused_shards`をオフにします。

あまりにも多くの値があると処理に多大な時間がかかる可能性がありますが、利益は疑わしいです。なぜなら、`IN (...)`内に大量の値がある場合、ほとんどの場合、クエリはすべてのシャードに送信されるためです。

## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting}

タイプ: UInt64

デフォルト値: 0

分散クエリのネストレベルに応じて[`optimize_skip_unused_shards`](#optimize-skip-unused-shards)を制御します（たとえば、別の`Distributed`テーブルを参照する`Distributed`テーブルがある場合）。

可能な値：

- 0 — 無効、`optimize_skip_unused_shards`は常に動作します。
- 1 — 最初のレベルに対してのみ`optimize_skip_unused_shards`を有効にします。
- 2 — 2番目のレベルまで`optimize_skip_unused_shards`を有効にします。

## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in}

タイプ: Bool

デフォルト値: 1

リモートシャードのクエリ内のINを再記述し、シャードに属さない値を除外します（`optimize_skip_unused_shards`が必要です）。

可能な値：

- 0 — 無効。
- 1 — 有効。

## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties}

タイプ: Bool

デフォルト値: 1

入力ストリームのプロパティによるソートを最適化します。

## optimize_substitute_columns {#optimize_substitute_columns}

タイプ: Bool

デフォルト値: 0

[制約](../../sql-reference/statements/create/table.md#constraints)を使用してカラムの代替を行います。デフォルト値は`false`です。

可能な値：

- true, false

## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions}

タイプ: Bool

デフォルト値: 0

同一引数を持つ集計関数を融合させます。これは、`sum`、`count`、または`avg`の同一引数を持つ少なくとも2つの集計関数を`sumCount`に書き換えます。

可能な値：

- 0 — 同一引数を持つ関数は融合されない。
- 1 — 同一引数を持つ関数は融合される。

**例**

クエリ：

``` sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

結果：

``` text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```

## optimize_throw_if_noop {#optimize_throw_if_noop}

タイプ: Bool

デフォルト値: 0

マージを実行しなかった場合、[OPTIMIZE](../../sql-reference/statements/optimize.md)クエリが例外をスローするかどうかを有効または無効にします。

デフォルトでは、`OPTIMIZE`は何も行わなかった場合でも正常に戻ります。この設定を使用すると、これらの状況を区別し、例外メッセージで理由を取得できます。

可能な値：

- 1 — 例外をスローすることを有効にします。
- 0 — 例外をスローすることを無効にします。

## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage}

タイプ: Bool

デフォルト値: 1

変換なしで関数を同等の比較に変換することによって、日付および日時の述語を最適化します（例：toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'）。

## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query}

タイプ: Bool

デフォルト値: 0

この最適化をサポートするストレージのトリビアルカウントの近似値を使用します。たとえば、EmbeddedRocksDB。

可能な値：

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_trivial_count_query {#optimize_trivial_count_query}

タイプ: Bool

デフォルト値: 1

MergeTreeのメタデータを使用して、テーブルから`SELECT count()`のトリビアルクエリの最適化を有効または無効にします。行レベルのセキュリティを使用する必要がある場合は、この設定を無効にしてください。

可能な値：

- 0 — 最適化無効。
- 1 — 最適化有効。

参照：

- [optimize_functions_to_subcolumns](#optimize-functions-to-subcolumns)

## optimize_trivial_insert_select {#optimize_trivial_insert_select}

タイプ: Bool

デフォルト値: 0

トリビアルな'INSERT INTO table SELECT ... FROM TABLES'クエリを最適化します。

## optimize_uniq_to_count {#optimize_uniq_to_count}

タイプ: Bool

デフォルト値: 1

uniqおよびそのバリアント（uniqUpToを除く）を、サブクエリにdistinctまたはgroup by句がある場合にcountに書き換えます。

## optimize_use_implicit_projections {#optimize_use_implicit_projections}

タイプ: Bool

デフォルト値: 1

SELECTクエリを実行するために暗黙の投影を自動的に選択します。

## optimize_use_projections {#optimize_use_projections}

タイプ: Bool

デフォルト値: 1

`SELECT`クエリを処理する際の[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)最適化を有効または無効にします。

可能な値：

- 0 — プロジェクション最適化無効。
- 1 — プロジェクション最適化有効。

## optimize_using_constraints {#optimize_using_constraints}

タイプ: Bool

デフォルト値: 0

クエリ最適化のために[制約](../../sql-reference/statements/create/table.md#constraints)を使用します。デフォルトでは`false`です。

可能な値：

- true, false

## os_thread_priority {#os_thread_priority}

タイプ: Int64

デフォルト値: 0

クエリを実行するスレッドの優先度（[nice](https://en.wikipedia.org/wiki/Nice_(Unix)))を設定します。OSスケジューラーは、この優先度を考慮して、各利用可能なCPUコアで次に実行するスレッドを選択します。

:::note
この設定を使用するには、`CAP_SYS_NICE` の能力を設定する必要があります。`clickhouse-server`パッケージは、インストール中にこれを設定します。一部の仮想環境では、`CAP_SYS_NICE`の設定が許可されていません。この場合、`clickhouse-server`は起動時にそのメッセージを表示します。
:::

可能な値：

- 値は`[-20, 19]`の範囲で設定できます。

値が低いほど、優先度が高くなります。優先度の低い`nice`のスレッドは、優先度の高いスレッドよりも頻繁に実行されます。高い値は、長期間実行される非対話型クエリに好ましいです。なぜなら、これにより、短期間の対話型クエリが到着したときに、リソースをすばやく譲渡できるからです。

## output_format_compression_level {#output_format_compression_level}

タイプ: UInt64

デフォルト値: 3

クエリ出力が圧縮されている場合のデフォルトの圧縮レベル。この設定は、`SELECT`クエリが`INTO OUTFILE`を持っている場合、またはテーブル関数`file`、`url`、`hdfs`、`s3`、または`azureBlobStorage`に書き込む場合に適用されます。

可能な値：1から22まで。

## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log}

タイプ: UInt64

デフォルト値: 0

出力圧縮メソッドが`zstd`の場合に使用できます。0より大きい場合、この設定は圧縮ウィンドウのサイズ（2の冪）を明示的に設定し、zstd圧縮のロングレンジモードを有効にします。これにより、圧縮率が向上する可能性があります。

可能な値：非負の数。ただし、値が小さすぎるか大きすぎると、`zstdlib`は例外をスローします。典型的な値は`20`（ウィンドウサイズ=`1MB`）から`30`（ウィンドウサイズ=`1GB`）までです。

## output_format_parallel_formatting {#output_format_parallel_formatting}

タイプ: Bool

デフォルト値: 1

データフォーマットの並列フォーマットを有効または無効にします。サポートされるのは、[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)フォーマットのみです。

可能な値：

- 1 — 有効。
- 0 — 無効。

## page_cache_inject_eviction {#page_cache_inject_eviction}

タイプ: Bool

デフォルト値: 0

ユーザースペースのページキャッシュは、ランダムにいくつかのページを無効にする場合があります。テスト用です。

## parallel_distributed_insert_select {#parallel_distributed_insert_select}

タイプ: UInt64

デフォルト値: 0

並列分散`INSERT ... SELECT`クエリを有効にします。

`INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b`クエリを実行した場合、両方のテーブルが同じクラスタを使用し、両方のテーブルが[レプリケートされた](../../engines/table-engines/mergetree-family/replication.md)または非レプリケートである場合、このクエリは各シャードでローカルに処理されます。

可能な値：

- 0 — 無効。
- 1 — `SELECT`が分散エンジンの基になるテーブルの各シャードで実行されます。
- 2 — `SELECT`および`INSERT`が分散エンジンの基になるテーブルの各シャードで実行されます。

## parallel_replica_offset {#parallel_replica_offset}

タイプ: UInt64

デフォルト値: 0

これは内部設定であり、直接使用すべきではなく、'parallel replicas'モードの実装の詳細を表します。この設定は、並列レプリカの中でクエリ処理に参加しているレプリカのインデックスを示すために、分散クエリのイニシエーターサーバーによって自動的に設定されます。

## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery}

タイプ: Bool

デフォルト値: 1

trueの場合、INのサブクエリはすべてのフォロワーレプリカで実行されます。

## parallel_replicas_count {#parallel_replicas_count}

タイプ: UInt64

デフォルト値: 0

これは内部設定であり、直接使用すべきではなく、'parallel replicas'モードの実装の詳細を表します。この設定は、分散クエリのイニシエーターサーバーによって、クエリ処理に参加する並列レプリカの数を自動的に設定されます。

## parallel_replicas_custom_key {#parallel_replicas_custom_key}

タイプ: String

デフォルト値:

特定のテーブル間でレプリカ間の作業を分割するために使用できる任意の整数式。
値は任意の整数式を取ることができます。

主キーを使用した単純な式が推奨されます。

この設定が、複数のレプリカを持つ単一のシャードで構成されるクラスタで使用されている場合、これらのレプリカは仮想シャードに変換されます。
そうでなければ、それは`SAMPLE`キーと同じように動作し、各シャードの複数のレプリカを使用します。

## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower}

タイプ: UInt64

デフォルト値: 0

フィルタータイプ`range`がカスタム範囲`[parallel_replicas_custom_key_range_lower, INT_MAX]`に基づいてレプリカ間で作業を均等に分割できるようにします。

[parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper)と併用することで、カスタムキー範囲に基づいてレプリカ間で作業を均等に分割することができます。

注意：この設定は、クエリ処理中に追加のデータがフィルタリングされることはなく、並列処理のために範囲`[0, INT_MAX]`が分割されるポイントを変更します。

## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper}

タイプ: UInt64

デフォルト値: 0

フィルタータイプ`range`がカスタム範囲`[0, parallel_replicas_custom_key_range_upper]`に基づいてレプリカ間で作業を均等に分割できるようにします。0の値は上限を無効にし、カスタムキー式の最大値を設定します。

[parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower)と併用することで、カスタムキー範囲`[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]`でレプリカ間で作業を均等に分割できます。

注意：この設定は、クエリ処理中に追加のデータがフィルタリングされることはなく、並列処理のために範囲`[0, INT_MAX]`が分割されるポイントを変更します。

## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree}

タイプ: Bool

デフォルト値: 0

trueの場合、ClickHouseは非レプリケートのMergeTreeテーブルにも並列レプリカアルゴリズムを使用します。

## parallel_replicas_local_plan {#parallel_replicas_local_plan}

タイプ: Bool

デフォルト値: 1

ローカルレプリカのためのローカルプランを構築します。

## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size}

タイプ: UInt64

デフォルト値: 0

パーツが仮想的にセグメントに分割され、レプリカ間で並列読み取りのために配布されます。この設定は、これらのセグメントのサイズを制御します。確実な場合を除いて変更することはお勧めしません。値は[128; 16384]の範囲であるべきです。

## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica}

タイプ: UInt64

デフォルト値: 0

クエリで使用されるレプリカの数を（読み取る推定行数 / min_number_of_rows_per_replica）に制限します。制限は、'max_parallel_replicas'によって引き続き制限されます。

## parallel_replicas_mode {#parallel_replicas_mode}

タイプ: ParallelReplicasMode

デフォルト値: read_tasks

カスタムキーに対して並列レプリカで使用するフィルタのタイプ。デフォルト - カスタムキーでモジュロ演算を使用する、範囲 - カスタムキーに対して範囲フィルタを使用する。

## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join}

タイプ: Bool

デフォルト値: 1

trueの場合、JOINが並列レプリカアルゴリズムで実行でき、右JOIN部分のすべてのストレージが*MergeTreeの場合、ローカルJOINが使用され、GLOBAL JOINの代わりに使用されます。

## parallel_view_processing {#parallel_view_processing}

タイプ: Bool

デフォルト値: 0

添付されたビューに対して逐次的ではなく並行してプッシュすることを有効にします。

## parallelize_output_from_storages {#parallelize_output_from_storages}

タイプ: Bool

デフォルト値: 1

ストレージからの読み取りステップの出力を並列化します。これは、可能であれば、ストレージからの読み取りの直後にクエリ処理の並列化を許可します。

## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros}

タイプ: Bool

デフォルト値: 1

関数'parseDateTime()'内のフォーマッタ'%c'、'%l'および'%k'は、ゼロ埋めなしで月と時を解析します。

## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes}

タイプ: UInt64

デフォルト値: 0

0でない場合、部分的なマージJOINの左側テーブルのために、左側テーブルのブロックを大きくグループ化します。結合スレッドごとに指定メモリの最大2倍を使用します。

## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks}

タイプ: UInt64

デフォルト値: 65536

[JOIN](../../sql-reference/statements/select/join.md)クエリの部分的マージJOINアルゴリズムで、右側のJOINデータブロックのサイズを制限します。

ClickHouseサーバーは：

1.  右側のJOINデータを指定された行数のブロックに分割します。
2.  各ブロックをその最小値と最大値でインデックス化します。
3.  可能であれば準備されたブロックをディスクにアンロードします。

可能な値：

- 任意の正の整数。推奨される値の範囲：\[1000, 100000\]。

## partial_result_on_first_cancel {#partial_result_on_first_cancel}

タイプ: Bool

デフォルト値: 0

キャンセル後に部分結果を返すことを許可します。

## parts_to_delay_insert {#parts_to_delay_insert}

タイプ: UInt64

デフォルト値: 0

宛先テーブルにアクティブなパーツが1つのパーティション内に少なくともこの数存在する場合、テーブルへの挿入を人工的に遅延させます。

## parts_to_throw_insert {#parts_to_throw_insert}

タイプ: UInt64

デフォルト値: 0

宛先テーブルの単一パーティション内にアクティブなパーツがこの数を超える場合、'Too many parts ...'例外をスローします。

## periodic_live_view_refresh {#periodic_live_view_refresh}

タイプ: Seconds

デフォルト値: 60

定期的に更新されたライブビューが強制的にリフレッシュされる間隔。

## poll_interval {#poll_interval}

タイプ: UInt64

デフォルト値: 10

サーバー上のクエリ待機ループで指定された秒数ブロックします。

## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout}

タイプ: UInt64

デフォルト値: 2

PostgreSQLエンドポイントへの単一接続試行の秒単位接続タイムアウト。
この値は接続URLの`connect_timeout`パラメータとして渡されます。

## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection}

タイプ: Bool

デフォルト値: 0

プールに返す前に接続を閉じます。

## postgresql_connection_pool_retries {#postgresql_connection_pool_retries}

タイプ: UInt64

デフォルト値: 2

PostgreSQLテーブルエンジンおよびデータベースエンジンに対する接続プールのプッシュ/ポップリトライ数。

## postgresql_connection_pool_size {#postgresql_connection_pool_size}

タイプ: UInt64

デフォルト値: 16

PostgreSQLテーブルエンジンおよびデータベースエンジンのための接続プールサイズ。

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout}

タイプ: UInt64

デフォルト値: 5000

PostgreSQLテーブルエンジンおよびデータベースエンジンの空のプールに対する接続プールのプッシュ/ポップタイムアウト。デフォルトでは、空のプールの上でブロックします。

## prefer_column_name_to_alias {#prefer_column_name_to_alias}

タイプ: Bool

デフォルト値: 0

クエリ式および句内でエイリアスの代わりに元のカラム名を使用するかどうかを有効または無効にします。これは、エイリアスがカラム名と同じである場合に特に重要です。[Expression Aliases](../../sql-reference/syntax.md/#notes-on-usage)を参照してください。この設定を有効にすると、ClickHouseのエイリアス構文ルールはほとんどの他のデータベースエンジンとより互換性が高くなります。

可能な値：

- 0 — カラム名はエイリアスに置き換えられます。
- 1 — カラム名はエイリアスに置き換えられません。

**例**

有効と無効の違い：

クエリ：

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果：

```text
Received exception from server (version 21.5.1):
Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.
```

クエリ：

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果：

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```

## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes}

タイプ: UInt64

デフォルト値: 16744704

外部ソートのための最大ブロックバイトを優先し、マージ時のメモリ使用量を削減します。

## prefer_global_in_and_join {#prefer_global_in_and_join}

タイプ: Bool

デフォルト値: 0

`IN`/`JOIN`演算子を`GLOBAL IN`/`GLOBAL JOIN`に置き換えることを有効または無効にします。

可能な値：

- 0 — 無効。 `IN`/`JOIN`演算子は`GLOBAL IN`/`GLOBAL JOIN`に置き換えられません。
- 1 — 有効。 `IN`/`JOIN`演算子は`GLOBAL IN`/`GLOBAL JOIN`に置き換えられます。

**使用方法**

`SET distributed_product_mode=global`が分散テーブルに対するクエリの動作を変更することがありますが、ローカルテーブルや外部リソースのテーブルには適していません。この時に`prefer_global_in_and_join`設定が登場します。

たとえば、ローカルテーブルが含まれるクエリ処理ノードがあり、これらは分散処理に適していません。`GLOBAL`キーワードを使用して分散処理中にデータをその場で散らばらせる必要があります—`GLOBAL IN`/`GLOBAL JOIN`を使用します。
`prefer_global_in_and_join`のもう一つの使用例は、外部エンジンによって作成されたテーブルにアクセスすることです。この設定は、そのようなテーブルを結合する際に外部ソースへの呼び出し回数を減らすのに役立ちます：クエリごとに1回の呼び出しのみです。

**参照：**

- `GLOBAL IN`/`GLOBAL JOIN`の使用方法についての詳細は、[分散サブクエリ](../../sql-reference/operators/in.md/#select-distributed-subqueries)を参照してください。

## prefer_localhost_replica {#prefer_localhost_replica}

タイプ: Bool

デフォルト値: 1

分散クエリを処理する際に、ローカルホストのレプリカを優先的に使用するかどうかを有効/無効にします。

可能な値：

- 1 — ClickHouseはローカルホストのレプリカが存在する場合、常にクエリをそのレプリカに送信します。
- 0 — ClickHouseは、[load_balancing](#load_balancing)設定で指定されたバランス戦略を使用します。

:::note
[parallel_replicas_custom_key](#parallel_replicas_custom_key)を使用せずに[max_parallel_replicas](#max_parallel_replicas)を使用している場合は、この設定を無効にしてください。
[parallel_replicas_custom_key](#parallel_replicas_custom_key)が設定されている場合、複数のレプリカを含む複数のシャードを持つクラスターで使用する場合にのみ、この設定を無効にしてください。
単一のシャードと複数のレプリカを持つクラスターで使用する場合、この設定を無効にすると悪影響があります。
:::

## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds}

タイプ: Int64

デフォルト値: 0

ClickHouse Cloudでのみ利用可能です。マージされていない部分がこの秒数未満で古く、プリウォームされていない場合（cache_populated_by_fetchを参照）、かつすべてのソース部分が利用可能でプリウォームされている場合、SELECTクエリはそれらの部分から読み取ります。ReplicatedMergeTree専用です。この設定は、CacheWarmerがその部分を処理したかどうかのみをチェックします。もしその部分が他の何かによってキャッシュに取得されていても、CacheWarmerがそれを処理するまで「コールド」と見なされます。もしプリウォームされてキャッシュから排除された場合も、「ウォーム」と見なされます。

## preferred_block_size_bytes {#preferred_block_size_bytes}

タイプ: UInt64

デフォルト値: 1000000

この設定は、クエリ処理のためのデータブロックサイズを調整し、粗い 'max_block_size' 設定に対する追加的な微調整を表します。カラムが大きく、'max_block_size' 行が指定されたバイト数よりも大きい場合、そのサイズはCPUキャッシュの局所性を改善するために低くされます。

## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes}

タイプ: UInt64

デフォルト値: 0

読み取り時のブロック内の最大カラムサイズの制限です。キャッシュミスの回数を減少させるのに役立ちます。L2キャッシュサイズに近い必要があります。

## preferred_optimize_projection_name {#preferred_optimize_projection_name}

タイプ: String

デフォルト値:

空でない文字列に設定されている場合、ClickHouseはクエリで指定されたプロジェクションを適用しようとします。

可能な値：

- 文字列: 好みのプロジェクションの名前

## prefetch_buffer_size {#prefetch_buffer_size}

タイプ: UInt64

デフォルト値: 1048576

ファイルシステムから読み取るためのプリフェッチバッファの最大サイズです。

## print_pretty_type_names {#print_pretty_type_names}

タイプ: Bool

デフォルト値: 1

`DESCRIBE`クエリおよび`toTypeName()`関数内で、深くネストされたタイプの名前をインデントを付けて整形して印刷できるようにします。

例：

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```

## priority {#priority}

タイプ: UInt64

デフォルト値: 0

クエリの優先度。1 - 最も高い、値が高いほど優先度は低い；0 - 優先度を使用しない。

## query_cache_compress_entries {#query_cache_compress_entries}

タイプ: Bool

デフォルト値: 1

[クエリキャッシュ](../query-cache.md)内のエントリを圧縮します。クエリキャッシュのメモリ消費を抑えますが、挿入や読み取りの速度は低下します。

可能な値：

- 0 - 無効
- 1 - 有効

## query_cache_max_entries {#query_cache_max_entries}

タイプ: UInt64

デフォルト値: 0

現在のユーザーが[クエリキャッシュ](../query-cache.md)に格納できるクエリ結果の最大数。0は無制限を意味します。

可能な値：

- 0以上の正の整数。

## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes}

タイプ: UInt64

デフォルト値: 0

現在のユーザーが[クエリキャッシュ](../query-cache.md)に割り当てられる最大メモリ量（バイト単位）。0は無制限を意味します。

可能な値：

- 0以上の正の整数。

## query_cache_min_query_duration {#query_cache_min_query_duration}

タイプ: ミリ秒

デフォルト値: 0

クエリの結果を[クエリキャッシュ](../query-cache.md)に保存するために、クエリが実行されなければならない最小の時間（ミリ秒単位）。

可能な値：

- 0以上の正の整数。

## query_cache_min_query_runs {#query_cache_min_query_runs}

タイプ: UInt64

デフォルト値: 0

`SELECT`クエリが結果を[クエリキャッシュ](../query-cache.md)に保存する前に、実行する必要がある最小回数。

可能な値：

- 0以上の正の整数。

## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling}

タイプ: QueryCacheNondeterministicFunctionHandling

デフォルト値: throw

非決定論的関数（例えば、`rand()`や`now()`）を含む`SELECT`クエリに対して[クエリキャッシュ](../query-cache.md)がどのように処理するかを制御します。

可能な値：

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしない。
- `'save'` - クエリ結果をキャッシュする。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしない。

## query_cache_share_between_users {#query_cache_share_between_users}

タイプ: Bool

デフォルト値: 0

有効にすると、[クエリキャッシュ](../query-cache.md)にキャッシュされた`SELECT`クエリの結果を他のユーザーが読み取れるようになります。この設定を有効にすることは、セキュリティ上の理由から推奨されません。

可能な値：

- 0 - 無効
- 1 - 有効

## query_cache_squash_partial_results {#query_cache_squash_partial_results}

タイプ: Bool

デフォルト値: 1

部分結果ブロックを[max_block_size](#setting-max_block_size)のサイズのブロックに圧縮します。[クエリキャッシュ](../query-cache.md)への挿入のパフォーマンスを低下させますが、キャッシュエントリの圧縮性を向上させます（[query_cache_compress-entries](#query_cache_compress-entries)を参照）。

可能な値：

- 0 - 無効
- 1 - 有効

## query_cache_system_table_handling {#query_cache_system_table_handling}

タイプ: QueryCacheSystemTableHandling

デフォルト値: throw

システムテーブル（すなわち、`system.*`および`information_schema.*`データベースのテーブル）に対する`SELECT`クエリに対して、[クエリキャッシュ](../query-cache.md)がどのように処理するかを制御します。

可能な値：

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしない。
- `'save'` - クエリ結果をキャッシュする。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしない。

## query_cache_tag {#query_cache_tag}

タイプ: String

デフォルト値:

[クエリキャッシュ](../query-cache.md)エントリにラベルとして機能する文字列です。同じクエリが異なるタグを持つ場合、クエリキャッシュでは異なるものと見なされます。

可能な値：

- 任意の文字列

## query_cache_ttl {#query_cache_ttl}

タイプ: 秒

デフォルト値: 60

この時間（秒単位）を経過すると、[クエリキャッシュ](../query-cache.md)のエントリが古くなります。

可能な値：

- 0以上の正の整数。

## query_metric_log_interval {#query_metric_log_interval}

タイプ: Int64

デフォルト値: -1

個々のクエリに対する[query_metric_log](../../operations/system-tables/query_metric_log.md)の収集間隔（ミリ秒単位）です。

任意の負の値に設定された場合、[query_metric_log設定](../../operations/server-configuration-parameters/settings.md#query_metric_log)から`collect_interval_milliseconds`の値を使用するか、存在しない場合は1000にデフォルトされます。

単一クエリの収集を無効にするには、`query_metric_log_interval`を0に設定します。

デフォルト値: -1

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order}

タイプ: Bool

デフォルト値: 1

集約を順序通りに処理するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join}

タイプ: Bool

デフォルト値: 1

JOINの後にフィルタが常にデフォルト値をフィルタリングする場合、OUTER JOINをINNER JOINに変換することを許可します。

## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions}

タイプ: Bool

デフォルト値: 1

ウィンドウ関数を評価した後にマルチスレッド処理を可能にします。

## query_plan_enable_optimizations {#query_plan_enable_optimizations}

タイプ: Bool

デフォルト値: 1

クエリプランレベルでのクエリ最適化を切り替えます。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - クエリプランレベルでのすべての最適化を無効
- 1 - クエリプランレベルでの最適化を有効（ただし、個々の最適化は個別の設定で無効にされる可能性があります）

## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting}

タイプ: Bool

デフォルト値: 1

ソート処理の後に式を移動するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_filter_push_down {#query_plan_filter_push_down}

タイプ: Bool

デフォルト値: 1

実行プランでフィルタを下に移動するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_lift_up_array_join {#query_plan_lift_up_array_join}

タイプ: Bool

デフォルト値: 1

ARRAY JOINを実行プランの上部に移動するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_lift_up_union {#query_plan_lift_up_union}

タイプ: Bool

デフォルト値: 1

クエリプランの大きな部分木を一つのユニオンに移動するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply}

タイプ: UInt64

デフォルト値: 10000

クエリプランに適用される最適化の総数を制限します。[query_plan_enable_optimizations](#query_plan_enable_optimizations)設定を参照してください。
複雑なクエリについて長時間の最適化を避けるのに役立ちます。
この設定を超える最適化の実際の数がある場合、例外がスローされます。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

## query_plan_merge_expressions {#query_plan_merge_expressions}

タイプ: Bool

デフォルト値: 1

連続するフィルタをマージするクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_merge_filters {#query_plan_merge_filters}

タイプ: Bool

デフォルト値: 0

クエリプラン内のフィルタをマージすることを許可します。

## query_plan_optimize_prewhere {#query_plan_optimize_prewhere}

タイプ: Bool

デフォルト値: 1

サポートされているストレージのためにフィルタをPREWHERE式にプッシュダウンすることを許可します。

## query_plan_push_down_limit {#query_plan_push_down_limit}

タイプ: Bool

デフォルト値: 1

実行プランでLIMITを下に移動するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_read_in_order {#query_plan_read_in_order}

タイプ: Bool

デフォルト値: 1

順序通りに読み取る最適化クエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct}

タイプ: Bool

デフォルト値: 1

冗長なDISTINCTステップを削除するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting}

タイプ: Bool

デフォルト値: 1

冗長なソートステップを削除するクエリプランレベルの最適化を切り替えます（例えば、サブクエリ内など）。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions}

タイプ: Bool

デフォルト値: 1

ウィンドウ関数のソート時にストレージソートを再使用するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

可能な値：

- 0 - 無効
- 1 - 有効

## query_plan_split_filter {#query_plan_split_filter}

タイプ: Bool

デフォルト値: 1

:::note
これは専門家レベルの設定であり、開発者によるデバッグのためにのみ使用するべきです。この設定は、将来的に後方互換性のない方法で変更されたり、削除されたりする可能性があります。
:::

フィルタを式に分割するクエリプランレベルの最適化を切り替えます。設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

可能な値：

- 0 - 無効
- 1 - 有効

## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}

タイプ: UInt64

デフォルト値: 1000000000

[クエリプロファイラ](../../operations/optimizing-performance/sampling-query-profiler.md)のCPUクロックタイマーの期間を設定します。このタイマーはCPU時間のみをカウントします。

可能な値：

- 正の整数のナノ秒。

    推奨値：

            - 単一クエリに対しては10000000（秒に100回）のナノ秒以上。
            - クラスター全体のプロファイリングには1000000000（1秒に1回）。

- タイマーをオフにするには0。

**ClickHouse Cloudでは一時的に無効です。**

参照：

- システムテーブル[trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}

タイプ: UInt64

デフォルト値: 1000000000

[クエリプロファイラ](../../operations/optimizing-performance/sampling-query-profiler.md)の実際のクロックタイマーの期間を設定します。実際のクロックタイマーは壁時計時間をカウントします。

可能な値：

- 正の整数数（ナノ秒）。

    推奨値：

            - 単一クエリに対しては10000000（秒に100回）ナノ秒未満。
            - クラスター全体のプロファイリングには1000000000（1秒に1回）。

- タイマーをオフにするには0。

**ClickHouse Cloudでは一時的に無効です。**

参照：

- システムテーブル[trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## queue_max_wait_ms {#queue_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 0

同時リクエストの数が最大を超えた場合の、リクエストキュー内の待機時間です。

## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 5000

リトライ前にRabbitMQから読み取る際の待機時間です。

## read_backoff_max_throughput {#read_backoff_max_throughput}

タイプ: UInt64

デフォルト値: 1048576

遅い読み取りが発生した場合にスレッドの数を減らすための設定です。読み取り帯域幅がこのバイト数/秒を下回るイベントをカウントします。

## read_backoff_min_concurrency {#read_backoff_min_concurrency}

タイプ: UInt64

デフォルト値: 1

遅い読み取りが発生した場合に最小限のスレッド数を維持しようとする設定です。

## read_backoff_min_events {#read_backoff_min_events}

タイプ: UInt64

デフォルト値: 2

遅い読み取りが発生した場合にスレッドの数を減らすための設定です。スレッド数を減少させる前にカウントされるイベント数です。

## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms}

タイプ: ミリ秒

デフォルト値: 1000

遅い読み取りが発生した場合にスレッドの数を減らすための設定です。前のイベントが一定の時間未満しか経過していない場合、そのイベントを無視します。

## read_backoff_min_latency_ms {#read_backoff_min_latency_ms}

タイプ: ミリ秒

デフォルト値: 1000

遅い読み取りが発生した場合にスレッドの数を減らすための設定です。この時間以上かかる読み取りのみに注意します。

## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache}

タイプ: Bool

デフォルト値: 0

ファイルシステムキャッシュを受動モードで使用することを許可します - 既存のキャッシュエントリからの利益を得るが、新しいエントリをキャッシュに入れません。この設定を重いアドホッククエリに設定し、短いリアルタイムクエリに対して無効にすることで、過度のクエリによるキャッシュのスラッシングを避け、全体のシステム効率を改善できます。

## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache}

タイプ: Bool

デフォルト値: 0

受動モードでユーザースペースのページキャッシュを使用します。read_from_filesystem_cache_if_exists_otherwise_bypass_cacheに類似しています。

## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold}

タイプ: UInt64

デフォルト値: 100

プライマリキーの順序で複数スレッドで読み取る際にプレリミナリマージステップを実行するために読み取る必要がある部分の最小数です。

## read_in_order_use_buffering {#read_in_order_use_buffering}

タイプ: Bool

デフォルト値: 1

プライマリキーの順序で読み取る際にマージ前にバッファリングを使用します。クエリ実行の並列性が向上します。

## read_overflow_mode {#read_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合の処理の方法です。

## read_overflow_mode_leaf {#read_overflow_mode_leaf}

タイプ: OverflowMode

デフォルト値: throw

リーフ制限を超えた場合の処理の方法です。

## read_priority {#read_priority}

タイプ: Int64

デフォルト値: 0

ローカルファイルシステムまたはリモートファイルシステムからデータを読み取る優先度。ローカルファイルシステムの`pread_threadpool`メソッドとリモートファイルシステムの`threadpool`メソッドに対してのみサポートされています。

## read_through_distributed_cache {#read_through_distributed_cache}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloudでのみ。分散キャッシュからの読み取りを許可します。

## readonly {#readonly}

タイプ: UInt64

デフォルト値: 0

0 - 読み取り専用制限なし。1 - 読み取りリクエストのみ、および明示的に許可された設定のみ変更可能。2 - 読み取りリクエストのみ、および設定を変更可能（'readonly'設定を除く）。

## receive_data_timeout_ms {#receive_data_timeout_ms}

タイプ: ミリ秒

デフォルト値: 2000

最初のデータパケットまたはレプリカからの進行状況を示すパケットを受信するための接続タイムアウトです。

## receive_timeout {#receive_timeout}

タイプ: 秒

デフォルト値: 300

ネットワークからデータを受信するためのタイムアウト（秒単位）。この間にバイトが受信されなかった場合、例外がスローされます。クライアントでこの設定を設定すると、ソケットの'send_timeout'もサーバーの対応する接続の終端で設定されます。

## regexp_max_matches_per_row {#regexp_max_matches_per_row}

タイプ: UInt64

デフォルト値: 1000

行ごとに単一の正規表現の最大マッチ数を設定します。[extractAllGroupsHorizontal](../../sql-reference/functions/string-search-functions.md/#extractallgroups-horizontal)関数で欲張りな正規表現を使用する際のメモリ過負荷から保護するために使用します。

可能な値：

- 正の整数。

## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps}

タイプ: Bool

デフォルト値: 1

ハイパースキャンで評価するのが高コストになると考えられるパターンを拒否します（NFA状態の爆発による）。

## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio}

タイプ: Float

デフォルト値: 2

再マージ後のメモリ使用量がこの比率で減少しない場合、再マージは無効化されます。

## remote_filesystem_read_method {#remote_filesystem_read_method}

タイプ: String

デフォルト値: threadpool

リモートファイルシステムからデータを読み取る方法。readまたはthreadpoolのいずれか。

## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch}

タイプ: Bool

デフォルト値: 1

リモートファイルシステムからデータを読み取る際にプリフェッチを使用すべきかどうかを示します。

## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries}

タイプ: UInt64

デフォルト値: 5

バックオフのための最大読み取り試行回数です。

## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms}

タイプ: UInt64

デフォルト値: 10000

リモートディスクからデータを読み取ろうとする際の最大待機時間です。

## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek}

タイプ: UInt64

デフォルト値: 4194304

リモート読み取り（URL、S3）でseekを実行するために必要な最小バイト数であり、無視して読み取らずに実行します。

## rename_files_after_processing {#rename_files_after_processing}

タイプ: String

デフォルト値:

- **タイプ:** String

- **デフォルト値:** 空の文字列

この設定により、`file`テーブル関数によって処理されたファイルのリネームパターンを指定できます。オプションが設定されると、`file`テーブル関数によって読み取られたすべてのファイルは、処理が成功した場合に指定されたパターンに従ってプレースホルダ付きでリネームされます。

### プレースホルダ

- `%a` — 完全な元のファイル名（例: "sample.csv"）。
- `%f` — 拡張子なしの元のファイル名（例: "sample"）。
- `%e` — 元のファイルの拡張子（例: ".csv"）。
- `%t` — タイムスタンプ（マイクロ秒）。
- `%%` — パーセント記号 ("%")。

### 例
- オプション: `--rename_files_after_processing="processed_%f_%t%e"`

- クエリ: `SELECT * FROM file('sample.csv')`

`sample.csv`の読み取りが成功した場合、ファイルは`processed_sample_1683473210851438.csv`にリネームされます。

## replace_running_query {#replace_running_query}

タイプ: Bool

デフォルト値: 0

HTTPインターフェースを使用する際、'query_id'パラメータを渡すことができます。これは、クエリの識別子として機能する任意の文字列です。
この時、同じユーザーから同じ'query_id'のクエリがすでに存在する場合、動作は'replace_running_query'パラメータによって異なります。

`0`（デフォルト） – 例外をスローし、同じ'query_id'のクエリがすでに実行中の場合、そのクエリを実行できません。

`1` – 古いクエリをキャンセルし、新しいクエリを実行し始めます。

このパラメータを1に設定するとセグメンテーション条件の提案を実装することができます。次の文字を入力すると、古いクエリがまだ完了していなければキャンセルされるべきです。

## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 5000

[replace_running_query](#replace-running-query)設定がアクティブな場合、同じ`query_id`のクエリを実行するのを待つための時間です。

可能な値：

- 正の整数。
- 0 — 同じ`query_id`のクエリが既に実行されている場合、新しいクエリを実行することを許可しない例外をスローします。

## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout}

タイプ: Int64

デフォルト値: 120

非アクティブなレプリカが[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または[TRUNCATE](../../sql-reference/statements/truncate.md)クエリを実行するのを待つ時間（秒単位）を指定します。

可能な値：

- 0 — 待機しない。
- 負の整数 — 無制限に待機。
- 正の整数 — 待機する秒数。

## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null}

タイプ: Bool

デフォルト値: 0

復元時に外部DictionaryソースをNullに置き換えます。テスト目的に有用です。

## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null}

タイプ: Bool

デフォルト値: 0

テスト目的のため。すべての外部エンジンをNullに置き換え、外部接続を開始しないようにします。

## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null}

タイプ: Bool

デフォルト値: 0

テスト目的のため。すべての外部テーブル関数をNullに置き換え、外部接続を開始しないようにします。

## result_overflow_mode {#result_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合の処理の方法です。

## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation}

タイプ: Bool

デフォルト値: 0

`countDistcintIf`を[count_distinct_implementation](#count_distinct_implementation)設定で書き換えることを許可します。

可能な値：

- true — 許可。
- false — 不許可。

## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload}

タイプ: Bool

デフォルト値: 1

S3のマルチパートアップロードに複数スレッドを使用します。これにより、わずかにメモリ使用量が増加する可能性があります。

## s3_check_objects_after_upload {#s3_check_objects_after_upload}

タイプ: Bool

デフォルト値: 0

アップロードが成功したことを確認するために、HEADリクエストでアップロードされた各オブジェクトをS3にチェックします。

## s3_connect_timeout_ms {#s3_connect_timeout_ms}

タイプ: UInt64

デフォルト値: 1000

S3ディスクのホストへの接続タイムアウトです。

## s3_create_new_file_on_insert {#s3_create_new_file_on_insert}

タイプ: Bool

デフォルト値: 0

S3エンジンテーブルへの各挿入時に新しいファイルを作成するかどうかを有効または無効にします。有効にすると、各挿入で次のパターンに似た新しいS3オブジェクトが作成されます：

初期: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz`など。

可能な値：
- 0 — `INSERT`クエリはファイルの最後に新しいデータを追加します。
- 1 — `INSERT`クエリは新しいファイルを作成します。

## s3_disable_checksum {#s3_disable_checksum}

タイプ: Bool

デフォルト値: 0

ファイルをS3に送信する際にチェックサムを計算しない。これにより、ファイルに過剰な処理がかかるのを避け、書き込み速度が向上します。MergeTreeテーブルのデータはClickHouseによってすでにチェックサムされているため、主に安全です。また、S3にHTTPSでアクセスする場合、TLSレイヤーはネットワークを介しての転送中に整合性を提供します。S3に追加のチェックサムを計算することで、深層防御を提供します。

## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist}

タイプ: Bool

デフォルト値: 0

特定のキーを読み取るときにファイルが存在しない場合の無視動作です。

可能な値：
- 1 — `SELECT`は空の結果を返します。
- 0 — `SELECT`は例外をスローします。

## s3_list_object_keys_size {#s3_list_object_keys_size}

タイプ: UInt64

デフォルト値: 1000

ListObjectリクエストによってバッチで返される可能性のある最大ファイル数です。

## s3_max_connections {#s3_max_connections}

タイプ: UInt64

デフォルト値: 1024

サーバーごとの最大接続数です。

## s3_max_get_burst {#s3_max_get_burst}

タイプ: UInt64

デフォルト値: 0

リクエストごとの秒の制限に達する前に同時に発行できるリクエストの最大数。デフォルト（0）は`s3_max_get_rps`に等しいです。

## s3_max_get_rps {#s3_max_get_rps}

タイプ: UInt64

デフォルト値: 0

スロットリングの前のS3 GETリクエストの毎秒の制限。ゼロは無限を意味します。

## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file}

タイプ: UInt64

デフォルト値: 20

マルチパートアップロードリクエストで同時に読み込まれる部分の最大数。0は無制限を意味します。

## s3_max_part_number {#s3_max_part_number}

タイプ: UInt64

デフォルト値: 10000

最大部分番号数のs3アップロード部分。

## s3_max_put_burst {#s3_max_put_burst}

タイプ: UInt64

デフォルト値: 0

リクエスト毎秒制限に達する前に同時に発行できる最大リクエスト数。デフォルト値は（0）は`s3_max_put_rps`と等しい。

## s3_max_put_rps {#s3_max_put_rps}

タイプ: UInt64

デフォルト値: 0

スロットリング前のS3 PUTリクエスト毎秒の制限。ゼロは無制限を意味します。

## s3_max_redirects {#s3_max_redirects}

タイプ: UInt64

デフォルト値: 10

許可される最大S3リダイレクトホップ数。

## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size}

タイプ: UInt64

デフォルト値: 33554432

s3における単一コピー操作の最大サイズ。

## s3_max_single_part_upload_size {#s3_max_single_part_upload_size}

タイプ: UInt64

デフォルト値: 33554432

S3に対して単一部分アップロードを使用してアップロードするオブジェクトの最大サイズ。

## s3_max_single_read_retries {#s3_max_single_read_retries}

タイプ: UInt64

デフォルト値: 4

単一のS3読み取り中の最大リトライ回数。

## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries}

タイプ: UInt64

デフォルト値: 4

S3書き込み中の予期しないエラーが発生した場合の最大リトライ回数。

## s3_max_upload_part_size {#s3_max_upload_part_size}

タイプ: UInt64

デフォルト値: 5368709120

マルチパートアップロード中にS3にアップロードする部分の最大サイズ。

## s3_min_upload_part_size {#s3_min_upload_part_size}

タイプ: UInt64

デフォルト値: 16777216

マルチパートアップロード中にS3にアップロードする部分の最小サイズ。

## s3_request_timeout_ms {#s3_request_timeout_ms}

タイプ: UInt64

デフォルト値: 30000

S3とのデータの送受信に関するアイドリングタイムアウト。単一のTCP読み取りまたは書き込み呼び出しがこの時間ブロックされると失敗します。

## s3_retry_attempts {#s3_retry_attempts}

タイプ: UInt64

デフォルト値: 100

Aws::Client::RetryStrategyの設定。Aws::Clientは自動的にリトライを行い、0はリトライなしを意味します。

## s3_skip_empty_files {#s3_skip_empty_files}

タイプ: Bool

デフォルト値: 0

[S3](../../engines/table-engines/integrations/s3.md)エンジンテーブルで空のファイルをスキップするかどうかの設定。

可能な値:
- 0 — 空のファイルが要求された形式と互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して空の結果を返します。

## s3_strict_upload_part_size {#s3_strict_upload_part_size}

タイプ: UInt64

デフォルト値: 0

マルチパートアップロード中にS3にアップロードする部分の正確なサイズ（いくつかの実装は可変サイズ部分をサポートしていません）。

## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match}

タイプ: Bool

デフォルト値: 0

ListObjectsリクエストがファイルに一致しない場合にエラーをスローします。

## s3_truncate_on_insert {#s3_truncate_on_insert}

タイプ: Bool

デフォルト値: 0

s3エンジンテーブルへの挿入の前にトランケートを有効または無効にします。無効にすると、既にS3オブジェクトが存在する場合、挿入試行時に例外がスローされます。

可能な値:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリはファイルの既存の内容を新しいデータで置き換えます。

## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor}

タイプ: UInt64

デフォルト値: 2

s3_multiply_parts_count_thresholdからの単一書き込みでアップロードされた各回のs3_min_upload_part_sizeをこの係数で掛けます。

## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold}

タイプ: UInt64

デフォルト値: 500

この数の部分がS3にアップロードされるたびに、s3_min_upload_part_sizeはs3_upload_part_size_multiply_factorで乗算されます。

## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts}

タイプ: Bool

デフォルト値: 1

`true`に設定すると、すべてのs3リクエストに対して最初の2回の試行が低い送信および受信タイムアウトで行われます。
`false`に設定すると、すべての試行が同一のタイムアウトで行われます。

## s3_validate_request_settings {#s3_validate_request_settings}

タイプ: Bool

デフォルト値: 1

s3リクエスト設定の検証を有効にします。

可能な値:
- 1 — 設定を検証します。
- 0 — 設定を検証しません。

## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path}

タイプ: String

デフォルト値: /clickhouse/s3queue/

S3QueueエンジンのデフォルトのZooKeeperパスプレフィックス。

## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log}

タイプ: Bool

デフォルト値: 0

system.s3queue_logへの書き込みを有効にします。この値はテーブル設定で上書きできます。

## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url}

タイプ: Bool

デフォルト値: 1

最終変更時刻の検証が必要なURLのキャッシュからスキーマを使用します（Last-Modifiedヘッダーを持つURLの場合）。

## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure}

タイプ: Bool

デフォルト値: 1

Azureテーブル関数を使用している間、スキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file}

タイプ: Bool

デフォルト値: 1

ファイルテーブル関数を使用している間、スキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs}

タイプ: Bool

デフォルト値: 1

HDFSテーブル関数を使用している間、スキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3}

タイプ: Bool

デフォルト値: 1

S3テーブル関数を使用している間、スキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url}

タイプ: Bool

デフォルト値: 1

URLテーブル関数を使用している間、スキーマ推論でキャッシュを使用します。

## select_sequential_consistency {#select_sequential_consistency}

タイプ: UInt64

デフォルト値: 0

:::note
この設定はSharedMergeTreeとReplicatedMergeTreeでの挙動が異なります。[SharedMergeTreeの整合性](/docs/ja/cloud/reference/shared-merge-tree/#consistency)を参照して、SharedMergeTreeにおける`select_sequential_consistency`の挙動についての詳細を確認してください。
:::

`SELECT`クエリに対する連続整合性を有効または無効にします。`insert_quorum_parallel`が無効である必要があります（デフォルトでは有効）。

可能な値:

- 0 — 無効。
- 1 — 有効。

使用方法

連続整合性が有効になっている場合、ClickHouseはクライアントが`insert_quorum`で実行されたすべての前回の`INSERT`クエリでデータを含むレプリカに対してのみ`SELECT`クエリを実行することを許可します。クライアントが部分的なレプリカを参照する場合、ClickHouseは例外を生成します。SELECTクエリは、まだレプリカのクォラムに書き込まれていないデータを含めません。

`insert_quorum_parallel`が有効（デフォルト）である場合、`select_sequential_consistency`は機能しません。これは、並行する`INSERT`クエリが異なるレプリカのセットに書き込まれる可能性があるため、単一のレプリカがすべての書き込みを受け取った保証がないからです。

関連情報:

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)

## send_logs_level {#send_logs_level}

タイプ: LogsLevel

デフォルト値: fatal

指定された最小レベルのサーバーテキストログをクライアントに送信します。有効な値: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'

## send_logs_source_regexp {#send_logs_source_regexp}

タイプ: String

デフォルト値:

指定された正規表現に一致するログソース名を持つサーバーテキストログを送信します。空はすべてのソースを意味します。

## send_progress_in_http_headers {#send_progress_in_http_headers}

タイプ: Bool

デフォルト値: 0

`clickhouse-server`のレスポンスに`X-ClickHouse-Progress` HTTPレスポンスヘッダーを有効または無効にします。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

可能な値:

- 0 — 無効。
- 1 — 有効。

## send_timeout {#send_timeout}

タイプ: 秒

デフォルト値: 300

ネットワークへのデータ送信のタイムアウト（秒単位）。クライアントがデータを送信する必要があるが、このインターバル内にバイトを送信できない場合、例外がスローされます。この設定をクライアントで設定すると、ソケットの'receive_timeout'もサーバーの対応する接続側に設定されます。

## session_timezone {#session_timezone}

タイプ: タイムゾーン

デフォルト値:

現在のセッションまたはクエリの暗黙のタイムゾーンを設定します。
暗黙のタイムゾーンは、明示的に指定されたタイムゾーンがないDateTime/DateTime64型の値に適用されるタイムゾーンです。
この設定は、グローバルに設定された（サーバーレベルの）暗黙のタイムゾーンよりも優先されます。
''（空の文字列）の値は、現在のセッションまたはクエリの暗黙のタイムゾーンが[サーバーのタイムゾーン](../server-configuration-parameters/settings.md#timezone)と等しいことを意味します。

`timeZone()`および`serverTimeZone()`関数を使用して、セッションタイムゾーンとサーバータイムゾーンを取得できます。

可能な値:

- `system.time_zones`からの任意のタイムゾーン名、例えば`Europe/Berlin`、`UTC`、または`Zulu`。

例:

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

セッションタイムゾーン'America/Denver'を明示的に指定されていない内側のDateTimeに割り当て:

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
DateTime/DateTime64を解析するすべての関数が`session_timezone`を尊重するわけではありません。これにより微妙なエラーが生じる可能性があります。
以下の例と説明を参照してください。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

これは異なる解析パイプラインによるものです:

- 明示的に与えられたタイムゾーンなしで使用される`toDateTime()`は、最初の`SELECT`クエリで`session_timezone`とグローバルタイムゾーンの設定を尊重します。
- 2番目のクエリでは、文字列からDateTimeが解析され、既存のカラム`d`の型とタイムゾーンを引き継ぎます。したがって、`session_timezone`とグローバルタイムゾーンの設定は尊重されません。

**関連情報**

- [timezone](../server-configuration-parameters/settings.md#timezone)

## set_overflow_mode {#set_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた際に何をするか。

## short_circuit_function_evaluation {#short_circuit_function_evaluation}

タイプ: ShortCircuitFunctionEvaluation

デフォルト値: enable

[if](../../sql-reference/functions/conditional-functions.md/#if)、[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)、[and](../../sql-reference/functions/logical-functions.md/#logical-and-function)、および[or](../../sql-reference/functions/logical-functions.md/#logical-or-function)関数を短絡的に計算できるようにします。これにより、これらの関数内の複雑な式の実行を最適化し、予期しない例外（ゼロ除算など）を防ぐのに役立ちます。

可能な値:

- `enable` — 適用可能な関数に対して短絡評価が有効になります（例外をスローする可能性があるか、計算コストの高い場合）。
- `force_enable` — すべての関数に対して短絡評価が有効になります。
- `disable` — 短絡評価が無効になります。

## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil}

タイプ: Bool

デフォルト値: 0

`SHOW TABLE`クエリの表示を設定します。

可能な値:

- 0 — クエリはテーブルUUIDなしで表示されます。
- 1 — クエリはテーブルUUID付きで表示されます。

## single_join_prefer_left_table {#single_join_prefer_left_table}

タイプ: Bool

デフォルト値: 1

識別子の曖昧さがある場合、単一JOINの場合は左側のテーブルを優先します。

## skip_download_if_exceeds_query_cache {#skip_download_if_exceeds_query_cache}

タイプ: Bool

デフォルト値: 1

クエリキャッシュサイズを超える場合、リモートファイルシステムからのダウンロードをスキップします。

## skip_unavailable_shards {#skip_unavailable_shards}

タイプ: Bool

デフォルト値: 0

使用できないシャードを静かにスキップするかどうかを有効または無効にします。

シャードはすべてのレプリカが使用できない場合、使用できないと見なされます。レプリカは以下の状況で使用できません:

- ClickHouseが何らかの理由でレプリカに接続できない。

    レプリカに接続する際、ClickHouseは何度か試みます。これらのすべての試みが失敗した場合、そのレプリカは使用できないと見なされます。

- レプリカがDNS経由で解決できない。

    レプリカのホスト名がDNSを介して解決できない場合、次の状況が考えられます:

    - レプリカのホストにDNSレコードがない。これはダイナミックDNSを持つシステムで発生することがあります。たとえば、[Kubernetes](https://kubernetes.io)のように、ノードがダウンタイム中に解決できないことがあるが、これはエラーではありません。

    - 設定エラー。ClickHouseの構成ファイルに誤ったホスト名が含まれています。

可能な値:

- 1 — スキップが有効。

    シャードが利用できなくなった場合、ClickHouseは部分的なデータに基づいた結果を返し、ノードの可用性の問題を報告しません。

- 0 — スキップが無効。

    シャードが利用できない場合、ClickHouseは例外をスローします。

## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms}

タイプ: ミリ秒

デフォルト値: 0

TCPHandler内でクエリを受信した後のスリープ時間。

## sleep_in_send_data_ms {#sleep_in_send_data_ms}

タイプ: ミリ秒

デフォルト値: 0

TCPHandler内でデータを送信する際のスリープ時間。

## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms}

タイプ: ミリ秒

デフォルト値: 0

TCPHandler内でテーブルステータス応答を送信する際のスリープ時間。

## sort_overflow_mode {#sort_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた際に何をするか。

## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final}

タイプ: Bool

デフォルト値: 1

FINAL最適化中に交差する部分範囲をレイヤーに分割します。

## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final}

タイプ: Bool

デフォルト値: 1

FINAL最適化中に部分範囲を交差する部分と交差しない部分に分割します。

## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string}

タイプ: Bool

デフォルト値: 0

引数`max_substrings` > 0の[splitBy*()](../../sql-reference/functions/splitting-merging-functions.md)関数が残りの文字列を結果配列の最後の要素に含めるかどうかを制御します。

可能な値:

- `0` - 残りの文字列は結果配列の最後の要素には含まれません。
- `1` - 残りの文字列は結果配列の最後の要素に含まれます。これはSparkの[`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html)関数やPythonの['string.split()'](https://docs/ja/interfaces/cli)または[gRPCインターフェース](/docs/ja/interfaces/grpc)の動作です。

## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations}

タイプ: Bool

デフォルト値: 0

書き込み操作中（INSERT、マージ）のキャッシュのエラーを無視します。

## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block}

タイプ: Bool

デフォルト値: 1

max_partitions_per_insert_blockと共に使用されます。true（デフォルト）の場合、max_partitions_per_insert_blockに達したときに例外がスローされます。falseの場合、この限界に達した挿入クエリの詳細がログに記録されます。これは、max_partitions_per_insert_blockを変更した場合のユーザーへの影響を理解するのに役立ちます。

## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction}

タイプ: Bool

デフォルト値: 1

トランザクション内でサポートされていないクエリが使用されると例外をスローします。

## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed}

タイプ: 秒

デフォルト値: 10

指定された時間が経過した後、速度があまりにも低くないことを確認します。

## timeout_overflow_mode {#timeout_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた際に何をするか。

## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf}

タイプ: OverflowMode

デフォルト値: throw

葉制限が超えた際に何をするか。

## totals_auto_threshold {#totals_auto_threshold}

タイプ: Float

デフォルト値: 0.5

`totals_mode = 'auto'`の閾値。
「WITH TOTALS修飾子」セクションを参照してください。

## totals_mode {#totals_mode}

タイプ: TotalsMode

デフォルト値: after_having_exclusive

HAVINGが存在する場合、またはmax_rows_to_group_byおよびgroup_by_overflow_mode = 'any'が存在する場合、どのようにTOTALSを計算するか。
「WITH TOTALS修飾子」セクションを参照してください。

## trace_profile_events {#trace_profile_events}

タイプ: Bool

デフォルト値: 0

プロファイルイベントの各更新時のスタックトレースを収集し、プロファイルイベントの名前とインクリメントの値を含むことを有効または無効にします。この情報は[trace_log](../../operations/system-tables/trace_log.md#system_tables-trace_log)に送信されます。

可能な値:

- 1 — プロファイルイベントのトレースが有効。
- 0 — プロファイルイベントのトレースが無効。

## transfer_overflow_mode {#transfer_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた際に何をするか。

## transform_null_in {#transform_null_in}

タイプ: Bool

デフォルト値: 0

[IN](../../sql-reference/operators/in.md)演算子に対する[NULL](../../sql-reference/syntax.md/#null-literal)値の平等性を有効にします。

デフォルトでは、`NULL`値は比較できません。なぜなら、`NULL`は未定義の値を意味するからです。したがって、比較`expr = NULL`は常に`false`を返さなければなりません。この設定が有効な場合、`NULL = NULL`は`IN`演算子に対して`true`を返します。

可能な値:

- 0 — `IN`演算子における`NULL`値の比較は`false`を返します。
- 1 — `IN`演算子における`NULL`値の比較は`true`を返します。

**例**

`null_in`テーブルを考えます:

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

クエリ:

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

結果:

``` text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

クエリ:

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

結果:

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**関連情報**

- [IN演算子におけるNULL処理](../../sql-reference/operators/in.md/#in-null-processing)

## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths}

タイプ: Bool

デフォルト値: 0

system.remote_data_pathsクエリを実行するときに、実際のテーブルデータに加えて凍結データ（シャドウディレクトリ）をトラバースします。

## union_default_mode {#union_default_mode}

タイプ: SetOperationMode

デフォルト値:

`SELECT`クエリの結果を結合するモードを設定します。この設定は、`UNION`を使用する際に明示的に`UNION ALL`または`UNION DISTINCT`を指定しない場合にのみ使用されます。

可能な値:

- `'DISTINCT'` — ClickHouseは重複行を削除し、クエリを結合した結果を出力します。
- `'ALL'` — ClickHouseは重複行を含むすべての行を出力します。
- `''` — ClickHouseは`UNION`で使用されたときに例外を生成します。

[UNION](../../sql-reference/statements/select/union.md)の例を参照してください。

## unknown_packet_in_send_data {#unknown_packet_in_send_data}

タイプ: UInt64

デフォルト値: 0

N番目のデータパケットの代わりに不明なパケットを送信します。

## use_cache_for_count_from_files {#use_cache_for_count_from_files}

タイプ: Bool

デフォルト値: 1

テーブル関数`file` / `s3` / `url` / `hdfs` / `azureBlobStorage`からのカウント時に行数のキャッシュを有効にします。

デフォルトで有効です。

## use_client_time_zone {#use_client_time_zone}

タイプ: Bool

デフォルト値: 0

サーバーのタイムゾーンを採用するのではなく、DateTime文字列値を解釈するためにクライアントタイムゾーンを使用します。

## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names}

タイプ: Bool

デフォルト値: 1

Distributedエンジンを持つテーブルに対するバックグラウンド（`distributed_foreground_insert`）INSERTのためのブロックを格納するためにコンパクトな形式を使用します。

可能な値:

- 0 — `user[:password]@host:port#default_database`ディレクトリ形式を使用します。
- 1 — `[shard{shard_index}[_replica{replica_index}]]`ディレクトリ形式を使用します。

:::note
- `use_compact_format_in_distributed_parts_names=0`の場合、クラスタ定義の変更は、バックグラウンドINSERTでは適用されません。
- `use_compact_format_in_distributed_parts_names=1`の場合、クラスタ定義のノードの順序を変更すると、`shard_index` / `replica_index`が変更されるため、注意が必要です。
:::

## use_concurrency_control {#use_concurrency_control}

タイプ: Bool

デフォルト値: 1

サーバーの同時実行制御を遵守します（`concurrent_threads_soft_limit_num`および`concurrent_threads_soft_limit_ratio_to_cores` グローバルサーバー設定を参照）。無効にすると、サーバーが過負荷であってもより多くのスレッドを使用することができます（通常の使用には推奨されず、主にテストに必要です）。

## use_hedged_requests {#use_hedged_requests}

タイプ: Bool

デフォルト値: 1

リモートクエリのためのヘッジリクエストロジックを有効にします。これにより、クエリのために異なるレプリカと多くの接続を確立することができます。
既存の接続が`hedged_connection_timeout`内に確立されなかった場合や、`receive_data_timeout`内にデータが受信されなかった場合に新しい接続が有効になります。クエリは最初に非空の進行状況パケット（またはデータパケット）を送信する接続を使用し、他の接続はキャンセルされます。`max_parallel_replicas > 1`のクエリもサポートされています。

デフォルトで有効です。

クラウドではデフォルトで無効です。

## use_hive_partitioning {#use_hive_partitioning}

タイプ: Bool

デフォルト値: 0

有効にすると、ClickHouseはファイル関連のテーブルエンジンのパス中でのHiveスタイルのパーティショニングを検出します（`/name=value/`）。これにより、クエリ内でパーティション列を仮想列として使用することができます。これらの仮想列は、パーティション化されたパスで同じ名前を持ちますが、`_`で始まります。

## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries}

タイプ: Bool

デフォルト値: 1

IN演算子の右側にサブクエリまたはテーブル式がある場合、インデックスを使用してみます。

## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values}

タイプ: UInt64

デフォルト値: 0

フィルタリングにテーブルインデックスを使用するためのIN演算子の右側のセットの最大サイズ。これにより、大きなクエリに対して追加のデータ構造の準備によるパフォーマンス劣化と高いメモリ使用を回避できます。ゼロは制限なしを意味します。

## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type}

タイプ: Bool

デフォルト値: 0

有効にすると、`JSON`データ型エイリアスは新しい[JSON](../../sql-reference/data-types/newjson.md)型の代わりに古い[Object('json')](../../sql-reference/data-types/json.md)型を作成するために使用されます。

## use_local_cache_for_remote_storage {#use_local_cache_for_remote_storage}

タイプ: Bool

デフォルト値: 1

HDFSやS3のようなリモートストレージのためにローカルキャッシュを使用します。この設定はリモートテーブルエンジンのみに使用されます。

## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache}

タイプ: Bool

デフォルト値: 0

ファイルキャッシュが有効になっていないリモートディスク用にユーザースペースページキャッシュを使用します。

## use_query_cache {#use_query_cache}

タイプ: Bool

デフォルト値: 0

オンになっている場合、`SELECT`クエリは[クエリキャッシュ](../query-cache.md)を利用できる可能性があります。[enable_reads_from_query_cache](#enable-reads-from-query-cache)および[enable_writes_to_query_cache](#enable-writes-to-query-cache)パラメーターが、キャッシュの使用方法に関してより詳細に制御します。

可能な値:

- 0 - 無効
- 1 - 有効

## use_skip_indexes {#use_skip_indexes}

タイプ: Bool

デフォルト値: 1

クエリ実行中にデータスキッピングインデックスを使用します。

可能な値:

- 0 — 無効。
- 1 — 有効。

## use_skip_indexes_if_final {#use_skip_indexes_if_final}

タイプ: Bool

デフォルト値: 0

FINAL修飾子を使用したクエリを実行する際にスキップインデックスを使用するかどうかを制御します。

デフォルトでは、この設定は無効です。なぜなら、スキップインデックスは、最近のデータを含む行（グラニュール）を除外する可能性があり、不正確な結果につながる可能性があるからです。有効にすると、FINAL修飾子を持つクエリでもスキップインデックスが適用され、パフォーマンスが向上する可能性がありますが、最近の更新を見逃すリスクがあります。

可能な値:

- 0 — 無効。
- 1 — 有効。

## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions}

タイプ: UInt64

デフォルト値: 2

データからのスキーマ推論の代わりに、挿入テーブルからの構造を使用します。可能な値: 0 - 無効、1 - 有効、2 - 自動。

## use_uncompressed_cache {#use_uncompressed_cache}

タイプ: Bool

デフォルト値: 0

未圧縮ブロックのキャッシュを使用するかどうか。0または1を受け入れます。デフォルトでは0（無効）。
未圧縮キャッシュの使用（MergeTreeファミリのテーブルにのみ）は、多数の短いクエリを扱う際にレイテンシを大幅に減少させ、スループットを増加させることができます。頻繁に短いリクエストを送信するユーザーにはこの設定を有効にします。また、未圧縮キャッシュブロックのサイズを設定する[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)構成パラメーターに注意してください（構成ファイル内のみ設定） – デフォルトは8GiBです。未圧縮キャッシュは必要に応じて充填され、最も使用されていないデータは自動的に削除されます。

少なくともある程度の大量データ（100万行以上）を読み取るクエリについては、スペースを節約するために自動的に未圧縮キャッシュが無効になります。つまり、`use_uncompressed_cache`設定は常に1に設定しておくことができます。

## use_variant_as_common_type {#use_variant_as_common_type}

タイプ: Bool

デフォルト値: 0

引数型に共通の型がない場合、[if](../../sql-reference/functions/conditional-functions.md/#if)/[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)/[array](../../sql-reference/functions/array-functions.md)/[map](../../sql-reference/functions/tuple-map-functions.md)関数の結果型として`Variant`型を使用することを許可します。

例:

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```

## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix}

タイプ: Bool

デフォルト値: 1

ORDER BY句でFILL列より前のカラムがソーティングプレフィックスを形成します。ソーティングプレフィックス内で異なる値を持つ行は独立して埋められます。

## validate_polygons {#validate_polygons}

タイプ: Bool

デフォルト値: 1

多角形が自己交差または自己接触している場合、[pointInPolygon](../../sql-reference/functions/geo/index.md#pointinpolygon)関数で例外をスローするかどうかを有効または無効にします。

可能な値:

- 0 — 例外をスローすることを無効にします。`pointInPolygon`は無効な多角形を受け入れ、それらに対して不正確な結果を返す可能性があります。
- 1 — 例外をスローすることを有効にします。

## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode}

タイプ: TransactionsWaitCSNMode

デフォルト値: wait_unknown

コミットされた変更が最新のスナップショットで実際に見えるようになるまで待機します。

## wait_for_async_insert {#wait_for_async_insert}

タイプ: Bool

デフォルト値: 1

trueの場合、非同期挿入処理の完了を待機します。

## wait_for_async_insert_timeout {#wait_for_async_insert_timeout}

タイプ: 秒

デフォルト値: 120

非同期挿入処理の完了を待つためのタイムアウト。

## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout}

タイプ: 秒

デフォルト値: 10

イベント時間処理においてウィンドウビューの信号発火を待つためのタイムアウト。

## window_view_clean_interval {#window_view_clean_interval}

タイプ: 秒

デフォルト値: 60

古いデータを解放するためのウィンドウビューのクリーン間隔（秒）。

## window_view_heartbeat_interval {#window_view_heartbeat_interval}

タイプ: 秒

デフォルト値: 15

ウォッチクエリが生存していることを示すためのハートビート間隔（秒）。

## workload {#workload}

タイプ: 文字列

デフォルト値: default

リソースにアクセスするために使用されるワークロードの名前。

## write_through_distributed_cache {#write_through_distributed_cache}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。分散キャッシュへの書き込みを許可します（S3への書き込みも分散キャッシュによって行われます）。

## zstd_window_log_max {#zstd_window_log_max}

タイプ: Int64

デフォルト値: 0

ZSTD の最大ウィンドウログを選択することができます（これは MergeTree ファミリーには使用されません）。
