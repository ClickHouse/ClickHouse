================BINARY==========================

clickhouse --help

Use one of the following commands:
clickhouse local [args] 
clickhouse client [args] 
clickhouse benchmark [args] 
clickhouse server [args] 
clickhouse extract-from-config [args] 
clickhouse compressor [args] 
clickhouse format [args] 
clickhouse copier [args] 
clickhouse obfuscator [args] 
clickhouse git-import [args] 
clickhouse keeper [args] 
clickhouse keeper-converter [args] 
clickhouse keeper-client [args] 
clickhouse install [args] 
clickhouse start [args] 
clickhouse stop [args] 
clickhouse status [args] 
clickhouse restart [args] 
clickhouse static-files-disk-uploader [args] 
clickhouse su [args] 
clickhouse hash-binary [args] 
clickhouse disks [args] 
clickhouse help [args] 

clickhouse help

Use one of the following commands:
clickhouse local [args] 
clickhouse client [args] 
clickhouse benchmark [args] 
clickhouse server [args] 
clickhouse extract-from-config [args] 
clickhouse compressor [args] 
clickhouse format [args] 
clickhouse copier [args] 
clickhouse obfuscator [args] 
clickhouse git-import [args] 
clickhouse keeper [args] 
clickhouse keeper-converter [args] 
clickhouse keeper-client [args] 
clickhouse install [args] 
clickhouse start [args] 
clickhouse stop [args] 
clickhouse status [args] 
clickhouse restart [args] 
clickhouse static-files-disk-uploader [args] 
clickhouse su [args] 
clickhouse hash-binary [args] 
clickhouse disks [args] 
clickhouse help [args] 

clickhouse benchmark

Usage: clickhouse benchmark [options] < queries.tsv
Usage: clickhouse benchmark [options] --query "query text"
clickhouse-benchmark connects to ClickHouse server, repeatedly sends specified queries and produces reports query statistics. Multiple queries can be used if passed in TSV format.

clickhouse client

Usage: clickhouse client [initial table definition] [--query <query>]
clickhouse client is a client application that is used to connect to ClickHouse.
It can run queries as command line tool if you pass queries as an argument or as interactive client. Queries can run one at a time, or in in a multiquery mode with --multiquery option. To change settings you may use 'SET' statements and SETTINGS clause in queries or set is for a  session with corresponding clickhouse client arguments.
'clickhouse client' command will try connect to clickhouse-server running on the same server. If you have credentials set up pass them with --user <username> --password <password> or with --ask-password argument that will open command prompt.

This one will try connect to tcp native port(9000) without encryption:
    clickhouse client --host clickhouse.example.com --password mysecretpassword
To connect to secure endpoint just set --secure argument. If you have  artered port set it with --port <your port>.
    clickhouse client --secure --host clickhouse.example.com --password mysecretpassword


clickhouse local

Usage: clickhouse local [initial table definition] [--query <query>]
clickhouse-local allows to execute SQL queries on your data files without running clickhouse-server.

It can run as command line tool that does single action or as interactive client. For interactive experience you can just run 'clickhouse local' or add --interactive argument to your command. It will set up tables, run queries and pass control as if it is clickhouse-client. Then you can execute your SQL queries in usual manner. Non-interactive mode requires query as an argument and exits when queries finish. Multiple SQL queries can be passed as --query argument.

To configure initial environment two ways are supported: queries or command line parameters. Either just in first query like this:
    CREATE TABLE <table> (<structure>) ENGINE = File(<input-format>, <file>);
Or through corresponding command line parameters --table --structure --input-format and --file.

clickhouse local supports all features and engines of ClickHouse. You can query data from remote engines and store results locally or other way around. For table engines that actually store data on a disk like Log and MergeTree clickhouse local puts data to temporary directory that is not reused between runs.

clickhouse local can be used to query data from stopped clickhouse-server installation with --path to local directory with data.

Example reading file from S3, converting format and writing to a file:
clickhouse local --query "SELECT c1 as version, c2 as date FROM url('https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/utils/list-versions/version_date.tsv') INTO OUTFILE '/tmp/versions.json'"
In addition, --param_name=value can be specified for substitution of parameters for parametrized queries.

clickhouse compressor

Usage: clickhouse compressor [options] < INPUT > OUTPUT
Alternative usage: clickhouse compressor [options] INPUT OUTPUT

clickhouse disks

ClickHouse disk management tool
Usage clickhouse disks [OPTION] 	read	Read a file from `FROM_PATH` to `TO_PATH`

	write	Write a file from `FROM_PATH` to `TO_PATH`

	link	Create hardlink from `from_path` to `to_path`

	mkdir	Create a directory

	remove	Remove file or directory with all children. Throws exception if file doesn't exists. Path should be in format './' or './path' or 'path'

	move	Move file or directory from `from_path` to `to_path`

	copy	Recursively copy data from `FROM_PATH` to `TO_PATH`

	list	List files at path[s]

	list-disks	List disks names

clickhouse-disks:
  -h [ --help ]     Print common help message
  -C [ --config-file ] arg
                    Set config file
  --disk arg        Set disk name
  --command_name arg
                    Name for command to do
  --save-logs       Save logs to a file
  --log-level arg   Logging level


clickhouse extract

Preprocess config file and extract value of the given key.

Usage: clickhouse extract-from-config [options]

Allowed options:
  --help                   produce this help message
  --stacktrace             print stack traces of exceptions
  --process-zk-includes    if there are from_zk elements in config, connect to 
                           ZooKeeper and process them
  --try                    Do not warn about missing keys
  --log-level arg (=error) log level
  -c [ --config-file ] arg path to config file
  -k [ --key ] arg         key to get value for


clickhouse format

Usage: clickhouse format [options] < query

clickhouse git-import


A tool to extract information from Git repository for analytics.

It dumps the data for the following tables:
- commits - commits with statistics;
- file_changes - files changed in every commit with the info about the change and statistics;
- line_changes - every changed line in every changed file in every commit with full info about the line and the information about previous change of this line.

The largest and the most important table is "line_changes".

Allows to answer questions like:
- list files with maximum number of authors;
- show me the oldest lines of code in the repository;
- show me the files with longest history;
- list favorite files for author;
- list largest files with lowest number of authors;
- at what weekday the code has highest chance to stay in repository;
- the distribution of code age across repository;
- files sorted by average code age;
- quickly show file with blame info (rough);
- commits and lines of code distribution by time; by weekday, by author; for specific subdirectories;
- show history for every subdirectory, file, line of file, the number of changes (lines and commits) across time; how the number of contributors was changed across time;
- list files with most modifications;
- list files that were rewritten most number of time or by most of authors;
- what is percentage of code removal by other authors, across authors;
- the matrix of authors that shows what authors tends to rewrite another authors code;
- what is the worst time to write code in sense that the code has highest chance to be rewritten;
- the average time before code will be rewritten and the median (half-life of code decay);
- comments/code percentage change in time / by author / by location;
- who tend to write more tests / cpp code / comments.

The data is intended for analytical purposes. It can be imprecise by many reasons but it should be good enough for its purpose.

The data is not intended to provide any conclusions for managers, it is especially counter-indicative for any kinds of "performance review". Instead you can spend multiple days looking at various interesting statistics.

Run this tool inside your git repository. It will create .tsv files that can be loaded into ClickHouse (or into other DBMS if you dare).

The tool can process large enough repositories in a reasonable time.
It has been tested on:
- ClickHouse: 31 seconds; 3 million rows;
- LLVM: 8 minutes; 62 million rows;
- Linux - 12 minutes; 85 million rows;
- Chromium - 67 minutes; 343 million rows;
(the numbers as of Sep 2020)


Prepare the database by executing the following queries:

DROP DATABASE IF EXISTS git;
CREATE DATABASE git;

CREATE TABLE git.commits
(
    hash String,
    author LowCardinality(String),
    time DateTime,
    message String,
    files_added UInt32,
    files_deleted UInt32,
    files_renamed UInt32,
    files_modified UInt32,
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.file_changes
(
    change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.line_changes
(
    sign Int8,
    line_number_old UInt32,
    line_number_new UInt32,
    hunk_num UInt32,
    hunk_start_line_number_old UInt32,
    hunk_start_line_number_new UInt32,
    hunk_lines_added UInt32,
    hunk_lines_deleted UInt32,
    hunk_context LowCardinality(String),
    line LowCardinality(String),
    indent UInt8,
    line_type Enum('Empty' = 0, 'Comment' = 1, 'Punct' = 2, 'Code' = 3),

    prev_commit_hash String,
    prev_author LowCardinality(String),
    prev_time DateTime,

    file_change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    file_lines_added UInt32,
    file_lines_deleted UInt32,
    file_hunks_added UInt32,
    file_hunks_removed UInt32,
    file_hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

Run the tool.

Then insert the data with the following commands:

clickhouse-client --query "INSERT INTO git.commits FORMAT TSV" < commits.tsv
clickhouse-client --query "INSERT INTO git.file_changes FORMAT TSV" < file_changes.tsv
clickhouse-client --query "INSERT INTO git.line_changes FORMAT TSV" < line_changes.tsv


Usage: clickhouse git-import
clickhouse git-import --skip-paths 'generated\.cpp|^(contrib|docs?|website|libs/(libcityhash|liblz4|libdivide|libvectorclass|libdouble-conversion|libcpuid|libzstd|libfarmhash|libmetrohash|libpoco|libwidechar_width))/' --skip-commits-with-messages '^Merge branch '

clickhouse install

Usage: clickhouse install [options]
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --binary-path arg (=usr/bin)          where to install binaries
  --config-path arg (=etc/clickhouse-server)
                                        where to install configs
  --log-path arg (=var/log/clickhouse-server)
                                        where to create log directory
  --data-path arg (=var/lib/clickhouse) directory for data
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --user arg (=clickhouse)              clickhouse user
  --group arg (=clickhouse)             clickhouse group
  -y [ --noninteractive ]               run non-interactively
  --link                                create symlink to the binary instead of
                                        copying to binary-path


clickhouse keeper-converter

Usage: clickhouse keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/data/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/data/version-2 --output-dir /var/lib/clickhouse/coordination/snapshots
Allowed options:
  -h [ --help ]     produce help message
  --zookeeper-logs-dir arg
                    Path to directory with ZooKeeper logs
  --zookeeper-snapshots-dir arg
                    Path to directory with ZooKeeper 
                    snapshots
  --output-dir arg  Directory to place output 
                    clickhouse-keeper snapshot


clickhouse obfuscator


Simple tool for table data obfuscation.

It reads input table and produces output table, that retain some properties of input, but contains different data.
It allows to publish almost real production data for usage in benchmarks.

It is designed to retain the following properties of data:
- cardinalities of values (number of distinct values) for every column and for every tuple of columns;
- conditional cardinalities: number of distinct values of one column under condition on value of another column;
- probability distributions of absolute value of integers; sign of signed integers; exponent and sign for floats;
- probability distributions of length of strings;
- probability of zero values of numbers; empty strings and arrays, NULLs;
- data compression ratio when compressed with LZ77 and entropy family of codecs;
- continuity (magnitude of difference) of time values across table; continuity of floating point values.
- date component of DateTime values;
- UTF-8 validity of string values;
- string values continue to look somewhat natural.

Most of the properties above are viable for performance testing:
- reading data, filtering, aggregation and sorting will work at almost the same speed
    as on original data due to saved cardinalities, magnitudes, compression ratios, etc.

It works in deterministic fashion: you define a seed value and transform is totally determined by input data and by seed.
Some transforms are one to one and could be reversed, so you need to have large enough seed and keep it in secret.

It use some cryptographic primitives to transform data, but from the cryptographic point of view,
    it doesn't do anything properly and you should never consider the result as secure, unless you have other reasons for it.

It may retain some data you don't want to publish.

It always leave numbers 0, 1, -1 as is. Also it leaves dates, lengths of arrays and null flags exactly as in source data.
For example, you have a column IsMobile in your table with values 0 and 1. In transformed data, it will have the same value.
So, the user will be able to count exact ratio of mobile traffic.

Another example, suppose you have some private data in your table, like user email and you don't want to publish any single email address.
If your table is large enough and contain multiple different emails and there is no email that have very high frequency than all others,
    it will perfectly anonymize all data. But if you have small amount of different values in a column, it can possibly reproduce some of them.
And you should take care and look at exact algorithm, how this tool works, and probably fine tune some of it command line parameters.

This tool works fine only with reasonable amount of data (at least 1000s of rows).

Usage: clickhouse obfuscator [options] < in > out

Input must be seekable file (it will be read twice).
Example:
    clickhouse obfuscator --seed "$(head -c16 /dev/urandom | base64)" --input-format TSV --output-format TSV --structure 'CounterID UInt32, URLDomain String, URL String, SearchPhrase String, Title String' < stats.tsv

clickhouse static


clickhouse start

Usage: clickhouse start
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --binary-path arg (=usr/bin)          directory with binary
  --config-path arg (=etc/clickhouse-server)
                                        directory with configs
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --user arg (=clickhouse)              clickhouse user
  --group arg (=clickhouse)             clickhouse group
  --max-tries arg (=60)                 Max number of tries for waiting the 
                                        server (with 1 second delay)
  --no-sudo                             Use clickhouse su if sudo is 
                                        unavailable


clickhouse stop

Usage: clickhouse stop
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --force                               Stop with KILL signal instead of TERM
  --do-not-kill                         Do not send KILL even if TERM did not 
                                        help
  --max-tries arg (=60)                 Max number of tries for waiting the 
                                        server to finish after sending TERM 
                                        (with 1 second delay)


clickhouse status

Usage: clickhouse status
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file


clickhouse restart

Usage: clickhouse restart
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --binary-path arg (=usr/bin)          directory with binary
  --config-path arg (=etc/clickhouse-server)
                                        directory with configs
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --user arg (=clickhouse)              clickhouse user
  --group arg (=clickhouse)             clickhouse group
  --force arg (=0)                      Stop with KILL signal instead of TERM
  --do-not-kill                         Do not send KILL even if TERM did not 
                                        help
  --max-tries arg (=60)                 Max number of tries for waiting the 
                                        server (with 1 second delay)
  --no-sudo                             Use clickhouse su if sudo is 
                                        unavailable


clickhouse su

Usage: clickhouse su user:group ...

clickhouse hash

Usage: clickhouse hash
Prints hash of clickhouse binary.
 -h, --help   Prints this message
Result is intentionally without newline. So you can run:
objcopy --add-section .clickhouse.hash=<(./clickhouse hash-binary) clickhouse.

================SYMLINK==============================

clickhouse-local

Usage: clickhouse-local [initial table definition] [--query <query>]
clickhouse-local allows to execute SQL queries on your data files without running clickhouse-server.

It can run as command line tool that does single action or as interactive client. For interactive experience you can just run 'clickhouse-local' or add --interactive argument to your command. It will set up tables, run queries and pass control as if it is clickhouse-client. Then you can execute your SQL queries in usual manner. Non-interactive mode requires query as an argument and exits when queries finish. Multiple SQL queries can be passed as --query argument.

To configure initial environment two ways are supported: queries or command line parameters. Either just in first query like this:
    CREATE TABLE <table> (<structure>) ENGINE = File(<input-format>, <file>);
Or through corresponding command line parameters --table --structure --input-format and --file.

clickhouse-local supports all features and engines of ClickHouse. You can query data from remote engines and store results locally or other way around. For table engines that actually store data on a disk like Log and MergeTree clickhouse-local puts data to temporary directory that is not reused between runs.

clickhouse-local can be used to query data from stopped clickhouse-server installation with --path to local directory with data.

Example reading file from S3, converting format and writing to a file:
clickhouse-local --query "SELECT c1 as version, c2 as date FROM url('https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/utils/list-versions/version_date.tsv') INTO OUTFILE '/tmp/versions.json'"
In addition, --param_name=value can be specified for substitution of parameters for parametrized queries.

clickhouse-client

Usage: clickhouse-client [initial table definition] [--query <query>]
clickhouse-client is a client application that is used to connect to ClickHouse.
It can run queries as command line tool if you pass queries as an argument or as interactive client. Queries can run one at a time, or in in a multiquery mode with --multiquery option. To change settings you may use 'SET' statements and SETTINGS clause in queries or set is for a  session with corresponding clickhouse-client arguments.
'clickhouse-client' command will try connect to clickhouse-server running on the same server. If you have credentials set up pass them with --user <username> --password <password> or with --ask-password argument that will open command prompt.

This one will try connect to tcp native port(9000) without encryption:
    clickhouse-client --host clickhouse.example.com --password mysecretpassword
To connect to secure endpoint just set --secure argument. If you have  artered port set it with --port <your port>.
    clickhouse-client --secure --host clickhouse.example.com --password mysecretpassword


clickhouse-benchmark

Usage: clickhouse-benchmark [options] < queries.tsv
Usage: clickhouse-benchmark [options] --query "query text"
clickhouse-benchmark connects to ClickHouse server, repeatedly sends specified queries and produces reports query statistics. Multiple queries can be used if passed in TSV format.

clickhouse-extract

Preprocess config file and extract value of the given key.

Usage: clickhouse extract-from-config [options]


clickhouse-compressor

Usage: clickhouse-compressor [options] < INPUT > OUTPUT
Alternative usage: clickhouse-compressor [options] INPUT OUTPUT

clickhouse-format

Usage: clickhouse format [options] < query

clickhouse-obfuscator


Simple tool for table data obfuscation.

It reads input table and produces output table, that retain some properties of input, but contains different data.
It allows to publish almost real production data for usage in benchmarks.

It is designed to retain the following properties of data:
- cardinalities of values (number of distinct values) for every column and for every tuple of columns;
- conditional cardinalities: number of distinct values of one column under condition on value of another column;
- probability distributions of absolute value of integers; sign of signed integers; exponent and sign for floats;
- probability distributions of length of strings;
- probability of zero values of numbers; empty strings and arrays, NULLs;
- data compression ratio when compressed with LZ77 and entropy family of codecs;
- continuity (magnitude of difference) of time values across table; continuity of floating point values.
- date component of DateTime values;
- UTF-8 validity of string values;
- string values continue to look somewhat natural.

Most of the properties above are viable for performance testing:
- reading data, filtering, aggregation and sorting will work at almost the same speed
    as on original data due to saved cardinalities, magnitudes, compression ratios, etc.

It works in deterministic fashion: you define a seed value and transform is totally determined by input data and by seed.
Some transforms are one to one and could be reversed, so you need to have large enough seed and keep it in secret.

It use some cryptographic primitives to transform data, but from the cryptographic point of view,
    it doesn't do anything properly and you should never consider the result as secure, unless you have other reasons for it.

It may retain some data you don't want to publish.

It always leave numbers 0, 1, -1 as is. Also it leaves dates, lengths of arrays and null flags exactly as in source data.
For example, you have a column IsMobile in your table with values 0 and 1. In transformed data, it will have the same value.
So, the user will be able to count exact ratio of mobile traffic.

Another example, suppose you have some private data in your table, like user email and you don't want to publish any single email address.
If your table is large enough and contain multiple different emails and there is no email that have very high frequency than all others,
    it will perfectly anonymize all data. But if you have small amount of different values in a column, it can possibly reproduce some of them.
And you should take care and look at exact algorithm, how this tool works, and probably fine tune some of it command line parameters.

This tool works fine only with reasonable amount of data (at least 1000s of rows).

Usage: clickhouse-obfuscator [options] < in > out

Input must be seekable file (it will be read twice).
Example:
    clickhouse-obfuscator --seed "$(head -c16 /dev/urandom | base64)" --input-format TSV --output-format TSV --structure 'CounterID UInt32, URLDomain String, URL String, SearchPhrase String, Title String' < stats.tsv

clickhouse-git-import


A tool to extract information from Git repository for analytics.

It dumps the data for the following tables:
- commits - commits with statistics;
- file_changes - files changed in every commit with the info about the change and statistics;
- line_changes - every changed line in every changed file in every commit with full info about the line and the information about previous change of this line.

The largest and the most important table is "line_changes".

Allows to answer questions like:
- list files with maximum number of authors;
- show me the oldest lines of code in the repository;
- show me the files with longest history;
- list favorite files for author;
- list largest files with lowest number of authors;
- at what weekday the code has highest chance to stay in repository;
- the distribution of code age across repository;
- files sorted by average code age;
- quickly show file with blame info (rough);
- commits and lines of code distribution by time; by weekday, by author; for specific subdirectories;
- show history for every subdirectory, file, line of file, the number of changes (lines and commits) across time; how the number of contributors was changed across time;
- list files with most modifications;
- list files that were rewritten most number of time or by most of authors;
- what is percentage of code removal by other authors, across authors;
- the matrix of authors that shows what authors tends to rewrite another authors code;
- what is the worst time to write code in sense that the code has highest chance to be rewritten;
- the average time before code will be rewritten and the median (half-life of code decay);
- comments/code percentage change in time / by author / by location;
- who tend to write more tests / cpp code / comments.

The data is intended for analytical purposes. It can be imprecise by many reasons but it should be good enough for its purpose.

The data is not intended to provide any conclusions for managers, it is especially counter-indicative for any kinds of "performance review". Instead you can spend multiple days looking at various interesting statistics.

Run this tool inside your git repository. It will create .tsv files that can be loaded into ClickHouse (or into other DBMS if you dare).

The tool can process large enough repositories in a reasonable time.
It has been tested on:
- ClickHouse: 31 seconds; 3 million rows;
- LLVM: 8 minutes; 62 million rows;
- Linux - 12 minutes; 85 million rows;
- Chromium - 67 minutes; 343 million rows;
(the numbers as of Sep 2020)


Prepare the database by executing the following queries:

DROP DATABASE IF EXISTS git;
CREATE DATABASE git;

CREATE TABLE git.commits
(
    hash String,
    author LowCardinality(String),
    time DateTime,
    message String,
    files_added UInt32,
    files_deleted UInt32,
    files_renamed UInt32,
    files_modified UInt32,
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.file_changes
(
    change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.line_changes
(
    sign Int8,
    line_number_old UInt32,
    line_number_new UInt32,
    hunk_num UInt32,
    hunk_start_line_number_old UInt32,
    hunk_start_line_number_new UInt32,
    hunk_lines_added UInt32,
    hunk_lines_deleted UInt32,
    hunk_context LowCardinality(String),
    line LowCardinality(String),
    indent UInt8,
    line_type Enum('Empty' = 0, 'Comment' = 1, 'Punct' = 2, 'Code' = 3),

    prev_commit_hash String,
    prev_author LowCardinality(String),
    prev_time DateTime,

    file_change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    file_lines_added UInt32,
    file_lines_deleted UInt32,
    file_hunks_added UInt32,
    file_hunks_removed UInt32,
    file_hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

Run the tool.

Then insert the data with the following commands:

clickhouse-client --query "INSERT INTO git.commits FORMAT TSV" < commits.tsv
clickhouse-client --query "INSERT INTO git.file_changes FORMAT TSV" < file_changes.tsv
clickhouse-client --query "INSERT INTO git.line_changes FORMAT TSV" < line_changes.tsv


Usage: clickhouse git-import
clickhouse git-import --skip-paths 'generated\.cpp|^(contrib|docs?|website|libs/(libcityhash|liblz4|libdivide|libvectorclass|libdouble-conversion|libcpuid|libzstd|libfarmhash|libmetrohash|libpoco|libwidechar_width))/' --skip-commits-with-messages '^Merge branch '

clickhouse-keeper-converter

Usage: clickhouse keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/data/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/data/version-2 --output-dir /var/lib/clickhouse/coordination/snapshots

clickhouse-static-files-disk-uploader


clickhouse-su

Usage: clickhouse su user:group ...

clickhouse-disks

ClickHouse disk management tool
Usage clickhouse-disks [OPTION] 	read	Read a file from `FROM_PATH` to `TO_PATH`

	write	Write a file from `FROM_PATH` to `TO_PATH`

	link	Create hardlink from `from_path` to `to_path`

	mkdir	Create a directory

	remove	Remove file or directory with all children. Throws exception if file doesn't exists. Path should be in format './' or './path' or 'path'

	move	Move file or directory from `from_path` to `to_path`

	copy	Recursively copy data from `FROM_PATH` to `TO_PATH`

	list	List files at path[s]

	list-disks	List disks names

clickhouse-disks:
  -h [ --help ]     Print common help message
  -C [ --config-file ] arg
                    Set config file
  --disk arg        Set disk name
  --command_name arg
                    Name for command to do
  --save-logs       Save logs to a file
  --log-level arg   Logging level

