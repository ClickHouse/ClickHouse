================BINARY==========================

clickhouse --help

Use one of the following commands:
clickhouse local [args] 
clickhouse client [args] 
clickhouse benchmark [args] 
clickhouse server [args] 
clickhouse extract-from-config [args] 
clickhouse compressor [args] 
clickhouse format [args] 
clickhouse copier [args] 
clickhouse obfuscator [args] 
clickhouse git-import [args] 
clickhouse keeper [args] 
clickhouse keeper-converter [args] 
clickhouse install [args] 
clickhouse start [args] 
clickhouse stop [args] 
clickhouse status [args] 
clickhouse restart [args] 
clickhouse static-files-disk-uploader [args] 
clickhouse su [args] 
clickhouse hash-binary [args] 
clickhouse disks [args] 
clickhouse help [args] 

clickhouse help

Use one of the following commands:
clickhouse local [args] 
clickhouse client [args] 
clickhouse benchmark [args] 
clickhouse server [args] 
clickhouse extract-from-config [args] 
clickhouse compressor [args] 
clickhouse format [args] 
clickhouse copier [args] 
clickhouse obfuscator [args] 
clickhouse git-import [args] 
clickhouse keeper [args] 
clickhouse keeper-converter [args] 
clickhouse install [args] 
clickhouse start [args] 
clickhouse stop [args] 
clickhouse status [args] 
clickhouse restart [args] 
clickhouse static-files-disk-uploader [args] 
clickhouse su [args] 
clickhouse hash-binary [args] 
clickhouse disks [args] 
clickhouse help [args] 

clickhouse local

Usage: clickhouse local [initial table definition] [--query <query>]
clickhouse-local allows to execute SQL queries on your data files without running clickhouse-server.

It can run as command line tool that does single action or as interactive client. For interactive experience you can just run 'clickhouse local' or add --interactive argument to your command. It will set up tables, run queries and pass control as if it is clickhouse-client. Then you can execute your SQL queries in usual manner. Non-interactive mode requires query as an argument and exits when queries finish. Multiple SQL queries can be passed as --query argument.

To configure initial environment two ways are supported: queries or command line parameters. Either just in first query like this:
    CREATE TABLE <table> (<structure>) ENGINE = File(<input-format>, <file>);
Or through corresponding command line parameters --table --structure --input-format and --file.

clickhouse-local supports all features and engines of ClickHouse. You can query data from remote engines and store results locally or other way around. For table engines that actually store data on a disk like Log and MergeTree clickhouse-local puts data to temporary directory that is not reused between runs.

clickhouse-local can be used to query data from stopped clickhouse-server installation with --path to local directory with data.

Example reading file from S3, converting format and writing to a file:
clickhouse-local --query "SELECT c1 as version, c2 as date FROM url('https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/utils/list-versions/version_date.tsv') INTO OUTFILE '/tmp/versions.json'"

clickhouse client

Usage: clickhouse client [initial table definition] [--query <query>]
clickhouse-client is a client application that is used to connect to ClickHouse.
It can run queries as command line tool if you pass queries as an argument or as interactive client. Queries can run one at a time, or in in a multiquery mode with --multiquery option. To change settings you may use 'SET' statements and SETTINGS clause in queries or set is for a  session with corresponding clickhouse-client arguments.
'clickhouse client' command will try connect to clickhouse-server running on the same server. If you have credentials set up pass them with --user <username> --password <password> or with --ask-password argument that will open command prompt.

This one will try connect to tcp native port(9000) without encryption:
    clickhouse client --host clickhouse.example.com --password mysecretpassword
To connect to secure endpoint just set --secure argument. If you have  artered port set it with --port <your port>.
    clickhouse client --secure --host clickhouse.example.com --password mysecretpassword


clickhouse benchmark

Usage: clickhouse benchmark [options] < queries.txt
Usage: clickhouse benchmark [options] --query "query text"
clickhouse-benchmark connects to ClickHouse server, repeatedly sends specified queries and produces reports query statistics.

clickhouse server

usage: 
clickhouse server [OPTION] [-- [ARG]...]
positional arguments can be used to rewrite config.xml properties, for 
example, --http_port=8010

-h, --help                        show help and exit
-V, --version                     show version and exit
-C<file>, --config-file=<file>    load configuration from a given file
-L<file>, --log-file=<file>       use given log file
-E<file>, --errorlog-file=<file>  use given log file for errors only
-P<file>, --pid-file=<file>       use given pidfile
--daemon                          Run application as a daemon.
--umask=mask                      Set the daemon's umask (octal, e.g. 027).
--pidfile=path                    Write the process ID of the application to 
                                  given file.

clickhouse extract

Preprocess config file and extract value of the given key.

Usage: clickhouse extract-from-config [options]

Allowed options:
  --help                   produce this help message
  --stacktrace             print stack traces of exceptions
  --process-zk-includes    if there are from_zk elements in config, connect to 
                           ZooKeeper and process them
  --try                    Do not warn about missing keys
  --log-level arg (=error) log level
  -c [ --config-file ] arg path to config file
  -k [ --key ] arg         key to get value for


clickhouse compressor

Usage: clickhouse compressor [options] < INPUT > OUTPUT
Alternative usage: clickhouse compressor [options] INPUT OUTPUT
Allowed options:
  -h [ --help ]                       produce help message
  --input INPUT                       input file
  --output OUTPUT                     output file
  -d [ --decompress ]                 decompress
  --offset-in-compressed-file arg (=0)
                                      offset to the compressed block (i.e. 
                                      physical file offset)
  --offset-in-decompressed-block arg (=0)
                                      offset to the decompressed block (i.e. 
                                      virtual offset)
  -b [ --block-size ] arg (=1048576)  compress in blocks of specified size
  --hc                                use LZ4HC instead of LZ4
  --zstd                              use ZSTD instead of LZ4
  --deflate_qpl                       use deflate_qpl instead of LZ4
  --codec arg                         use codecs combination instead of LZ4
  --level arg                         compression level for codecs specified 
                                      via flags
  --none                              use no compression instead of LZ4
  --stat                              print block statistics of compressed 
                                      data


clickhouse format

Usage: clickhouse format [options] < query
Allowed options:
  --query arg                         query to format
  -h [ --help ]                       produce help message
  --hilite                            add syntax highlight with ANSI terminal
                                      escape sequences
  --oneline                           format in single line
  -q [ --quiet ]                      just check syntax, no output on success
  -n [ --multiquery ]                 allow multiple queries in the same file
  --obfuscate                         obfuscate instead of formatting
  --backslash                         add a backslash at the end of each line
                                      of the formatted query
  --allow_settings_after_format_in_insert 
                                      Allow SETTINGS after FORMAT, but note, 
                                      that this is not always safe
  --seed arg                          seed (arbitrary string) that determines
                                      the result of obfuscation
  --max_query_size arg                Which part of the query can be read 
                                      into RAM for parsing (the remaining 
                                      data for INSERT, if any, is read later)
  --max_parser_depth arg              Maximum parser depth (recursion depth 
                                      of recursive descend parser).


clickhouse copier

usage: clickhouse copier --config-file <config-file> --task-path <task-path>
Copies tables from one cluster to another

-C<file>, --config-file=<file>                                                         
                                                                                       load
                                                                                       configuration
                                                                                       from
                                                                                       a
                                                                                       given
                                                                                       file
-L<file>, --log-file=<file>                                                            
                                                                                       use
                                                                                       given
                                                                                       log
                                                                                       file
-E<file>, --errorlog-file=<file>                                                       
                                                                                       use
                                                                                       given
                                                                                       log
                                                                                       file
                                                                                       for
                                                                                       errors
                                                                                       only
-P<file>, --pid-file=<file>                                                            
                                                                                       use
                                                                                       given
                                                                                       pidfile
--daemon                                                                               
                                                                                       Run
                                                                                       application
                                                                                       as
                                                                                       a
                                                                                       daemon.
--umask=mask                                                                           
                                                                                       Set
                                                                                       the
                                                                                       daemon's
                                                                                       umask
                                                                                       (octal,
                                                                                       e.g.
                                                                                       027).
--pidfile=path                                                                         
                                                                                       Write
                                                                                       the
                                                                                       process
                                                                                       ID
                                                                                       of
                                                                                       the
                                                                                       application
                                                                                       to
                                                                                       given
                                                                                       file.
--task-path=task-path                                                                  
                                                                                       path
                                                                                       to
                                                                                       task
                                                                                       in
                                                                                       ZooKeeper
--task-file=task-file                                                                  
                                                                                       path
                                                                                       to
                                                                                       task
                                                                                       file
                                                                                       for
                                                                                       uploading
                                                                                       in
                                                                                       ZooKeeper
                                                                                       to
                                                                                       task-path
--task-upload-force=task-upload-force                                                  
                                                                                       Force
                                                                                       upload
                                                                                       task-file
                                                                                       even
                                                                                       node
                                                                                       already
                                                                                       exists
--safe-mode                                                                            
                                                                                       disables
                                                                                       ALTER
                                                                                       DROP
                                                                                       PARTITION
                                                                                       in
                                                                                       case
                                                                                       of
                                                                                       errors
--copy-fault-probability=copy-fault-probability                                        
                                                                                       the
                                                                                       copying
                                                                                       fails
                                                                                       with
                                                                                       specified
                                                                                       probability
                                                                                       (used
                                                                                       to
                                                                                       test
                                                                                       partition
                                                                                       state
                                                                                       recovering)
--move-fault-probability=move-fault-probability                                        
                                                                                       the
                                                                                       moving
                                                                                       fails
                                                                                       with
                                                                                       specified
                                                                                       probability
                                                                                       (used
                                                                                       to
                                                                                       test
                                                                                       partition
                                                                                       state
                                                                                       recovering)
--log-level=log-level                                                                  
                                                                                       sets
                                                                                       log
                                                                                       level
--base-dir=base-dir                                                                    
                                                                                       base
                                                                                       directory
                                                                                       for
                                                                                       copiers,
                                                                                       consecutive
                                                                                       copier
                                                                                       launches
                                                                                       will
                                                                                       populate
                                                                                       /base-dir/launch_id/*
                                                                                       directories
--experimental-use-sample-offset=experimental-use-sample-offset                        
                                                                                       Use
                                                                                       SAMPLE
                                                                                       OFFSET
                                                                                       query
                                                                                       instead
                                                                                       of
                                                                                       cityHash64(PRIMARY
                                                                                       KEY)
                                                                                       %
                                                                                       n
                                                                                       ==
                                                                                       k
--status                                                                               
                                                                                       Get
                                                                                       for
                                                                                       status
                                                                                       for
                                                                                       current
                                                                                       execution
--max-table-tries=max-table-tries                                                      
                                                                                       Number
                                                                                       of
                                                                                       tries
                                                                                       for
                                                                                       the
                                                                                       copy
                                                                                       table
                                                                                       task
--max-shard-partition-tries=max-shard-partition-tries                                  
                                                                                       Number
                                                                                       of
                                                                                       tries
                                                                                       for
                                                                                       the
                                                                                       copy
                                                                                       one
                                                                                       partition
                                                                                       task
--max-shard-partition-piece-tries-for-alter=max-shard-partition-piece-tries-for-alter  
                                                                                       Number
                                                                                       of
                                                                                       tries
                                                                                       for
                                                                                       final
                                                                                       ALTER
                                                                                       ATTACH
                                                                                       to
                                                                                       destination
                                                                                       table
--retry-delay-ms=retry-delay-ms                                                        
                                                                                       Delay
                                                                                       between
                                                                                       task
                                                                                       retries
--help                                                                                 
                                                                                       produce
                                                                                       this
                                                                                       help
                                                                                       message

clickhouse obfuscator


Simple tool for table data obfuscation.

It reads input table and produces output table, that retain some properties of input, but contains different data.
It allows to publish almost real production data for usage in benchmarks.

It is designed to retain the following properties of data:
- cardinalities of values (number of distinct values) for every column and for every tuple of columns;
- conditional cardinalities: number of distinct values of one column under condition on value of another column;
- probability distributions of absolute value of integers; sign of signed integers; exponent and sign for floats;
- probability distributions of length of strings;
- probability of zero values of numbers; empty strings and arrays, NULLs;
- data compression ratio when compressed with LZ77 and entropy family of codecs;
- continuity (magnitude of difference) of time values across table; continuity of floating point values.
- date component of DateTime values;
- UTF-8 validity of string values;
- string values continue to look somewhat natural.

Most of the properties above are viable for performance testing:
- reading data, filtering, aggregation and sorting will work at almost the same speed
    as on original data due to saved cardinalities, magnitudes, compression ratios, etc.

It works in deterministic fashion: you define a seed value and transform is totally determined by input data and by seed.
Some transforms are one to one and could be reversed, so you need to have large enough seed and keep it in secret.

It use some cryptographic primitives to transform data, but from the cryptographic point of view,
    it doesn't do anything properly and you should never consider the result as secure, unless you have other reasons for it.

It may retain some data you don't want to publish.

It always leave numbers 0, 1, -1 as is. Also it leaves dates, lengths of arrays and null flags exactly as in source data.
For example, you have a column IsMobile in your table with values 0 and 1. In transformed data, it will have the same value.
So, the user will be able to count exact ratio of mobile traffic.

Another example, suppose you have some private data in your table, like user email and you don't want to publish any single email address.
If your table is large enough and contain multiple different emails and there is no email that have very high frequency than all others,
    it will perfectly anonymize all data. But if you have small amount of different values in a column, it can possibly reproduce some of them.
And you should take care and look at exact algorithm, how this tool works, and probably fine tune some of it command line parameters.

This tool works fine only with reasonable amount of data (at least 1000s of rows).


Usage: clickhouse obfuscator [options] < in > out

Input must be seekable file (it will be read twice).

Options:
  --help                              produce help message
  -S [ --structure ] arg              structure of the initial table (list of
                                      column and type names)
  --input-format arg                  input format of the initial table data
  --output-format arg                 default output format
  --seed arg                          seed (arbitrary string), must be random
                                      string with at least 10 bytes length; 
                                      note that a seed for each column is 
                                      derived from this seed and a column 
                                      name: you can obfuscate data for 
                                      different tables and as long as you use
                                      identical seed and identical column 
                                      names, the data for corresponding 
                                      non-text columns for different tables 
                                      will be transformed in the same way, so
                                      the data for different tables can be 
                                      JOINed after obfuscation
  --limit arg                         if specified - stop after generating 
                                      that number of rows; the limit can be 
                                      also greater than the number of source 
                                      dataset - in this case it will process 
                                      the dataset in a loop more than one 
                                      time, using different seeds on every 
                                      iteration, generating result as large 
                                      as needed
  --silent arg (=0)                   don't print information messages to 
                                      stderr
  --save arg                          save the models after training to the 
                                      specified file. You can use --limit 0 
                                      to skip the generation step. The file 
                                      is using binary, platform-dependent, 
                                      opaque serialization format. The model 
                                      parameters are saved, while the seed is
                                      not.
  --load arg                          load the models instead of training 
                                      from the specified file. The table 
                                      structure must match the saved file. 
                                      The seed should be specified 
                                      separately, while other model 
                                      parameters are loaded.
  --order arg (=5)                    order of markov model to generate 
                                      strings
  --frequency-cutoff arg (=5)         frequency cutoff for markov model: 
                                      remove all buckets with count less than
                                      specified
  --num-buckets-cutoff arg (=0)       cutoff for number of different possible
                                      continuations for a context: remove all
                                      histograms with less than specified 
                                      number of buckets
  --frequency-add arg (=0)            add a constant to every count to lower 
                                      probability distribution skew
  --frequency-desaturate arg (=0)     0..1 - move every frequency towards 
                                      average to lower probability 
                                      distribution skew
  --determinator-sliding-window-size arg (=8)
                                      size of a sliding window in a source 
                                      string - its hash is used as a seed for
                                      RNG in markov model


Example:
    clickhouse obfuscator --seed "$(head -c16 /dev/urandom | base64)" --input-format TSV --output-format TSV --structure 'CounterID UInt32, URLDomain String, URL String, SearchPhrase String, Title String' < stats.tsv

clickhouse git-import


A tool to extract information from Git repository for analytics.

It dumps the data for the following tables:
- commits - commits with statistics;
- file_changes - files changed in every commit with the info about the change and statistics;
- line_changes - every changed line in every changed file in every commit with full info about the line and the information about previous change of this line.

The largest and the most important table is "line_changes".

Allows to answer questions like:
- list files with maximum number of authors;
- show me the oldest lines of code in the repository;
- show me the files with longest history;
- list favorite files for author;
- list largest files with lowest number of authors;
- at what weekday the code has highest chance to stay in repository;
- the distribution of code age across repository;
- files sorted by average code age;
- quickly show file with blame info (rough);
- commits and lines of code distribution by time; by weekday, by author; for specific subdirectories;
- show history for every subdirectory, file, line of file, the number of changes (lines and commits) across time; how the number of contributors was changed across time;
- list files with most modifications;
- list files that were rewritten most number of time or by most of authors;
- what is percentage of code removal by other authors, across authors;
- the matrix of authors that shows what authors tends to rewrite another authors code;
- what is the worst time to write code in sense that the code has highest chance to be rewritten;
- the average time before code will be rewritten and the median (half-life of code decay);
- comments/code percentage change in time / by author / by location;
- who tend to write more tests / cpp code / comments.

The data is intended for analytical purposes. It can be imprecise by many reasons but it should be good enough for its purpose.

The data is not intended to provide any conclusions for managers, it is especially counter-indicative for any kinds of "performance review". Instead you can spend multiple days looking at various interesting statistics.

Run this tool inside your git repository. It will create .tsv files that can be loaded into ClickHouse (or into other DBMS if you dare).

The tool can process large enough repositories in a reasonable time.
It has been tested on:
- ClickHouse: 31 seconds; 3 million rows;
- LLVM: 8 minutes; 62 million rows;
- Linux - 12 minutes; 85 million rows;
- Chromium - 67 minutes; 343 million rows;
(the numbers as of Sep 2020)


Prepare the database by executing the following queries:

DROP DATABASE IF EXISTS git;
CREATE DATABASE git;

CREATE TABLE git.commits
(
    hash String,
    author LowCardinality(String),
    time DateTime,
    message String,
    files_added UInt32,
    files_deleted UInt32,
    files_renamed UInt32,
    files_modified UInt32,
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.file_changes
(
    change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.line_changes
(
    sign Int8,
    line_number_old UInt32,
    line_number_new UInt32,
    hunk_num UInt32,
    hunk_start_line_number_old UInt32,
    hunk_start_line_number_new UInt32,
    hunk_lines_added UInt32,
    hunk_lines_deleted UInt32,
    hunk_context LowCardinality(String),
    line LowCardinality(String),
    indent UInt8,
    line_type Enum('Empty' = 0, 'Comment' = 1, 'Punct' = 2, 'Code' = 3),

    prev_commit_hash String,
    prev_author LowCardinality(String),
    prev_time DateTime,

    file_change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    file_lines_added UInt32,
    file_lines_deleted UInt32,
    file_hunks_added UInt32,
    file_hunks_removed UInt32,
    file_hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

Run the tool.

Then insert the data with the following commands:

clickhouse-client --query "INSERT INTO git.commits FORMAT TSV" < commits.tsv
clickhouse-client --query "INSERT INTO git.file_changes FORMAT TSV" < file_changes.tsv
clickhouse-client --query "INSERT INTO git.line_changes FORMAT TSV" < line_changes.tsv


Usage: clickhouse git-import
Allowed options:
  -h [ --help ]                       produce help message
  --skip-commits-without-parents arg (=1)
                                      Skip commits without parents (except 
                                      the initial commit). These commits are 
                                      usually erroneous but they can make 
                                      sense in very rare cases.
  --skip-commits-with-duplicate-diffs arg (=1)
                                      Skip commits with duplicate diffs. 
                                      These commits are usually results of 
                                      cherry-pick or merge after rebase.
  --skip-commit arg                   Skip commit with specified hash. The 
                                      option can be specified multiple times.
  --skip-paths arg                    Skip paths that matches regular 
                                      expression (re2 syntax).
  --skip-commits-with-messages arg    Skip commits whose messages matches 
                                      regular expression (re2 syntax).
  --diff-size-limit arg (=100000)     Skip commits whose diff size (number of
                                      added + removed lines) is larger than 
                                      specified threshold. Does not apply for
                                      initial commit.
  --stop-after-commit arg             Stop processing after specified commit 
                                      hash.
  --threads arg (=8)                  Number of concurrent git subprocesses 
                                      to spawn


Example:

clickhouse git-import --skip-paths 'generated\.cpp|^(contrib|docs?|website|libs/(libcityhash|liblz4|libdivide|libvectorclass|libdouble-conversion|libcpuid|libzstd|libfarmhash|libmetrohash|libpoco|libwidechar_width))/' --skip-commits-with-messages '^Merge branch '

clickhouse keeper

usage: 
clickhouse keeper [OPTION] [-- [ARG]...]
positional arguments can be used to rewrite config.xml properties, for 
example, --http_port=8010

-h, --help                         show help and exit
-V, --version                      show version and exit
-force-recovery, --force-recovery  Force recovery mode allowing Keeper to 
                                   overwrite cluster configuration without 
                                   quorum
-C<file>, --config-file=<file>     load configuration from a given file
-L<file>, --log-file=<file>        use given log file
-E<file>, --errorlog-file=<file>   use given log file for errors only
-P<file>, --pid-file=<file>        use given pidfile
--daemon                           Run application as a daemon.
--umask=mask                       Set the daemon's umask (octal, e.g. 027).
--pidfile=path                     Write the process ID of the application to 
                                   given file.

clickhouse keeper-converter

Usage: clickhouse keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/data/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/data/version-2 --output-dir /var/lib/clickhouse/coordination/snapshots
Allowed options:
  -h [ --help ]                 produce help message
  --zookeeper-logs-dir arg      Path to directory with ZooKeeper logs
  --zookeeper-snapshots-dir arg Path to directory with ZooKeeper snapshots
  --output-dir arg              Directory to place output clickhouse-keeper 
                                snapshot


clickhouse install

Usage: sudo clickhouse install [options]
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --binary-path arg (=usr/bin)          where to install binaries
  --config-path arg (=etc/clickhouse-server)
                                        where to install configs
  --log-path arg (=var/log/clickhouse-server)
                                        where to create log directory
  --data-path arg (=var/lib/clickhouse) directory for data
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --user arg (=clickhouse)              clickhouse user
  --group arg (=clickhouse)             clickhouse group


clickhouse start

Usage: sudo clickhouse start
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --binary-path arg (=usr/bin)          directory with binary
  --config-path arg (=etc/clickhouse-server)
                                        directory with configs
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --user arg (=clickhouse)              clickhouse user
  --group arg (=clickhouse)             clickhouse group
  --max-tries arg (=60)                 Max number of tries for waiting the 
                                        server (with 1 second delay)
  --no-sudo                             Use clickhouse su if sudo is 
                                        unavailable


clickhouse stop

Usage: sudo clickhouse stop
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --force                               Stop with KILL signal instead of TERM
  --do-not-kill                         Do not send KILL even if TERM did not 
                                        help
  --max-tries arg (=60)                 Max number of tries for waiting the 
                                        server to finish after sending TERM 
                                        (with 1 second delay)


clickhouse status

Usage: sudo clickhouse status
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file


clickhouse restart

Usage: sudo clickhouse restart
  -h [ --help ]                         produce help message
  --prefix arg (=/)                     prefix for all paths
  --binary-path arg (=usr/bin)          directory with binary
  --config-path arg (=etc/clickhouse-server)
                                        directory with configs
  --pid-path arg (=var/run/clickhouse-server)
                                        directory for pid file
  --user arg (=clickhouse)              clickhouse user
  --group arg (=clickhouse)             clickhouse group
  --force arg (=0)                      Stop with KILL signal instead of TERM
  --do-not-kill                         Do not send KILL even if TERM did not 
                                        help
  --max-tries arg (=60)                 Max number of tries for waiting the 
                                        server (with 1 second delay)
  --no-sudo                             Use clickhouse su if sudo is 
                                        unavailable


clickhouse static

Allowed options:
  -h [ --help ]         produce help message
  --metadata-path arg   Metadata path (SELECT data_paths FROM system.tables 
                        WHERE name = 'table_name' AND database = 
                        'database_name')
  --test-mode           Use test mode, which will put data on given url via 
                        PUT
  --link                Create symlinks instead of copying
  --url arg             Web server url for test mode
  --output-dir arg      Directory to put files in non-test mode


clickhouse su

Usage: clickhouse su user:group ...

clickhouse hash

Usage: clickhouse hash
Prints hash of clickhouse binary.
 -h, --help   Prints this message
Result is intentionally without newline. So you can run:
objcopy --add-section .clickhouse.hash=<(./clickhouse hash-binary) clickhouse.


clickhouse disks

ClickHouse disk management tool
usage clickhouse disks [OPTION]
clickhouse-disks

read	read File `from_path` to `to_path` or to stdout
Path should be in format './' or './path' or 'path'

write	Write File `from_path` or stdin to `to_path`

link	Create hardlink from `from_path` to `to_path`
Path should be in format './' or './path' or 'path'

mkdir	Create directory or directories recursively

remove	Remove file or directory with all children. Throws exception if file doesn't exists.
Path should be in format './' or './path' or 'path'

move	Move file or directory from `from_path` to `to_path`
Path should be in format './' or './path' or 'path'

copy	Recursively copy data containing at `from_path` to `to_path`
Path should be in format './' or './path' or 'path'

list	List files (the default disk is used by default)
Path should be in format './' or './path' or 'path'

list-disks	List disks names

clickhouse-disks:
  -h [ --help ]            Print common help message
  -C [ --config-file ] arg Set config file
  --disk arg               Set disk name
  --command_name arg       Name for command to do
  --save-logs              Save logs to a file
  --log-level arg          Logging level

================SYMLINK==============================

clickhouse-local

Usage: clickhouse local [initial table definition] [--query <query>]
clickhouse-local allows to execute SQL queries on your data files without running clickhouse-server.

It can run as command line tool that does single action or as interactive client. For interactive experience you can just run 'clickhouse local' or add --interactive argument to your command. It will set up tables, run queries and pass control as if it is clickhouse-client. Then you can execute your SQL queries in usual manner. Non-interactive mode requires query as an argument and exits when queries finish. Multiple SQL queries can be passed as --query argument.

To configure initial environment two ways are supported: queries or command line parameters. Either just in first query like this:
    CREATE TABLE <table> (<structure>) ENGINE = File(<input-format>, <file>);
Or through corresponding command line parameters --table --structure --input-format and --file.

clickhouse-local supports all features and engines of ClickHouse. You can query data from remote engines and store results locally or other way around. For table engines that actually store data on a disk like Log and MergeTree clickhouse-local puts data to temporary directory that is not reused between runs.

clickhouse-local can be used to query data from stopped clickhouse-server installation with --path to local directory with data.

Example reading file from S3, converting format and writing to a file:
clickhouse-local --query "SELECT c1 as version, c2 as date FROM url('https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/utils/list-versions/version_date.tsv') INTO OUTFILE '/tmp/versions.json'"

clickhouse-client

Usage: clickhouse client [initial table definition] [--query <query>]
clickhouse-client is a client application that is used to connect to ClickHouse.
It can run queries as command line tool if you pass queries as an argument or as interactive client. Queries can run one at a time, or in in a multiquery mode with --multiquery option. To change settings you may use 'SET' statements and SETTINGS clause in queries or set is for a  session with corresponding clickhouse-client arguments.
'clickhouse client' command will try connect to clickhouse-server running on the same server. If you have credentials set up pass them with --user <username> --password <password> or with --ask-password argument that will open command prompt.

This one will try connect to tcp native port(9000) without encryption:
    clickhouse client --host clickhouse.example.com --password mysecretpassword
To connect to secure endpoint just set --secure argument. If you have  artered port set it with --port <your port>.
    clickhouse client --secure --host clickhouse.example.com --password mysecretpassword


clickhouse-benchmark

Usage: clickhouse benchmark [options] < queries.txt
Usage: clickhouse benchmark [options] --query "query text"
clickhouse-benchmark connects to ClickHouse server, repeatedly sends specified queries and produces reports query statistics.

clickhouse-server

usage: 
clickhouse-server [OPTION] [-- [ARG]...]
positional arguments can be used to rewrite config.xml properties, for 
example, --http_port=8010

-h, --help                        show help and exit
-V, --version                     show version and exit
-C<file>, --config-file=<file>    load configuration from a given file
-L<file>, --log-file=<file>       use given log file
-E<file>, --errorlog-file=<file>  use given log file for errors only
-P<file>, --pid-file=<file>       use given pidfile
--daemon                          Run application as a daemon.
--umask=mask                      Set the daemon's umask (octal, e.g. 027).
--pidfile=path                    Write the process ID of the application to 
                                  given file.

clickhouse-extract

Preprocess config file and extract value of the given key.

Usage: clickhouse extract-from-config [options]

Allowed options:
  --help                   produce this help message
  --stacktrace             print stack traces of exceptions
  --process-zk-includes    if there are from_zk elements in config, connect to 
                           ZooKeeper and process them
  --try                    Do not warn about missing keys
  --log-level arg (=error) log level
  -c [ --config-file ] arg path to config file
  -k [ --key ] arg         key to get value for


clickhouse-compressor

Usage: clickhouse compressor [options] < INPUT > OUTPUT
Alternative usage: clickhouse compressor [options] INPUT OUTPUT
Allowed options:
  -h [ --help ]                       produce help message
  --input INPUT                       input file
  --output OUTPUT                     output file
  -d [ --decompress ]                 decompress
  --offset-in-compressed-file arg (=0)
                                      offset to the compressed block (i.e. 
                                      physical file offset)
  --offset-in-decompressed-block arg (=0)
                                      offset to the decompressed block (i.e. 
                                      virtual offset)
  -b [ --block-size ] arg (=1048576)  compress in blocks of specified size
  --hc                                use LZ4HC instead of LZ4
  --zstd                              use ZSTD instead of LZ4
  --deflate_qpl                       use deflate_qpl instead of LZ4
  --codec arg                         use codecs combination instead of LZ4
  --level arg                         compression level for codecs specified 
                                      via flags
  --none                              use no compression instead of LZ4
  --stat                              print block statistics of compressed 
                                      data


clickhouse-format

Usage: clickhouse format [options] < query
Allowed options:
  --query arg                         query to format
  -h [ --help ]                       produce help message
  --hilite                            add syntax highlight with ANSI terminal
                                      escape sequences
  --oneline                           format in single line
  -q [ --quiet ]                      just check syntax, no output on success
  -n [ --multiquery ]                 allow multiple queries in the same file
  --obfuscate                         obfuscate instead of formatting
  --backslash                         add a backslash at the end of each line
                                      of the formatted query
  --allow_settings_after_format_in_insert 
                                      Allow SETTINGS after FORMAT, but note, 
                                      that this is not always safe
  --seed arg                          seed (arbitrary string) that determines
                                      the result of obfuscation
  --max_query_size arg                Which part of the query can be read 
                                      into RAM for parsing (the remaining 
                                      data for INSERT, if any, is read later)
  --max_parser_depth arg              Maximum parser depth (recursion depth 
                                      of recursive descend parser).


clickhouse-copier

usage: clickhouse-copier --config-file <config-file> --task-path <task-path>
Copies tables from one cluster to another

-C<file>, --config-file=<file>                                                         
                                                                                       load
                                                                                       configuration
                                                                                       from
                                                                                       a
                                                                                       given
                                                                                       file
-L<file>, --log-file=<file>                                                            
                                                                                       use
                                                                                       given
                                                                                       log
                                                                                       file
-E<file>, --errorlog-file=<file>                                                       
                                                                                       use
                                                                                       given
                                                                                       log
                                                                                       file
                                                                                       for
                                                                                       errors
                                                                                       only
-P<file>, --pid-file=<file>                                                            
                                                                                       use
                                                                                       given
                                                                                       pidfile
--daemon                                                                               
                                                                                       Run
                                                                                       application
                                                                                       as
                                                                                       a
                                                                                       daemon.
--umask=mask                                                                           
                                                                                       Set
                                                                                       the
                                                                                       daemon's
                                                                                       umask
                                                                                       (octal,
                                                                                       e.g.
                                                                                       027).
--pidfile=path                                                                         
                                                                                       Write
                                                                                       the
                                                                                       process
                                                                                       ID
                                                                                       of
                                                                                       the
                                                                                       application
                                                                                       to
                                                                                       given
                                                                                       file.
--task-path=task-path                                                                  
                                                                                       path
                                                                                       to
                                                                                       task
                                                                                       in
                                                                                       ZooKeeper
--task-file=task-file                                                                  
                                                                                       path
                                                                                       to
                                                                                       task
                                                                                       file
                                                                                       for
                                                                                       uploading
                                                                                       in
                                                                                       ZooKeeper
                                                                                       to
                                                                                       task-path
--task-upload-force=task-upload-force                                                  
                                                                                       Force
                                                                                       upload
                                                                                       task-file
                                                                                       even
                                                                                       node
                                                                                       already
                                                                                       exists
--safe-mode                                                                            
                                                                                       disables
                                                                                       ALTER
                                                                                       DROP
                                                                                       PARTITION
                                                                                       in
                                                                                       case
                                                                                       of
                                                                                       errors
--copy-fault-probability=copy-fault-probability                                        
                                                                                       the
                                                                                       copying
                                                                                       fails
                                                                                       with
                                                                                       specified
                                                                                       probability
                                                                                       (used
                                                                                       to
                                                                                       test
                                                                                       partition
                                                                                       state
                                                                                       recovering)
--move-fault-probability=move-fault-probability                                        
                                                                                       the
                                                                                       moving
                                                                                       fails
                                                                                       with
                                                                                       specified
                                                                                       probability
                                                                                       (used
                                                                                       to
                                                                                       test
                                                                                       partition
                                                                                       state
                                                                                       recovering)
--log-level=log-level                                                                  
                                                                                       sets
                                                                                       log
                                                                                       level
--base-dir=base-dir                                                                    
                                                                                       base
                                                                                       directory
                                                                                       for
                                                                                       copiers,
                                                                                       consecutive
                                                                                       copier
                                                                                       launches
                                                                                       will
                                                                                       populate
                                                                                       /base-dir/launch_id/*
                                                                                       directories
--experimental-use-sample-offset=experimental-use-sample-offset                        
                                                                                       Use
                                                                                       SAMPLE
                                                                                       OFFSET
                                                                                       query
                                                                                       instead
                                                                                       of
                                                                                       cityHash64(PRIMARY
                                                                                       KEY)
                                                                                       %
                                                                                       n
                                                                                       ==
                                                                                       k
--status                                                                               
                                                                                       Get
                                                                                       for
                                                                                       status
                                                                                       for
                                                                                       current
                                                                                       execution
--max-table-tries=max-table-tries                                                      
                                                                                       Number
                                                                                       of
                                                                                       tries
                                                                                       for
                                                                                       the
                                                                                       copy
                                                                                       table
                                                                                       task
--max-shard-partition-tries=max-shard-partition-tries                                  
                                                                                       Number
                                                                                       of
                                                                                       tries
                                                                                       for
                                                                                       the
                                                                                       copy
                                                                                       one
                                                                                       partition
                                                                                       task
--max-shard-partition-piece-tries-for-alter=max-shard-partition-piece-tries-for-alter  
                                                                                       Number
                                                                                       of
                                                                                       tries
                                                                                       for
                                                                                       final
                                                                                       ALTER
                                                                                       ATTACH
                                                                                       to
                                                                                       destination
                                                                                       table
--retry-delay-ms=retry-delay-ms                                                        
                                                                                       Delay
                                                                                       between
                                                                                       task
                                                                                       retries
--help                                                                                 
                                                                                       produce
                                                                                       this
                                                                                       help
                                                                                       message

clickhouse-obfuscator


Simple tool for table data obfuscation.

It reads input table and produces output table, that retain some properties of input, but contains different data.
It allows to publish almost real production data for usage in benchmarks.

It is designed to retain the following properties of data:
- cardinalities of values (number of distinct values) for every column and for every tuple of columns;
- conditional cardinalities: number of distinct values of one column under condition on value of another column;
- probability distributions of absolute value of integers; sign of signed integers; exponent and sign for floats;
- probability distributions of length of strings;
- probability of zero values of numbers; empty strings and arrays, NULLs;
- data compression ratio when compressed with LZ77 and entropy family of codecs;
- continuity (magnitude of difference) of time values across table; continuity of floating point values.
- date component of DateTime values;
- UTF-8 validity of string values;
- string values continue to look somewhat natural.

Most of the properties above are viable for performance testing:
- reading data, filtering, aggregation and sorting will work at almost the same speed
    as on original data due to saved cardinalities, magnitudes, compression ratios, etc.

It works in deterministic fashion: you define a seed value and transform is totally determined by input data and by seed.
Some transforms are one to one and could be reversed, so you need to have large enough seed and keep it in secret.

It use some cryptographic primitives to transform data, but from the cryptographic point of view,
    it doesn't do anything properly and you should never consider the result as secure, unless you have other reasons for it.

It may retain some data you don't want to publish.

It always leave numbers 0, 1, -1 as is. Also it leaves dates, lengths of arrays and null flags exactly as in source data.
For example, you have a column IsMobile in your table with values 0 and 1. In transformed data, it will have the same value.
So, the user will be able to count exact ratio of mobile traffic.

Another example, suppose you have some private data in your table, like user email and you don't want to publish any single email address.
If your table is large enough and contain multiple different emails and there is no email that have very high frequency than all others,
    it will perfectly anonymize all data. But if you have small amount of different values in a column, it can possibly reproduce some of them.
And you should take care and look at exact algorithm, how this tool works, and probably fine tune some of it command line parameters.

This tool works fine only with reasonable amount of data (at least 1000s of rows).


Usage: clickhouse obfuscator [options] < in > out

Input must be seekable file (it will be read twice).

Options:
  --help                              produce help message
  -S [ --structure ] arg              structure of the initial table (list of
                                      column and type names)
  --input-format arg                  input format of the initial table data
  --output-format arg                 default output format
  --seed arg                          seed (arbitrary string), must be random
                                      string with at least 10 bytes length; 
                                      note that a seed for each column is 
                                      derived from this seed and a column 
                                      name: you can obfuscate data for 
                                      different tables and as long as you use
                                      identical seed and identical column 
                                      names, the data for corresponding 
                                      non-text columns for different tables 
                                      will be transformed in the same way, so
                                      the data for different tables can be 
                                      JOINed after obfuscation
  --limit arg                         if specified - stop after generating 
                                      that number of rows; the limit can be 
                                      also greater than the number of source 
                                      dataset - in this case it will process 
                                      the dataset in a loop more than one 
                                      time, using different seeds on every 
                                      iteration, generating result as large 
                                      as needed
  --silent arg (=0)                   don't print information messages to 
                                      stderr
  --save arg                          save the models after training to the 
                                      specified file. You can use --limit 0 
                                      to skip the generation step. The file 
                                      is using binary, platform-dependent, 
                                      opaque serialization format. The model 
                                      parameters are saved, while the seed is
                                      not.
  --load arg                          load the models instead of training 
                                      from the specified file. The table 
                                      structure must match the saved file. 
                                      The seed should be specified 
                                      separately, while other model 
                                      parameters are loaded.
  --order arg (=5)                    order of markov model to generate 
                                      strings
  --frequency-cutoff arg (=5)         frequency cutoff for markov model: 
                                      remove all buckets with count less than
                                      specified
  --num-buckets-cutoff arg (=0)       cutoff for number of different possible
                                      continuations for a context: remove all
                                      histograms with less than specified 
                                      number of buckets
  --frequency-add arg (=0)            add a constant to every count to lower 
                                      probability distribution skew
  --frequency-desaturate arg (=0)     0..1 - move every frequency towards 
                                      average to lower probability 
                                      distribution skew
  --determinator-sliding-window-size arg (=8)
                                      size of a sliding window in a source 
                                      string - its hash is used as a seed for
                                      RNG in markov model


Example:
    clickhouse obfuscator --seed "$(head -c16 /dev/urandom | base64)" --input-format TSV --output-format TSV --structure 'CounterID UInt32, URLDomain String, URL String, SearchPhrase String, Title String' < stats.tsv

clickhouse-git-import


A tool to extract information from Git repository for analytics.

It dumps the data for the following tables:
- commits - commits with statistics;
- file_changes - files changed in every commit with the info about the change and statistics;
- line_changes - every changed line in every changed file in every commit with full info about the line and the information about previous change of this line.

The largest and the most important table is "line_changes".

Allows to answer questions like:
- list files with maximum number of authors;
- show me the oldest lines of code in the repository;
- show me the files with longest history;
- list favorite files for author;
- list largest files with lowest number of authors;
- at what weekday the code has highest chance to stay in repository;
- the distribution of code age across repository;
- files sorted by average code age;
- quickly show file with blame info (rough);
- commits and lines of code distribution by time; by weekday, by author; for specific subdirectories;
- show history for every subdirectory, file, line of file, the number of changes (lines and commits) across time; how the number of contributors was changed across time;
- list files with most modifications;
- list files that were rewritten most number of time or by most of authors;
- what is percentage of code removal by other authors, across authors;
- the matrix of authors that shows what authors tends to rewrite another authors code;
- what is the worst time to write code in sense that the code has highest chance to be rewritten;
- the average time before code will be rewritten and the median (half-life of code decay);
- comments/code percentage change in time / by author / by location;
- who tend to write more tests / cpp code / comments.

The data is intended for analytical purposes. It can be imprecise by many reasons but it should be good enough for its purpose.

The data is not intended to provide any conclusions for managers, it is especially counter-indicative for any kinds of "performance review". Instead you can spend multiple days looking at various interesting statistics.

Run this tool inside your git repository. It will create .tsv files that can be loaded into ClickHouse (or into other DBMS if you dare).

The tool can process large enough repositories in a reasonable time.
It has been tested on:
- ClickHouse: 31 seconds; 3 million rows;
- LLVM: 8 minutes; 62 million rows;
- Linux - 12 minutes; 85 million rows;
- Chromium - 67 minutes; 343 million rows;
(the numbers as of Sep 2020)


Prepare the database by executing the following queries:

DROP DATABASE IF EXISTS git;
CREATE DATABASE git;

CREATE TABLE git.commits
(
    hash String,
    author LowCardinality(String),
    time DateTime,
    message String,
    files_added UInt32,
    files_deleted UInt32,
    files_renamed UInt32,
    files_modified UInt32,
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.file_changes
(
    change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    lines_added UInt32,
    lines_deleted UInt32,
    hunks_added UInt32,
    hunks_removed UInt32,
    hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

CREATE TABLE git.line_changes
(
    sign Int8,
    line_number_old UInt32,
    line_number_new UInt32,
    hunk_num UInt32,
    hunk_start_line_number_old UInt32,
    hunk_start_line_number_new UInt32,
    hunk_lines_added UInt32,
    hunk_lines_deleted UInt32,
    hunk_context LowCardinality(String),
    line LowCardinality(String),
    indent UInt8,
    line_type Enum('Empty' = 0, 'Comment' = 1, 'Punct' = 2, 'Code' = 3),

    prev_commit_hash String,
    prev_author LowCardinality(String),
    prev_time DateTime,

    file_change_type Enum('Add' = 1, 'Delete' = 2, 'Modify' = 3, 'Rename' = 4, 'Copy' = 5, 'Type' = 6),
    path LowCardinality(String),
    old_path LowCardinality(String),
    file_extension LowCardinality(String),
    file_lines_added UInt32,
    file_lines_deleted UInt32,
    file_hunks_added UInt32,
    file_hunks_removed UInt32,
    file_hunks_changed UInt32,

    commit_hash String,
    author LowCardinality(String),
    time DateTime,
    commit_message String,
    commit_files_added UInt32,
    commit_files_deleted UInt32,
    commit_files_renamed UInt32,
    commit_files_modified UInt32,
    commit_lines_added UInt32,
    commit_lines_deleted UInt32,
    commit_hunks_added UInt32,
    commit_hunks_removed UInt32,
    commit_hunks_changed UInt32
) ENGINE = MergeTree ORDER BY time;

Run the tool.

Then insert the data with the following commands:

clickhouse-client --query "INSERT INTO git.commits FORMAT TSV" < commits.tsv
clickhouse-client --query "INSERT INTO git.file_changes FORMAT TSV" < file_changes.tsv
clickhouse-client --query "INSERT INTO git.line_changes FORMAT TSV" < line_changes.tsv


Usage: clickhouse git-import
Allowed options:
  -h [ --help ]                       produce help message
  --skip-commits-without-parents arg (=1)
                                      Skip commits without parents (except 
                                      the initial commit). These commits are 
                                      usually erroneous but they can make 
                                      sense in very rare cases.
  --skip-commits-with-duplicate-diffs arg (=1)
                                      Skip commits with duplicate diffs. 
                                      These commits are usually results of 
                                      cherry-pick or merge after rebase.
  --skip-commit arg                   Skip commit with specified hash. The 
                                      option can be specified multiple times.
  --skip-paths arg                    Skip paths that matches regular 
                                      expression (re2 syntax).
  --skip-commits-with-messages arg    Skip commits whose messages matches 
                                      regular expression (re2 syntax).
  --diff-size-limit arg (=100000)     Skip commits whose diff size (number of
                                      added + removed lines) is larger than 
                                      specified threshold. Does not apply for
                                      initial commit.
  --stop-after-commit arg             Stop processing after specified commit 
                                      hash.
  --threads arg (=8)                  Number of concurrent git subprocesses 
                                      to spawn


Example:

clickhouse git-import --skip-paths 'generated\.cpp|^(contrib|docs?|website|libs/(libcityhash|liblz4|libdivide|libvectorclass|libdouble-conversion|libcpuid|libzstd|libfarmhash|libmetrohash|libpoco|libwidechar_width))/' --skip-commits-with-messages '^Merge branch '

clickhouse-keeper

usage: 
clickhouse-keeper [OPTION] [-- [ARG]...]
positional arguments can be used to rewrite config.xml properties, for 
example, --http_port=8010

-h, --help                         show help and exit
-V, --version                      show version and exit
-force-recovery, --force-recovery  Force recovery mode allowing Keeper to 
                                   overwrite cluster configuration without 
                                   quorum
-C<file>, --config-file=<file>     load configuration from a given file
-L<file>, --log-file=<file>        use given log file
-E<file>, --errorlog-file=<file>   use given log file for errors only
-P<file>, --pid-file=<file>        use given pidfile
--daemon                           Run application as a daemon.
--umask=mask                       Set the daemon's umask (octal, e.g. 027).
--pidfile=path                     Write the process ID of the application to 
                                   given file.

clickhouse-keeper-converter

Usage: clickhouse keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/data/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/data/version-2 --output-dir /var/lib/clickhouse/coordination/snapshots
Allowed options:
  -h [ --help ]                 produce help message
  --zookeeper-logs-dir arg      Path to directory with ZooKeeper logs
  --zookeeper-snapshots-dir arg Path to directory with ZooKeeper snapshots
  --output-dir arg              Directory to place output clickhouse-keeper 
                                snapshot


clickhouse-static-files-disk-uploader

Allowed options:
  -h [ --help ]         produce help message
  --metadata-path arg   Metadata path (SELECT data_paths FROM system.tables 
                        WHERE name = 'table_name' AND database = 
                        'database_name')
  --test-mode           Use test mode, which will put data on given url via 
                        PUT
  --link                Create symlinks instead of copying
  --url arg             Web server url for test mode
  --output-dir arg      Directory to put files in non-test mode


clickhouse-su

Usage: clickhouse su user:group ...

clickhouse-disks

ClickHouse disk management tool
usage clickhouse disks [OPTION]
clickhouse-disks

read	read File `from_path` to `to_path` or to stdout
Path should be in format './' or './path' or 'path'

write	Write File `from_path` or stdin to `to_path`

link	Create hardlink from `from_path` to `to_path`
Path should be in format './' or './path' or 'path'

mkdir	Create directory or directories recursively

remove	Remove file or directory with all children. Throws exception if file doesn't exists.
Path should be in format './' or './path' or 'path'

move	Move file or directory from `from_path` to `to_path`
Path should be in format './' or './path' or 'path'

copy	Recursively copy data containing at `from_path` to `to_path`
Path should be in format './' or './path' or 'path'

list	List files (the default disk is used by default)
Path should be in format './' or './path' or 'path'

list-disks	List disks names

clickhouse-disks:
  -h [ --help ]            Print common help message
  -C [ --config-file ] arg Set config file
  --disk arg               Set disk name
  --command_name arg       Name for command to do
  --save-logs              Save logs to a file
  --log-level arg          Logging level

